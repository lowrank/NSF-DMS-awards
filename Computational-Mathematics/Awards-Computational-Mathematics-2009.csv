"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0915211","Numerical real algebraic geometry","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","08/01/2009","07/23/2009","Frank Sottile","TX","Texas A&M Research Foundation","Standard Grant","Junping Wang","07/31/2013","$435,757.00","","sottile@math.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1264, 1271","0000, 6890, 9263, OTHR","$435,757.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>Numerical  methods are  the  future of  computation in  algebraic geometry. The reason for  this is  that increases  in computing power  will  be  due  to  massive  parallelization  and  symbolic algorithms  do not  appear to  be parallelizable  while numerical algorithms are  easily parallelized. This project aims  to help build the  infrastructure for this numerical future.   It will do this through two main  research programs and through the training of students and a postdoctoral fellow.  One project is to develop and  implement a radically  new numerical  continuation algorithm that computes only  the real solutions to a  system of polynomial equations,   in   contrast   to  homotopy   continuation,   which necessarily computes  all solutions, both real  and complex.  The other  is to  use  numerical methods  to  study subtle  geometric invariants  of important  geometric problems,  namely  the Galois groups of  Schubert problems.  The first will  extend the toolbox of numerical  algebraic geometry, while the  second will showcase its potential for pure  mathematical research.  A primary goal of this project is the training of one or more students in this area and the  training and professional development  of a postdoctoral researcher.   Both   projects  are  multi-year   tasks  requiring software  development that will  involve team-based  research and collaborators at Colorado State  and Georgia Tech and will result in publications, software packages, and Ph.D. theses.<br/><br/>Algebraic geometry is  concerned with theoretical questions about solutions to  systems of polynomial  equations, but it  has great potential  in  applications,  in particular  through  implemented algorithmic tools.   Numerical algebraic geometry is  a new field that uses  numerical methods in  algebraic geometry and  has been driven by applications of mathematics.  This project will further its progress by developing  new numerical tools for studying real solutions,  applying it  to pure  mathematical research,  and the training of students and postdoctoral fellows."
"0915202","Generalized simple regularization for linear and nonlinear inverse problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/23/2009","Patricia Lamm","MI","Michigan State University","Standard Grant","Junping Wang","07/31/2013","$250,000.00","","lamm@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","0000, 6890, 9263, OTHR","$250,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The investigator and her colleagues propose to develop a new regularization method for ill-posed inverse problems, a method which is an extension of the ideas of the classical ""simplified regularization method"" (or ""Lavrentiev's method"") and the newer method of ""local regularization"".   Both of these methods are known to preserve special structures of the inverse problem and lead to fast numerical solution methods, but they are often limited to specialized operator equations (for example, where the operator is nonnegative self-adjoint or of Volterra type).  The idea behind the new method is to approximate the composition of the governing (linear or nonlinear) operator and a localized smoothing/averaging operator by a ""generalized simple regularization operator"" which is the sum of a third operator and a function times the identity operator.  Requiring stricter approximation properties than is usually required for simplified regularization (but which is required for local regularization), there is hope that the resulting method improves upon the numerical results usually obtained for simplified regularization.  In addition, because the new method allows for approximation of the original operator by a third operator (as mentioned above), again in contrast to the simplified regularization method, there is the potential for the new method to apply to a number of operators which do not satisfy the restrictive assumptions needed for the classical method and for local regularization. <br/><br/>Mathematical inverse problems arise in a wide number of applications, from problems of satellite image reconstruction, biomedical imaging (CT scans, X-rays) and geophysical exploration, to the determination of ozone levels in the atmosphere from measurements taken aboard orbiting spacecraft.   The methods proposed by the investigator and her colleagues are applicable to many of these problems.  In particular, these ideas play a specific role in the solution of new models for ozone determination under study by the investigator and her mathematical colleagues, as well as proposed systems-level models for attention deficit disorder (ADD) and addiction in adults, models under investigation by the investigator and her clinical colleagues."
"0854961","FRG - Advanced Algorithms and Software for Problems in Computational Bio-Fluid Dynamics","DMS","COMPUTATIONAL MATHEMATICS","07/01/2009","07/16/2012","Michael Minion","NC","University of North Carolina at Chapel Hill","Standard Grant","Junping Wang","06/30/2013","$870,478.00","Laura Miller, Jingfang Huang, Jan Prins","minion@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1271","0000, 1616, 6890, 9263, OTHR","$870,478.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The numerical modeling of the dynamics of biological fluid-structure interactions is a rapidly expanding research area in mathematical biology. We will consider two successful strategies for computing fluid-structure interactions which are popular in the applied mathematics community: the Method of Regularized Stokeslets (appropriate in the Stokes regime), and the Immersed Boundary Method (when inertial forces cannot be ignored). Both of these methods are popular partially because they offer a reasonably straightforward option for modeling complex boundaries interacting with an incompressible fluid. <br/>Unfortunately, in both cases there are substantial computational bottlenecks which severely limit the efficiency and/or accuracy of these methods for a wide class of problems. For example, in both methods, the stiffness of the structure can restrict the allowable explicit time-step of a computation by orders of magnitude. In addition, the desire to model fine scale features of biological structures requires spatially adaptive computations and parallel processing. Recent advances in multi-resolution temporal integration methods and integral based methods for fast summation and elliptic equations provide the technology to substantially increase the accuracy and efficiency of these methods, although the analysis and implementation of these algorithms has not been attempted. Our goal is to complete the mathematical and computational analysis necessary to implement efficient parallel, adaptive, multi-resolution time integration and fast summation methods for fluid-structure problems and to build a computational infrastructure that will enable researchers in the biological sciences to perform numerical simulations of biological systems on massively parallel computers.<br/><br/>The development of improved algorithms and a computational interface to facilitate massively parallel computation will enable scientists to obtain a much more detailed understanding of the fluid dynamics in a wide range of problems in the biological and medical fields such as the swimming of fish or flying of insects,  the reduction of drag in plants and trees by changes in shape or texture, and the pumping of blood in the heart or the flow of materials in the digestive system, the kidney, and the lungs. In particular, our software will allow researchers to begin to study increasingly complex problems such as fluid flow through bristled appendages, or interactions or coordination between multiple structures or organisms. Beyond obtaining fundamental insight into biological design, this work could motivate innovation in the field of biomimetics, where engineers look to biology for design strategies. For example, improved understanding of fin deformations in fish or jet propulsion in jellyfish is of interest to both the biological community and engineers working on micro underwater vehicles.  Our computational methods can be applied to model other systems of interest in biommetics including muscular hydrostats such as octopus arms, earthworm bodies, and elephant trunks."
"0914815","Computerized Search for Combinatorial Objects","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/31/2009","Stephen Hartke","NE","University of Nebraska-Lincoln","Standard Grant","Junping Wang","07/31/2013","$220,000.00","","stephen.hartke@ucdenver.edu","2200 VINE ST BOX 830861","LINCOLN","NE","685032427","4024723171","MPS","1271","0000, 6890, 9150, 9263, OTHR","$220,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The goal of this proposal is to develop a means for using integer programming efficiently to search for combinatorial objects. Integer programming is one of the most powerful techniques developed by the computer science and operations research communities for computationally solving combinatorial optimization problems. However, solving integer programs using linear programming relaxations does not perform well when searching for a combinatorial object with a specified isomorph-invariant property. Every isomorphic copy of the desired object also has the specified property, and, counter-intuitively, the presence of many objects with the desired property significantly slows the standard solving techniques of branch-and-bound and cutting planes. <br/>Additionally, the integrality condition often means the rational relaxation is not a good approximation to the integer program. The proposed work focuses on two new methods to address the difficulties caused by symmetry and divisibility conditions: rejecting isomorphic nodes from the branch-and-bound search tree through the use of symmetry-breaking orbital cuts, and using modular relaxations where the linear constraints are considered modulo a prime.<br/><br/>Combinatorial objects such as graphs, codes, and designs are important mathematical structures that appear in many disciplines, including computer science, operations research, and information technology. The proposed work includes searching for combinatorial objects using the powerful and well-known computational technique of integer programming. The PI proposes new methods for increasing the effectiveness of integer programming when used for combinatorial search. The resulting framework and computer code are intended to be used as a general tool for exploration and experimentation of a wide range of combinatorial objects.<br/>"
"0906557","Workshop on Mathematical Biology and Numerical Analysis","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY, BIO COMPUTING","09/01/2009","08/25/2009","Thiab Taha","GA","University of Georgia Research Foundation Inc","Standard Grant","Mary Ann Horn","08/31/2011","$30,000.00","Anne Summers, Jonathan Arnold, James Prestegard, Andrew Sornborger","thiab@cs.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1271, 7334, 7946","0000, 7556, OTHR","$0.00","This award provides support for a workshop on Mathematical Biology and Numerical Analysis. The objective of this workshop is to give interdisciplinary education, training and research in the mathematical and biological sciences. It will provide the participants with an overview of the necessary mathematical and biological background that are essential for a good number of applications in biological sciences.<br/><br/>The workshop is aimed at students and postdocs. It will be interdisciplinary in nature, bringing together students and postdocs from mathematical and biological sciences. The format of the workshop will consist of 12 invited speakers giving a total of 24 lectures in two tracks in two days. There will be ample time for the students to interact with the speakers on a one-on-one basis outside of the formal lectures. The speakers will be asked to present overview lectures at the level of a graduate course to ensure the audience takes away a working knowledge of the material. Then some speakers will lead the hands on use of varied mathematical biology tools in a computer laboratory. The topics to be presented are: a) numerical simulations of ecosystems; b) systems biology of complex traits; c) analysis of compositional heterogeneity among microbial genomes; d) functional connectivity and neuronal network dynamics; e) reconstructing metabolic pathways through network modeling of experimental data.  Most of the topics presented at the workshop are not typically covered in standard curricula. Furthermore, the students, postdocs and professors will benefit greatly from interacting with groups from different disciplines and from the research that integrates mathematical and biological sciences.<br/><br/>The workshop is expected to have a substantial impact on the training of students and postdocs already in the mathematical and biological sciences, as well as for the recruitment of students from other disciplines into the mathematical and biological sciences. Both groups will benefit from the quality of the teaching in areas of major and active research by leaders in the field. We will be working actively to ensure the participation of minorities, women and students with disabilities at the proposed workshop."
"0940863","International Conference on Advances in Scientific Computing; December 2009; Providence, RI","DMS","COMPUTATIONAL MATHEMATICS","10/01/2009","09/28/2009","Chi-Wang Shu","RI","Brown University","Standard Grant","Junping Wang","09/30/2010","$19,686.00","Sigal Gottlieb, Jan Hesthaven","chi-wang_shu@brown.edu","1 PROSPECT ST","PROVIDENCE","RI","029129127","4018632777","MPS","1271","0000, 7556, 9150, OTHR","$0.00","This purpose of this grant is to support an International Conference <br/>on Advances in Scientific Computing at Brown University on December <br/>6-8, 2009, to honor the memory of Professor David Gottlieb and to <br/>review recent advances and explore exciting new directions in <br/>scientific computing and related numerical solution of partial<br/>differential equations and mathematical modeling for time dependent <br/>problems and their applications.  A notable feature of this <br/>conference will be the emphasis on the crucial role of significant <br/>mathematics in the design of advanced algorithms applicable to real <br/>world problems.  The conference will include invited speakers <br/>ranging from numerical analysts with a strong interest in <br/>applications, to applied and computational mathematicians to <br/>engineers, physicists and scientists in other fields.  The list of <br/>invited speakers includes both very senior leaders in the field<br/>and relatively young scientists.  Represented in this list are both <br/>women and minority mathematicians.<br/><br/>The proposed conference will be an effective venue for the exchange <br/>of ideas among a diversified list of invited speakers and <br/>participants.  It will provide valuable guidance to young <br/>participants to identify promising problems and application areas.  <br/>It is expected that this conference will push forward the research <br/>in algorithm design and applications utilizing significant <br/>mathematics to improve efficiency and effectiveness.  These <br/>algorithms have been and will continue to be widely used in <br/>applications which are of national interest, including homeland<br/>security (image processing and pattern recognizing), environment,<br/>aerospace engineering, communications, energy science, and climate<br/>modeling."
"0914977","Space and Time Adaptivity for Moving and Free Boundary Problems","DMS","COMPUTATIONAL MATHEMATICS","08/15/2009","08/12/2009","Andrea Bonito","TX","Texas A&M Research Foundation","Standard Grant","Junping Wang","09/30/2013","$137,096.00","","bonito@math.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","0000, 6890, 9263, OTHR","$137,096.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). The adequate numerical treatment of free boundary problems exhibiting disparate scales is a formidable mathematical and computational  challenge where the geometry plays a crucial role. Modern algorithms should be able to optimize and balance the computational effort to capture small scales without over-resolving the others producing in particular efficient interface geometry description. Adaptive procedures in this context are thus critical but yet suffering from their lake of mathematical understanding. The present research proposes to design, test and analyze space-time adaptive algorithms suited for free boundary problems. Particular instances in biophysics (such as biomembranes and cardiovascular system) and material sciences (such as crystal surfaces relaxation, injection molding viscoelastic flow, and micro-devices design) are addressed. All have in common the intriguing coupling between interfaces and some quantities of interest governed by partial differential equations.<br/><br/>Deformable domains are ubiquitous in several areas of research but are still a serious computational challenge. This project proposes robust and efficient algorithms particularly tuned for such problems leading to realistic predictions and deeper understanding. The outcome will benefit many different<br/>research areas. In fact, the applications discussed are of interest in strategic topics such as human cells morphology, communication technology, nanotechnology, and high performance computing. This project is collaborative and interdisciplinary involving a number of scientists in the US and abroad. A substantial effort is devoted to education."
"0845061","CAREER:  Modeling, Analysis and Computation of Stochastic Intracellular Reactions","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/13/2009","Di Liu","MI","Michigan State University","Standard Grant","Junping Wang","08/31/2015","$410,016.00","","richardl@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","0000, 1045, 1187, 9263, OTHR","$0.00","The main objective of the proposed research is to<br/>provide simplified dynamics and design efficient numerical methods for<br/>complex intracellular stochastic chemical reacting networks. Numerical<br/>schemes will be developed to simulate systems exhibiting multiple time<br/>and concentration scales, and to solve transition paths and transition<br/>rates for systems with metastability. Error analysis of the schemes as<br/>well as sharper estimates on the transition rates will be<br/>provided. Applications will be focused on developing and analyzing a<br/>quantitative model for the Insulin response network, through<br/>collaboration with biologists at Michigan State University (MSU). The<br/>education plan includes training of graduate students that will lead<br/>to Ph.D. theses, developing a graduate level course for students in<br/>applied and computational mathematics, introducing the related topics<br/>to undergraduate curriculum, and organizing a workshop on the same topic.<br/>The proposed research will be crucial to the understanding of<br/>functional issues of intracellular reacting networks at the system<br/>level, which is becoming the new focus of genomic research. <br/>Numerical studies of the multi-scale systems will lead to new insights into the simulation<br/>schemes for stochastic systems. The development of accurate and<br/>efficient numerical methods for multi-scale chemical reaction systems<br/>involves novel analytical approaches from stochastic analysis. The study<br/>of the fluctuation driven transitions in metastable chemical reaction<br/>systems will find new applications for advanced probability theory and<br/>optimization techniques. The multi-scale and stochastic methods<br/>developed by the proposed research will find a wide range of<br/>applications in biological sciences and nano-technologies.<br/><br/><br/>The PI is going to build mathematical models and design efficient<br/>computer based simulation algorithms for chemical reactions inside<br/>living cells, which ususally involve many types of reacting channels<br/>and reacting species, as well as random fluctuations. The proposed<br/>research is to simplify and reduce complex models, which will lead to<br/>insightful conceptualization of the system. The research will be<br/>conducted in collaboration with biologists and the theoretical results<br/>with be calibrated with real data generated by experiments in<br/>labs. The targeted application, namely the modeling of Insulin<br/>response network, has a significant potential for the promotion of<br/>public health. Type 2 diabetes and impaired glucose tolerance are top<br/>causes of morbidity and mortality in the United States. In the case of<br/>insulin resistance, tissues such as muscle, fat and liver become less<br/>responsive or resistant to insulin. The disorder is also linked to<br/>other common health problems, such as obesity, hyperlipidaemia,<br/>hypertension etc. The models and algorithms developed by the PI will<br/>help to dandify the major reactions and reactants that are maintaining<br/>the normal range of plasma glaucous in individuals throughout periods<br/>of feeding and fasting. The research output will help to understand<br/>the mechanism of the Insulin disorders and the cause of type2 diabetes."
"0915228","Collaborative Research:  Non-negative Matrix Factorizations for Data Mining:  Foundations, Capabilities, and Applications","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/18/2009","Chris Ding","TX","University of Texas at Arlington","Standard Grant","Junping Wang","08/31/2014","$200,031.00","Heng Huang","chqding@uta.edu","701 S. NEDDERMAN DR","ARLINGTON","TX","760199800","8172722105","MPS","1271","0000, 9263, OTHR","$0.00","Nonnegative matrix factorization (NMF) factorizes an input nonnegative matrix into two nonnegative matrices of lower rank. It was recently discovered that NMF has unique ability to solve challenging data mining and machine learning problems.  The advantage of NMF over existing unsupervised learning methods are (1) NMF can model widely varying data distributions, (2) NMF performs both hard and soft clustering simultaneously. (3) Many other data mining problems such as semi-supervised clustering problems can be reformulated as NMF problem. Building upon these foundations, the investigators propose to establish a NMF-based comprehensive framework for data mining: (a) Provide deeper understanding of NMF's clustering capability;<br/>(b) Extend data mining capability of NMF for solving various data mining and machine learning problems; (c) Develop fast numerical algorithms which incorporate the state-of-the-art developments from numerical optimization for various matrix factorization models; (d) Develop novel and rigorous proof strategies to prove the correctness and convergence properties of the numerical algorithms; (e) Apply and evaluate these new algorithms in real-world applications.<br/><br/><br/>The proposed work creates a new paradigm of analyzing vast amount of data and discovering new knowledge from the data by transforming established matrix computational methodologies. This new technology can automatically group news articles into meaningful categories, discover protein modules in protein networks, extract weather patterns in climate data, segment pictures into distinct objects, detect communities on the Web, and enable many other scientific discoveries and new technologies creation. On a fundamental level, the proposed work establishes that a simple matrix factorization in fact solves challenging data mining problems. This research reinforces the importance of mathematics in today's data centric world and encourages students to learn mathematics."
"0913089","Nonlinear Preconditioning Techniques for Coupled Multi-physics Problems on Massively Parallel Computers","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","08/26/2010","Xiao-Chuan Cai","CO","University of Colorado at Boulder","Standard Grant","Junping Wang","08/31/2014","$270,000.00","","cai@cs.colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1271","0000, 9251, 9263, OTHR","$0.00","Mature technologies are available for solving many types of single physics problems, but for coupled multi-physics problems, robust and scalable techniques are badly needed, especially for large scale parallel computers. The focus of the proposal is on some new domain decomposition based nonlinear preconditioning techniques for the numerical solution of some highly nonlinear, coupled systems of partial differential equations (PDEs) arising from multi-physics applications. These PDEs often represent multiple interacting fields (for example, fluid and solid), each is modeled by a certain type of equations. Current approaches usually involve a careful splitting of the fields and the use of field-by-field iterations to obtain a solution of the coupled problem. Such approaches have many advantages such as ease of implementation since only single field solvers are needed, but also exhibit disadvantages. For example, certain nonlinear interactions between the fields may not be fully captured, and for unsteady problems, stable time integration schemes are difficult to design. In addition, when implemented on large scale parallel computers, the sequential nature of the field-by-field iterations substantially reduces the parallel efficiency. To overcome the disadvantages, fully coupled approaches are investigated in order to obtain full physics simulations. The success of such a fully coupled approach depends almost entirely on a nonlinear algebraic system solver that is robust and scalable. Unfortunately, traditional nonlinear iterative methods do not work well, for example, Newton-like methods often converge very slowly because of the existence of local non-smooth components in the solution and the lack of good initial guess. The new algorithms are motivated by the nonlinear preconditioning methods recently introduced by the PI and his co-workers for solving algebraic nonlinear equations that have unbalanced nonlinearities. The scalability is obtained by incorporating the multigrid methods into the algorithms. Several important applications will be studied including the simulation of blood flows in compliant arteries using a coupled Navier-Stokes and elasticity equations.<br/><br/><br/>"
"0914647","Radial Basis Functions","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","07/25/2011","Bengt Fornberg","CO","University of Colorado at Boulder","Continuing Grant","Junping Wang","08/31/2013","$318,923.00","","fornberg@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1271","0000, 6863, 9263, OTHR","$0.00","Most phenomena in nature are either entirely or to some significant extent described by partial differential equations. Although finite differences had been used earlier for the numerical solution of ordinary differential equations, the proposal in 1910 by L.F. Richardson to use them also for partial differential equations (PDEs) is now recognized as a landmark event in the history of computing. In the nearly 100 years that has followed, there has been vast progress on many fronts in this area of numerically solving PDEs, including faster algorithms, higher accuracies, easier implementations, greater robustness, improved geometric flexibility, better scaling for massively parallel computer architectures, etc. No single method excels in all these respects, and the best choice of method varies between application areas. In cases requiring very high accuracy over long times (such as many convection-dominated equations, for example arising from weather, climate, or turbulence modeling), pseudospectral (PS) methods have been found to perform especially well, as long as local refinement is not needed, and the overall geometry is quite simple. Radial basis functions (RBFs) were first proposed by Rolland Hardy (in the different context of multivariate scattered node interpolation) in the early 1970's, and were tested for solving PDEs by Ed Kansa in 1990. Much subsequent development on using RBFs for solving PDEs have come from earlier NSF-supported works by the present investigator and his research group at University of Colorado. In particular, RBFs were found to reduce to PS methods in a certain limit, and it has also become clear that this limit is less than optimal in several respects. A particularly important aspect is that RBFs can maintain spectral accuracy also in meshfree settings, allowing both general domain shapes and easy-to-implement local node refinements. The presently proposed research aims towards still unexplored new opportunities in the RBF area, both with regard to algorithmic aspects, such as numerical stability and speed, as well as extending the method to new applications. For example, there now appears to be excellent chances of achieving spectral accuracy also for a wide range of free boundary problems.<br/><br/>While the rapid advances in computational hardware during the last decades are well known, the similar and equally favorable trend in terms of numerical algorithms might be less widely appreciated. Both aspects combine to make numerical computations an increasingly important approach for exploring a wide range of issues with great societal impact, such as weather and climate modeling, tsunami early warning calculations, etc. The main topic of the present work is a numerical methodology known as Radial Basis Functions (or RBFs for short). It has been found to be either very promising or already competitive with the best previous alternatives in all of the areas just mentioned. When formulated in mathematical terms, the key challenge becomes how to most effectively solve something known as partial differential equations. RBFs offer here numerous new opportunities, which are  increasingly pursued also by other research groups. The long term goal of the present research to advance the RBF methodology to the point that it becomes still more readily applicable for tasks such as those outlined above.<br/>"
"0915062","High Relative Accuracy Iterative Algorithms for Large Scale Matrix Eigenvalue Problems with Applications","DMS","COMPUTATIONAL MATHEMATICS","06/01/2009","05/15/2009","Qiang Ye","KY","University of Kentucky Research Foundation","Standard Grant","Junping Wang","05/31/2013","$178,250.00","","qye3@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1271","0000, 9150, 9263, OTHR","$0.00","Eigenvalue computation is a fundamental problem in numerical<br/>linear algebra. While there are numerous established algorithms<br/>for this, they do not guarantee in general to compute, in a<br/>floating point arithmetic, smaller eigenvalues to high relative<br/>accuracy. Indeed, the relative errors of smaller eigenvalues<br/>computed by conventional algorithms are proportional to the<br/>condition number of the matrix. Research over the last two<br/>decades has resulted in several classes of special matrices<br/>for which the eigenvalues/eigenvectors can be computed to high<br/>relative accuracy (i.e. independent of the condition number).<br/>All existing high relative accuracy algorithms, however, appear<br/>to concern small (dense) matrices only. Yet, large matrices are<br/>often inherently ill-conditioned and it is typically a few<br/>smaller eigenvalues that are of interests. The goals of this<br/>proposal are to study high relative accuracy iterative<br/>algorithms for computing a few eigenvalues/eigenvectors of a<br/>large symmetric positive definite matrix. The investigator will<br/>develop algorithms for matrices arising in some important<br/>applications such as discretization of differential operators<br/>and dimensionality reduction in machine learning, where special<br/>structure/properties of the matrices may be exploited to<br/>achieve higher accuracy.<br/><br/>Spectral (eigenvalue) analysis is a widely used mathematical<br/>tool in science and engineering. Eigenvalues of differential<br/>operators describe the natural vibrating frequencies of<br/>mechanical structures. Dimensionality reduction of<br/>high-dimensional data in machine learning often leads to<br/>minimization of certain quadratic functionals, the solutions of<br/>which are eigenvectors. Solving such eigenvalue problems to<br/>high relative accuracy pose a challenge to existing algorithms.<br/>Therefore, the proposed works will not only advance theoretical<br/>foundations and algorithm developments for large scale<br/>eigenvalue problems, but also contribute to the general areas<br/>of applied science/engineering and machine learning by<br/>significantly improving the accuracy of the algorithms used in<br/>these applications."
"0915247","Local and Direct discontinuous Galerkin methods: New algorithms and applications","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/26/2009","Jue Yan","IA","Iowa State University","Standard Grant","Junping Wang","08/31/2013","$99,235.00","","jyan@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1271","0000, 6890, 9263, OTHR","$99,235.00","This proposal is awarded using funds made available by the American Recoveryand Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The goal of the project is to design, analyze and implement new discontinuous Galerkin(DG) finite element methods solving partial differential equations arising from physics and engineering. DG method is a highly accurate numerical method with the advantage to handle complicated geometries, and apply h-p adaptive strategies in applications. The PI will focus on the development of two discontinuous Galerkin methods. 1)Local discontinuous Galerkin methods: a new DG method is proposed to directly solve Hamilton-Jacobi equations. There is a concept difficulty to design DG methods for Hamilton-Jacobi equation, because it is a nonlinear partial differential equation not in a ?conservative? form. When applied to level set related problems, the method can sharply capture the interface, and has a great potential for further practical applications on two-phase flow problems. To solve static Hamilton-Jacobi equations, the LDG method coupled with fast sweeping method will be developed. The PI and her collaborators will continue to study LDG methods for other nonlinear wave equations. 2)Direct discontinuous Galerkin methods(DDG): a new DG method is proposed to solve diffusion type equations. The novelty of the method is to figure out what terms essentially contribute the most to the solution derivative at the discontinuity. A class of admissible numerical fluxes will be studied and stability and error analysis will be carried out. Interface correction terms are introduced to obtain optimal order of accuracy for the DDG method. Furthermore, the PI will investigate DDG methods on incompressible Navier-Stokes equations in vorticity stream-function formulation. With the successful development of DDG method, efficient and accurate DG methods can be designed to solve problems arising from computational fluid dynamics.<br/><br/>The proposed activity lies in its comprehensive coverage of algorithm development, analysis and implementation. The nonlinear problems studied in this project have rich applications and involve interesting physical phenomena. Many fluid problems involve multi-component, examples include bubble/drops, jets, waves and films. These problems have interfaces to separate different materials, and special numerical techniques are required to treat the interface accurately. Discontinuous Galerkin methods produce very small dissipation errors when applying with high order polynomial approximations, thus it is an attractive numerical method for interface capturing. The proposed activity is expected to make positive contributions to broad areas of applications, including (but not limited to) fluid dynamics, computer vision, optimal control, semiconductor device simulation and weather forecasting, among many others. In addition, the investigator will integrate the project with graduate computational mathematics education in order to communicate in a broader context."
"0915121","Numerical optimization for large-scale experimental design of ill-posed inverse problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","12/14/2010","Eldad Haber","GA","Emory University","Continuing Grant","Junping Wang","08/31/2011","$117,025.00","","haber@mathcs.emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271","0000, 9263, OTHR","$0.00","Inverse problems play a key role in a variety of fields such as computer vision, geophysics and medical imaging. The proposed work focuses on experimental design of ill-posed inverse problems; a field that has not received sufficient attention in the inverse problems community where the focus is usually on the analysis of the inverse problem given the data. Obviously, this already imposes restrictions on the quality of the possible solutions.   On the other hand, the objective of the funded work is the study of an  important pre-data acquisition question: How should the experiment be conducted to obtain optimal data given the physical constraints and available resources? Solutions to this question require techniques from numerical optimization, statistics and inverse problem theory. In particular, the problem of experimental design can be cast as a bilevel optimization problem that consists of two nested optimization problems. The proposed work is a study of design criteria and new numerical algorithms for the solution of the bilevel optimization problems that arise from them.<br/><br/><br/>This work  will address the fundamental question of experimental design of   ill-posed inverse problems. Such problems arise in the design of any practical experiment  or instrumentation from geophysical  and medical imaging to the  production of better vision systems. This work will develop new criteria for the design and development of new algorithms that will enable its numerical implementation. The results of the research will be applied to electromagnetic imaging, a field that is  routinely used in geophysics and medical physics. <br/>It will lead to better experiments that yield better images and as such, will assist in  the decision making of geoscientists and and physicians.<br/><br/>"
"0914354","Large-Scale Optimization for Biomolecular Modeling","DMS","COMPUTATIONAL MATHEMATICS","08/15/2009","08/04/2009","Zhijun Wu","IA","Iowa State University","Standard Grant","Junping Wang","07/31/2012","$124,594.00","","zhijun@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1271","0000, 7237, 9263, OTHR","$0.00","Project Abstract<br/><br/>Two classes of optimization problems arising in protein modeling are investigated. The first class is for solving a generalized distance geometry problem for protein structure determination, where a large scale nonlinear constrained optimization problem needs to be solved. The second class is for solving a boundary-value problem for simulation of protein conformational transition. A geometric buildup algorithm is to be developed for the solution of the generalized distance geometry problem by determining the points (and their spheres), one at a time, using the distance bounds between the determined and undetermined points (and their spheres). The existence and uniqueness of the solution of the optimization problem, the necessary and sufficient conditions for obtaining a solution, and the convergence of the algorithm, will be analyzed. The algorithm will be tested on a large set of problems generated from existing protein structures, and be integrated into existing modeling software for practical applications. A multiple-shooting scheme is to be developed for the solution of the boundary-value problem. In particular, a constrained nonlinear least-squares problem is formulated for the implementation of the multiple-shooting scheme. The issues on the choice of the norm to be minimized, the formation of the initial trajectories, and the convergence of the algorithm, will be addressed. The algorithm will be tested on small to medium size problems (with up to several hundreds of atoms and nanoseconds of simulations) and extended to real and large scale simulation problems (with up to several thousands of atoms and milliseconds of simulations). <br/>Many problems in biomolecular modeling are formulated as optimization problems, such as the phase problem in X-ray crystallography, the distance geometry problem in nuclear magnetic resonance spectroscopy (NMR), and potential energy minimization problem for protein folding. These problems are typically in large scale with the number of variables or constraints ranging from tens to hundreds of thousands. They are difficult to solve as well since many of them demand a globally optimal solution and have proven to be computationally intractable in general. This project involves the development of novel algorithms for the solution of two large, difficult, yet critical classes of optimization problems arising in protein modeling. It promises a significant increase in the power of the current technologies for protein modeling, especially for NMR protein structure determination and simulation of protein conformational transition. It has potential impacts in such important biological and medical applications as structural genomics, rational drug design, and cancer and AIDS research. The problems to be investigated also represent two classes of interesting mathematical problems. But they have not been well posed, studied, and solved before. The proposed project will encourage research on these problems and help to advance the theory and algorithmic development in related mathematical areas such as distance geometry and optimization.  <br/>"
"0914876","Special finite element methods based on component mode synthesis techniques: analysis and applications","DMS","COMPUTATIONAL MATHEMATICS","10/01/2009","09/14/2009","Ulrich Hetmaniuk","WA","University of Washington","Standard Grant","Junping Wang","09/30/2013","$141,001.00","","hetmaniu@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1271","0000, 6890, 9263, OTHR","$141,001.00","This proposal is awarded using funds made available by the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The research objective is the development of new discretization methods for complex models driven by partial differential equations. These new discretization methods will enable the accurate, efficient, and robust solution of such complex models. For example, existing discretization methods are challenged by multi-scale problems due to the range of scales (spatial, temporal) present in the problem. Here the objective would be to design an improved discretization method that captures the small-scale spatial and temporal effects while avoiding the cost of a fully resolved simulation. The focus will be on special finite element methods, denoting methods of finite element type that employ special shape functions. These functions, typically non-polynomials, can incorporate specialized knowledge about the governing equation (via eigenmodes or particular solutions). The approach will supply a systematic procedure for defining accurate, robust, and efficient special finite element methods. The theoretical frame combines knowledge from the theories of domain decomposition and component mode synthesis, of spectral decomposition for linear operators, and of finite elements. <br/><br/>The tools developed in this project will apply to a wide range of complex problems of strategic importance. Examples include understanding the elastic behavior of heterogeneous structures, modeling tumor growth, and simulating flow through porous media. These problems are fundamentally multi-scale and remain mathematically and computationally challenging. The proposed research is also expected to be a fertile ground for graduate education in computational mathematics. A graduate student will be involved in the analysis and design of state-of-the-art discretization method and be trained in practical engineering techniques, like domain decomposition and component mode synthesis. He or she will get a rare perspective that combines knowledge from mathematical theory and practical engineering."
"0914942","Finite Volume Methods and Software for Hyperbolic Problems","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/13/2009","Randall LeVeque","WA","University of Washington","Standard Grant","Leland Jameson","08/31/2013","$490,242.00","","rjl@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1271","0000, 6863, 9263, OTHR","$0.00","Nonlinear hyperbolic systems of partial differential equations arise in many scientific and engineering applications where wave propagation or transport phenomena are important, giving rise to discontinuous solutions such as shock waves.  Often  the problems are posed in heterogeneous media in which the material parameters vary and may be discontinuous across interfaces.  Solving these equations requires special techniques based on the mathematical theory of weak solutions.  The investigator has previously developed a multidimensional ""wave-propagation algorithm"" that yields a very general framework for solving such problems.  These high-resolution finite volume methods are implemented in the open source Clawpack software package, which allows students and researchers studying a wide range of phenomena to use the technology of high-resolution methods and adaptive mesh refinement.  These algorithms and the software are being further developed and brought to bear on a variety of problems, particularly in geophysical flow and wave propagation, in collaboration with researchers in these fields.  A version of the Clawpack software (GeoClaw) specifically tuned to model tsunami propagation and innundation has recently been developed and is being extended to handle various related problems involving flow over topography.  The study of specific applications is motivating the development of new mathematical models and finite volume algorithms to more accurately and robustly model these extreme flows.<br/><br/>This work focuses on the development of computational models for simulating geophysical flow and wave propagation problems that are related to the prediction and mitigation of natural disasters.  The investigator and coworkers been working on tsunami modeling for several years and have developed a software package that has been used both for scientific research on tsunami phenomena and for the preparation of innundation maps for specific communities in the Pacific Northwest. In addition to continuing this work, the software is being extended to model other related phenomena, which often requires  the development of new mathematical models and computational algorithms.  Applications being studied include: flooding due to catastrophic dam breaks, storm surges generated by hurricanes such as Katrina, tsunamis generated by submarine landslides, and the flooding and debris flows that would result from the eruption of volcanoes such as Mt. Rainier that are covered with glaciers.  The investigator and his students are working with researchers in earth sciences, at the USGS Cascades Volcano Observatory, and at several other research centers on scientific and hazard mitigation projects related to this software development.  New algorithms for modeling earthquakes are also being developed and applied to the study of tremors at Mount St. Helens in order to help determine the risk of future eruptions.  The investigator is actively involved in training students and postdoctoral fellows at the University of Washington as well as at other institutions by hosting visiting graduate students and scientists.  The investigator has also taught several short courses elsewhere and has developed lecture notes, textbooks, software, and other educational material based on this research.  The software enhancements under way will further improve its usability as a freely-available educational tool for students and researchers in many fields where similar problems arise.<br/>"
"0914215","Large-Scale Structured Optimization:  Convex Relaxations and Gradient Methods","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","09/16/2009","Paul Tseng","WA","University of Washington","Standard Grant","Rosemary Renaut","08/31/2012","$0.00","","tseng@math.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1271","0000, 9263, OTHR","$0.00","Optimization problems arise in signal/image denoising,<br/>compressed sensing, sensor network localization, multi-task learning, data<br/>mining/classification, are large-scale, nonlinear, but highly structured.<br/>An effective solution approach is to approximate the problems by<br/>convex relaxations that are computationally tractable and inherit the<br/>structures of the original problems.  Specialized computer algorithms<br/>are then developed to exploit the structures and<br/>efficiently solve the convex relaxations in real time.<br/>The investigator and his colleagues/students study the accuracy of<br/>different convex relaxations (i.e., how well they approximate the<br/>original problems) when data may be noisy, and develop new computer<br/>algorithms that are more efficient in theory and in practice.<br/>This includes coordinatewise and incremental gradient algorithms,<br/>possibly accelerated through extrapolation.<br/>Complexity and convergence rate are studied, as well as<br/>computer implementation and testing.<br/>Thus the twin topics of approximation by convex relaxations and<br/>algorithms for convex relaxations are integrated in their development.<br/><br/>Measured data, from satellite images to biological images to stock indices<br/>to internet queries/usage, are inherently noisy and large size.<br/>How to process such large noisy data and extract key<br/>features/patterns is a major challenge, with important applications to<br/>image restoration/denoising, economic forecasting, pattern recognition,<br/>data classification, etc.  In the proposed research, parametrized<br/>mathematical models of the underlying data generating process are<br/>formulated and the parameters are tuned by specialized computer algorithms<br/>to optimize the model's predictive power.  The latter is achieved<br/>by optimizing a combination of the model's goodness-of-fit to data and a<br/>measure of model's simplicity.   This is motivated by Occam's Razor<br/>principle that the simplest is the best (in particular, for identifying<br/>underlying patterns and trends).  For example, a 1 Mega-pixel<br/>noisy image might be modeled by a weighted sum of 1000 ""elementary""<br/>images from a dictionary.  The computer algorithm then finds weights<br/>that are sparse (i.e., few nonzeros) and<br/>  fit the model closely to the noisy image (say, in a least squares<br/>sense of the pixel values).  Predicting<br/>people's behavior based on past data is another<br/>example, such as the Netflix prize for predicting movie rating which<br/>has a training data set of over 100 million ratings from over 480 thousand<br/>people.  Owing to the large data size (and, in some cases, a need to<br/>process the data fast and in real time) and possibly high-dimensional<br/>parameter space, specialized algorithms based on new techniques (e.g.,<br/>work with small chunks of data or small number of parameters at each time)<br/>need to be developed.  This is the aim of the proposed research."
"0915128","Collaborative Research: Computational problems in heterogeneous nanomaterials","DMS","COMPUTATIONAL MATHEMATICS","08/15/2009","08/19/2009","John Lowengrub","CA","University of California-Irvine","Standard Grant","Junping Wang","07/31/2012","$365,716.00","","lowengrb@math.uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271","0000, 7237, 9263, OTHR","$0.00","In many applications ranging from energy to biomedicine, nanocrystalline materials, such as quantum dots and nanowires, promise to yield revolutionary new technologies. The realization of this promise is hindered by the challenges inherent in reproducibly fabricating nanocrystalline materials with controlled morphologies and compositions.  These nanomaterials are typically heterogeneous and consist of alloys with multiple constituents. While there has been much work on formulating conditions under which spatially ordered nanocrystals with nearly uniform shapes and sizes may be produced, a quantitative description of the mechanisms that determine the spatial distribution of the alloy components, which is crucial to device performance, is still poorly understood. The investigators and their collaborators address this issue in this proposal. They study the nonlinear dynamics of  heterogeneous, strained strained nanocrystalline materials by (1) developing and applying state-of-the-art adaptive numerical methods to large-scale computation and (2) performing analytical, numerical and modelling studies of important constituent processes. The investigators focus on the dynamic, nonlinear coupling among shape, elastic stress and composition in the context of (i) the dynamics of thin film alloys and quantum dots under far-from-equilibrium processing conditions where there may be bulk and surface transport of the different constituents, as well as phase decomposition; and (ii) the coarsening dynamics and stability of capped nanocrystals. The cap material is needed in applications to provide the confinement potential for charge carriers as well as passivation against the external environment. These problems are characterized by the presence of multiple constitutive components, bulk-surface interactions, complex pattern formation and/or singularities (i.e. spatial complexity). The mathematical models involve high-order spatial derivatives (e.g. up to sixth-order), evolving free boundaries and highly nonlinear interactions that make analysis and simulation difficult, particularly in 3D. The highly nonlinear nature of these problems makes fast, accurate and robust numerical methods essential to their study.<br/><br/>Nanocrystalline alloy materials have physical properties that make them ideally suited for a wide range of potential applications including advanced electronic and magnetic devices as well as biological and chemical sensors. The properties of nanoscale devices are determined both by the spatial composition of the heterogeneous nanocrystal components and the nanocrystal geometry. Recent advances in experimental techniques have enabled the characterization of nanoscale composition variation in nanocrystals.<br/>However, a quantitative understanding of these variations remains elusive and yet is critical to device performance. The investigators and their collaborators address this issue by developing new mathematical models, theory and computational methods that make it possible to characterize and quantify the interactions among nanocrystal shape, elastic stress and composition. The investigators also consider capped nanostructures where the cap material provides protection from the environment that is needed in many applications including the use of nanocrystals in silicon-based electronic circuits. The interaction among the nanocrystalline and capping materials introduces additional complexity. These problems are multidisciplinary and progress requires the combined expertise of the investigators in materials science and applied and computational mathematics. Through this study, the investigators provide guidance in the quantitative interpretation of experimental measurements of composition variation in nanocrystals and suggest optimized processing conditions to achieve desired device shape, composition and performance. The project establishes a new collaboration between two institutions and provides interdisciplinary training of two Ph.D students and one postdoctoral researcher. In addition, the investigators build on their recent success and continue to develop and teach a course on crystal and epitaxial growth for gifted high school students as part of the Calif. State Summer School for Mathematics and Science (COSMOS) at UC Irvine. This course also helps to recruit new math and science majors and enhance the participation of high school students in research.<br/>"
"0914720","Collaborative Research: Computational and theoretical approaches for the morphological control of material microstructures","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/21/2009","John Lowengrub","CA","University of California-Irvine","Standard Grant","Leland Jameson","07/31/2013","$151,634.00","","lowengrb@math.uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271","0000, 6890, 9263, OTHR","$151,634.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The investigator and colleagues study the prediction and morphological control of two-phase microstructures in solid/liquid and solid/solid diffusional phase transitions. Much of the research in this area is concerned with detailed and extensive studies of complex patterns such as dendritic growing shapes. In many applications (e.g. castings), it is desirable to control the formation of dendrites and grow compact shapes, which, however, has been much less studied. This project helps to fill the gap and aims to develop guidelines by which microstructures with desired shapes may be grown. The research team plans to (1) develop a suitable nonlinear theory of compact precipitate growth including existence, uniqueness, and stability of self-similar shapes; (2) develop and employ state-of-the-art adaptive 3D numerical methods to test the validity and limitations of theory; <br/>(3) compare theoretical and numerical results with existing experiments to test the validity of the mathematical assumptions and to verify the accuracy of predictions derived from the theory and simulations. <br/><br/>Diffusional phase transformations deal with transformations of melts into precipitates (and vice-versa) as well as the separation of solids (e.g. metal alloys) into distinct phases. These phenomena have importance for a variety of processes including casting, welding and soldering, crystal growth, and related problems concerning protein and macromolecular crystallization. For example, crystal growth processes for technological applications began in the late 19th century, and form the cornerstone of virtually all modern semiconductor electronics and photonics today. The research activities will provide new mathematical theory and numerical simulations that can be used to develop guidelines for controlling the morphology of certain solidified materials. The new mathematical theory and adaptive numerical methods developed in the project have applications to a broader set of related problems including multiphase flows, biostructures and growth of solid tumors. In addition, this project will provide valuable interdisciplinary training opportunities for young researchers."
"0914995","Collaborative Research: Computing Dynamics of Multiparameter Systems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/15/2009","08/09/2011","William Kalies","FL","Florida Atlantic University","Continuing Grant","Junping Wang","08/31/2014","$252,246.00","","wkalies@comcast.net","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","MPS","1266, 1271","0000, 9263, OTHR","$0.00","The language and ideas of dynamical systems theory that have been developed over the last century have become ubiquitous in the applied sciences. While the analytic language of differential equations and maps is still the basis for most quantitative descriptions of scientific ideas, current scientific <br/>results are often obtained based on models which are not derived from first principles, for which many of the essential parameters have not been measured, and which often involve stochastic terms. The key objective of this project is to develop scalable computational techniques to provide correct robust information about global dynamics over large ranges of parameter values. Bifurcation theory implies that the cost for robustness is a coarse description. However, the fact that scientists and engineers are using numerical simulations of phenomenologically derived models to further their understanding of dynamic processes indicates that the information these techniques provide must be both quantitative and qualitative. Since the study of systems over broad ranges of possible parameter values produces <br/>considerable information, to be of practical use these methods must organize this information in an efficient, queriable manner. We expect the work proposed in this project will produce (1) reliable computational tools for global decompositions of dynamical systems by constructing a database in <br/>which the global dynamics is encoded in combinatorial and algebraic structures and (2) efficient methods for querying the database to identify dynamical structures and bifurcations of interest. <br/><br/>This work will address the fundamental question of determining global decompositions of dynamical systems over varying parameters. The global dynamics is stored in the form of a database based on calculations for deterministic systems, but within the framework of these computations we will also explore how to predict the effects of noise on the observable dynamical behavior. These computational techniques will be tested on and applied to a variety of problems from mathematical biology. The biological models <br/>which will be considered are used to address central questions in biology including the role of the spatial environment in ecology and evolution and the robustness of the dynamics of signal transduction/gene regulatory networks. These activities will produce computational tools for global decompositions of dynamical systems, which will be made available to scientists and engineers for potential applications in a wide variety of disciplines.<br/><br/><br/>"
"0914937","Collaborative Research: A Computational Framework for Assessing the Observation Impact in Air Quality Forecasting","DMS","COMPUTATIONAL MATHEMATICS, Atmospheric Chemistry","07/15/2009","02/22/2012","Dacian Daescu","OR","Portland State University","Standard Grant","Junping Wang","09/30/2013","$244,403.00","","daescu@pdx.edu","1600 SW 4TH AVE","PORTLAND","OR","972015522","5037259900","MPS","1271, 1524","0000, 4444, 6890, 9188, 9263, EGCH, OTHR","$244,403.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>This research develops the algorithmic and computational framework needed for a judicious assessment of the observation impact in air quality modeling. Novel algorithms in the framework of model-constrained optimization will allow to account for the data location in the time-space domain, observation type, instrument type, and data interaction in the presence of multiple observing systems. Specifically, the research is focused on: development of high-order adjoint-based observation impact techniques consistent to four-dimensional variational data assimilation schemes; assessment of forecast sensitivity to observation error variances and estimation of the forecast impact of uncertainties in the specification of the input error statistics; development of efficient computational approaches to allow for practical implementations of the observation impact algorithms; validation of the novel techniques through observing system experiments and assessment of the potential impact of new observing systems.<br/><br/>The ability to accurately represent the distribution of atmospheric pollutants in relation to various anthropogenic activities is essential for chemical weather forecasting to protect the population, for answering science questions related to the future of our planet, and for designing sound environmental policies. An accurate representation of the chemical composition of the atmosphere requires a close integration of models and observations through data assimilation.<br/>Data assimilation is the process by which model predictions utilize measurements to produce an optimal representation of the state of the atmosphere. As more observations are becoming available and new measurement networks are being planned, it is of critical importance to develop the capabilities to best utilize the data, to better manage the sensing resources, and to design more effective field experiments and networks to support atmospheric chemistry and air quality studies. This research develops the computational tools required to optimize the information provided by the existing observing systems and to provide guidance for future improvements to the observational network and instruments design. The new developments will also help the design process of new field experiments and of new chemical monitoring networks.<br/><br/><br/><br/><br/>"
"0915057","Treecode-Accelerated Implicit Solvent Models for Biomolecular Simulations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","02/29/2012","Robert Krasny","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Junping Wang","08/31/2013","$242,659.00","Weihua Geng","krasny@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","0000, 1719, 9263, OTHR","$0.00","Current biomolecular simulations are unable to reach the long time scales needed to study conformation changes such as protein folding. One of the main obstacles is the high cost of computing the electrostatic forces among the solvent water molecules surrounding the protein. To address this issue, this project adopts an implicit solvent model in which the electrostatic potential satisfies the Poisson-Boltzmann (PB) equation. Numerical solution of the PB equation poses a challenge due to the geometric complexity of the molecular surface, the discontinuity in the dielectric function, and the unbounded computational domain. The investigators will overcome these difficulties by developing a boundary integral PB solver using a new Cartesian treecode algorithm for screened Coulomb interactions. The treecode-accelerated PB solver will be tested on benchmark examples such as Kirkwood's solution for a spherical surface, and the results will be compared with those obtained using other PB solvers. In addition to the electrostatic potential, the code will be extended to compute other important quantities such as the solvation free energy and solvation forces needed for dynamics. <br/><br/><br/>  One obstacle facing current biomolecular simulations is the expense of computing the self-induced electrostatic forces among the molecules in the system. Advances in computer hardware alone won't achieve the improvements necesssary for studying long time molecular dynamics. This project therefore focuses on improving the mathematical algorithms used in these computer simulations. In addition to enabling more accurate and efficient biomolecular simulations, the algorithms developed will be potentially useful in other applications where electrostatic forces play a role, for example in modeling charge transport in fuel cells. The project will contribute to training the scientific workforce by supporting the research of a postdoc who will be mentored by the PI.<br/>"
"0915153","User-Friendly Solvers and Solver-Friendly Discretizations","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/18/2009","Jinchao Xu","PA","Pennsylvania State Univ University Park","Standard Grant","Junping Wang","08/31/2013","$219,000.00","Chensong Zhang, Chao-Yang Wang","xu@math.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1271","0000, 9263, OTHR","$0.00","In this proposal, various algorithms and theories will be developed to partially realize the following four-stage strategy for developing user-friendly solvers and solver-friendly discretizations: (1) develop user-friendly optimal solvers and relevant theories for a small number of basic solver-friendly systems, namely the discrete Poisson's equation and its variants; (2) extend the list of solver-friendly partial differential equations (such as the discrete Stokes and Maxwell equations) by reducing them to the solution of a handful of basic solver-friendly systems (for which optimal and user-friendly solvers can be applied); (3) develop solver-friendly discretization techniques for more complicated PDEs (systems) such that the discretized systems will join the list of solver-friendly systems (such as the Eulerian-Lagrangian method for the Navier-Stokes equation, the Johnson-Segalman equations, and the magnetohydrodynamics equations); and (4) solve the discretized system from a general discretization by using a solver-friendly discretization (if it is not a satisfactory discretization to obtain the numerical solution by itself) as an auxiliary discretization that can be used as a preconditioner or a means for obtaining a good initial guess for a linear or nonlinear iteration. These techniques will be developed with the purpose of making them effective for solving complicated problems such as non-Newtonian models and fuel cell model equations. Parallel implementations will be one major consideration in the design of these algorithms. Theoretical issues---such as the most fundamental open problem concerning the optimal convergence of algebraic multigrid methods---will be carefully investigated.<br/><br/><br/>Many problems in scientific and engineering computing can be reduced to the numerical solution of certain partial differential equations. Over the last few decades, researchers have expended significant effort on developing efficient iterative methods for solving discretized partial differential equations. Though these efforts have yielded many mathematically optimal solvers such as the multigrid method, the unfortunate reality is that multigrid methods have not been much used in practical applications. This marked gap between theory and practice is mainly due to the fragility of traditional multigrid methodology and the complexity of its implementation. This proposal aims to develop theories and techniques that will narrow this gap, specifically by developing mathematically optimal solvers that are robust and easy to use in practice. The proposed study will focus on an integrated application of user-friendly solvers and solver-friendly discretizations for various basic partial differential equations that arise in many applications; therefore, the results of this proposal are expected to be directly applicable in many areas of computational and applied mathematics. The solver and discretization techniques we produce, including mathematical algorithms, analyses, and software, will provide powerful tools for exploring multiscale models in physics, chemistry, and engineering.  Through the accompanying Matrix-Solver Community Project (http://www.multigrid.org/solvers/), the results of this proposal will lead to timely and broad impacts. The proposed project will have a strong educational impact as well, as it focuses on training graduate students in theoretical and practical aspects of modern computational science and interdisciplinary applications. <br/>"
"0915019","Collaborative proposal: Computing Dynamics of Multiparameter Systems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/15/2009","07/18/2011","Konstantin Mischaikow","NJ","Rutgers University New Brunswick","Continuing Grant","Junping Wang","08/31/2014","$275,698.00","","mischaik@math.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1266, 1271","0000, 9263, OTHR","$0.00","The language and ideas of dynamical systems theory that have been developed over the last century have become ubiquitous in the applied sciences. While the analytic language of differential equations and maps is still the basis for most quantitative descriptions of scientific ideas, current scientific <br/>results are often obtained based on models which are not derived from first principles, for which many of the essential parameters have not been measured, and which often involve stochastic terms. The key objective of this project is to develop scalable computational techniques to provide correct robust information about global dynamics over large ranges of parameter values. Bifurcation theory implies that the cost for robustness is a coarse description. However, the fact that scientists and engineers are using numerical simulations of phenomenologically derived models to further their understanding of dynamic processes indicates that the information these techniques provide must be both quantitative and qualitative. Since the study of systems over broad ranges of possible parameter values produces <br/>considerable information, to be of practical use these methods must organize this information in an efficient, queriable manner. We expect the work proposed in this project will produce (1) reliable computational tools for global decompositions of dynamical systems by constructing a database in <br/>which the global dynamics is encoded in combinatorial and algebraic structures and (2) efficient methods for querying the database to identify dynamical structures and bifurcations of interest. <br/><br/>This work will address the fundamental question of determining global decompositions of dynamical systems over varying parameters. The global dynamics is stored in the form of a database based on calculations for deterministic systems, but within the framework of these computations we will also explore how to predict the effects of noise on the observable dynamical behavior. These computational techniques will be tested on and applied to a variety of problems from mathematical biology. The biological models <br/>which will be considered are used to address central questions in biology including the role of the spatial environment in ecology and evolution and the robustness of the dynamics of signal transduction/gene regulatory networks. These activities will produce computational tools for global decompositions of dynamical systems, which will be made available to scientists and engineers for potential applications in a wide variety of disciplines.<br/><br/><br/>"
"0913048","Collaborative Research: Algorithms for Simulation and Design of Analog VLSI Lattices","DMS","COMPUTATIONAL MATHEMATICS","05/15/2009","05/18/2009","Harish Bhat","CA","University of California - Merced","Standard Grant","Leland Jameson","08/31/2011","$95,452.00","","hbhat@ucmerced.edu","5200 N LAKE RD","MERCED","CA","953435001","2092012039","MPS","1271","0000, 7237, 9263, OTHR","$0.00","New technologies in areas such as wireless communication, portable computing, and handheld electronics have increased the demand for signal processing at high frequencies.  Part of the challenge in designing silicon integrated circuits that can meet this demand is to overcome limitations in the efficiency and frequency bandwidth of modern transistors.  Here it is proposed to use two-dimensional networks of inductors and capacitors to overcome these limitations.  These high-speed, high-efficiency networks have a cut-off frequency that is higher than that for silicon-based transistors.  Moreover, such networks can be incorporated into standard silicon chips that can be fabricated at low cost.  The proposed research has the potential to revolutionize high-frequency analog signal processing, leading to chips that operate up to 1000 times faster than current ones.<br/><br/>There are a large number of possible designs for such networks, and only a small number of these possibilities have already been explored.  The proposed research seeks to develop algorithms that greatly assist in the simulation and design of two-dimensional inductor-capacitor networks.  Simulating a large network involves the solution of a large, coupled system of equations that can be simplified greatly through mathematical analysis.  It is proposed to use this simplification to develop fast, scalable algorithms and codes for network simulation.  This would enable engineers to quickly learn the effect of changing one or more of the thousands of parameters in a typical large-scale inductor-capacitor network.<br/><br/>It is also proposed to use optimization methods to automatically design lattices that achieve prescribed input-output relationships.  The optimization work will use as a foundation the prior results of the proposers, including, for example, the development of a two-dimensional network that computes Fourier transforms in the analog domain.  Such physically motivated ideas will be coupled with modern tools of parallel numerical computing such as PetSC (the Portable Extensible Toolkit for Scientific Computation) and TAO (the Toolkit for Advanced Optimization).  This will result in fast, accurate tools that enable engineers to rapidly optimize the design of a lattice to achieve desired performance specifications.<br/><br/>The expertise gained in carrying out the proposed research will enable the investigators to train students and researchers to solve problems in modern computational science and engineering. The proposed research encourages multidisciplinary interaction between scientists, engineers, applied mathematicians, and computer scientists spanning the spectrum from developers to users of computational tools."
"0965711","Collaborative Research: Second-order methods for large-scale optimization in compressed sensing","DMS","COMPUTATIONAL MATHEMATICS","12/15/2009","08/17/2010","Roummel Marcia","CA","University of California - Merced","Continuing Grant","Leland Jameson","12/31/2013","$131,428.00","","rmarcia@ucmerced.edu","5200 N LAKE RD","MERCED","CA","953435001","2092012039","MPS","1271","0000, 9263, OTHR","$0.00","Nonlinear image reconstruction based upon sparse representations of signals and images has received widespread attention recently with the advent of compressed sensing. This emerging theory indicates that, when feasible, judicious selection of the type of distortion induced by measurement systems may lead to dramatically improved inverse problem solutions. In particular, when the signal or image of interest is very sparse (i.e., zero-valued at most locations) or highly compressible in some basis, relatively few indirect observations are necessary to reconstruct the most significant non-zero signal components. Compressed sensing theory states that sparse signals can be recovered very accurately with high probability from indirect measurements by solving an appropriate optimization problem. This research aims at the development of significantly more efficient methods for solving compressed sensing minimization problems. A crucial property of the proposed methods is that the algorithms are designed to be ""matrix-free"", i.e., they do not require the storage of (potentially very large) second-derivative matrices. Instead, these methods use matrices only as operators for matrix-vector products. <br/>This research also includes the application of the developed solvers to real large-scale problems from image processing (e.g., coded aperture superresolution, hyperspectral image reconstruction, and compressive video reconstruction), as well as theoretical proofs of convergence and numerical stability of the algorithms. <br/><br/>""Compressed sensing"" is an emerging field in computational mathematics that aims to improve signal (and image) reconstruction with less data. <br/>More efficient methods for compressed sensing can benefit such fields as medical imaging, astrophysics, biosensing, and geophysical data analysis. The basic theory exploits the fact that many natural signals in science and engineering are ""sparse""--that is, they can be represented as a weighted combination of a small subset of commonly occurring signals. When a signal is sparse, scientists can accurately reconstruct the original signal using a relatively small number of measurements of the original signal. However, in practice finding the right weighted combination of signals can create staggering numerical and computational complexity. This research aims to develop novel optimization methods that can quickly find accurate solutions to these large-scale problems. For example, consider an astronomer wishing to image the night sky, which consists of small, bright stars against a dark background. A conventional digital camera or imaging system would need a very high resolution and sensitive photodetector to effectively localize the different stars, but collecting this large number of pixels can be very costly and energy inefficient. This work allows the astronomer to collect a relatively small number of random projection measurements of the scene and use these to reconstruct the image with a high probability of accuracy. Fast and accurate optimization algorithms for sparse signal reconstruction can impact many other areas of image and signal processing as well, from reducing the dose of CT scans in biomedical imaging and improving image resolution in video surveillance systems for airport security, to more efficiently transmitting communication signals from distant satellites and NASA spacecraft and more carefully monitoring the health of a forest ecosystem using hyperspectral imaging."
"0914963","A Theoretical and Algorithmic Study of the Immersed Boundary Method","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/26/2009","Yoichiro Mori","MN","University of Minnesota-Twin Cities","Standard Grant","Junping Wang","07/31/2013","$315,059.00","","y1mori@sas.upenn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","0000, 6890, 9263, OTHR","$315,059.00","This proposal is awarded using funds made available by the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The immersed boundary method is a popular numerical scheme to simulate problems problems of fluid-structure interaction. Despite the popularity of the immersed boundary method, its convergence properties are poorly understood. The goal of this proposal is to build a convergence theory of the immersed boundary method and to develop efficient implicit methods to <br/>overcome numerical stiffness often encountered in application. A salient feature of the immersed boundary method is the use of regularized Dirac delta <br/>functions to handle the presence of sharp interfaces within the computational domain. This idea is now widely used beyond the immersed boundary method in multiphase and complex flow simulations in the context of front-tracking and level-set methods. The theory developed here should also contribute to<br/>our understanding of such methods. One of the major difficulties in applications of the immersed boundary method is numerical stiffness, necessitating the practitioner to use very small time steps to avoid numerical instability. Building upon previous work, efficient implicit methods will <br/>be developed to overcome this difficulty.<br/><br/>Phenomena in which fluid and elastic materials interact abound in nature and engineering. Important examples include blood flow in the heart, swimming and flying of birds and insects, and flows that arise in industrial processes. Computational investigation is very important in understanding such phenomena, and the immersed boundary method is one of the most popular computational methods to simulate such problems. In this project, we seek to develop more accurate and efficient ways to perform immersed boundary computations. One important ingredient for this is to develop a theoretical framework to understand convergence properties of immersed boundary computations. Such a convergence theory should serve as a firm foundation upon which to develop better algorithms to simulate flows with immersed elastic structures."
"0915028","Novel Numerical Techniques for Complex Fluids Modeling","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/31/2009","Young-Ju Lee","NJ","Rutgers University New Brunswick","Standard Grant","Leland Jameson","07/31/2013","$144,795.00","","y_l39@txstate.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1271","0000, 6890, 9263, OTHR","$144,795.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>Non-Newtonian fluids or complex fluids are abundant in our daily lives. Examples of such fluids include molten plastics, engine oils with polymeric additives, paints, egg white and blood. Breakthrough technology can come from experiments or through utilization of new modeling methods that have the potential to provide rapid screening of possible processes. More likely is that simulation will play a greater role in providing a fundamental understanding of the breakthrough technology. Capturing this fundamental understanding with models provides the connectivity between an actual practical operating device and fundamental mechanistic information and allows for a more realistic assessment of process viability. Models provide the basis of extending the operating parameter space and have a significant impact on the decision to build the full scale and on the nature of the ultimate design. <br/><br/>The accurate design, efficient implementation, and the use of numerical methods for the computer simulation of such important fields as non-Newtonian mechanics will result in significant applications. The computational modeling package, once established, will be made for the public use."
"0910540","Finite Element Approximation of Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","09/11/2012","Richard Falk","NJ","Rutgers University New Brunswick","Standard Grant","Junping Wang","07/31/2013","$315,552.00","","falk@math.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1271","0000, 9263, OTHR","$0.00","The first area of proposed research involves the study of compatible discretizations of partial differential equations, i.e., numerical approximation schemes that inherit or mimic fundamental properties of a partial differential equation. The proposed research seeks to capitalize on the idea that some of the same concepts that lead to the well-posedness of<br/>boundary value problems for partial differential equations can be used to develop compatible and stable finite element schemes for their approximation. The plan is to use this idea to develop a simpler and more systematic analysis of finite element methods, which based on past work, should then lead to the design of new and better numerical approximation schemes for a variety of applied problems. The second project proposed is the further development of a new numerical approximation scheme recently developed by the P.I. for the simulation of a parallel-plate Electrowetting on Dielectric Device. Electrowetting is an effect, based on a relationship between electrical and surface tension phenomena, that allows for control of the shape and motion of a liquid-gas interface, through the use of an applied voltage. The liquid surface changes shapes when a voltage is applied in order to minimize the sum of the surface tension energy and electrical energy. The proposed research is to first investigate the new approach computationally to see how the results compare with experiments, and then to establish stability of the method and derive error estimates. Some novel features of the method are that the changing boundary position is now a variable in the formulation and the unknown velocity can be approximated by standard simple finite element spaces. The final project is the design of new simpler quadrilateral and hexahedral finite elements for use in the mixed finite element approximation of partial differential equations. A main issue is that such elements, when defined in the usual way, do not retain the same approximation properties as those defined on squares and cubes. <br/><br/>Mathematical modeling of physical and biological processes using partial differential equations has become the standard method of studying a host of important problems. Such models capture in a concise and precise way the fundamental features of the process being modeled. Unfortunately, the resulting equations rarely have solutions that can be expressed by simple mathematical formulas. Hence, the development of reliable and efficient numerical approximation schemes are necessary to make this method into a practical approach and is central to progress in many areas of science and engineering. Part of this development involves the investigation of the theoretical underpinnings of numerical methods. Such investigation can lead to a greater understanding of existing methods and to the development of new methods with desirable properties. Thus, such study has the potential to improve the accuracy of, or even make possible, essential computer simulations performed by scientists and engineers. This project is concerned with the study of numerical methods for approximating equations modeling phenomena in a variety of applications. One central theme is to develop approximation schemes that preserve discrete versions of some of the key properties of the mathematical model, in order to more accurately capture the fundamental features of the underlying process being modeled."
"0915064","Collaborative Research:  Multi-manifold data modeling: theory, algorithms and applications","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","06/11/2011","Gilad Lerman","MN","University of Minnesota-Twin Cities","Continuing Grant","Leland Jameson","08/31/2013","$362,384.00","","lerman@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","0000, 9263, OTHR","$0.00","The object of this proposal is the analysis of existing methods, and  the development of new ones, for the task of multi-manifold learning,  where the data is assumed to be comprised of low-dimensional  structures.  The main focus will be in studying the potential of  spectral methods for clustering and modeling of low-dimensional  surfaces embedded in high dimensions; in designing new spectral-based  approached to the task of detection of low-dimensional objects in  point clouds; and the analysis of popular manifold learning  algorithms, especially in terms of robustness to outliers.  A number  of applications will be specifically addressed, for example, motion  segmentation, structure from motion, classification of face images,  segmentation of diffusion tensor images and the characterization of  cosmological models in astrophysics. <br/><br/>Modern high-dimensional datasets often exhibit low-dimensional  structures. Such situations arise in image processing; e.g., in  target tracking, where a typical trajectory defines a curve through  successive frames; and also in medical imaging, e.g., in the  examination of vascular networks. The study of the galaxy  distribution, which contains filamentary and sheet-like structures,  is another example.  Traditional methods are known to be ineffective  in this context and the last decade has seen a massive amount of  research aiming at improving on these classical tools.  A number of  approaches for multi-manifold modeling have been suggested, mostly by  computer scientists and engineers.  Applied mathematicians and  statisticians have different perspectives to <br/>offer and their contribution is needed, not only in designing new  algorithms but also (and perhaps especially) in providing theoretical  foundations, which researchers in the field have been asking for. The  research in this proposal will address both issues, developing  rigorous mathematical theory combined with carefully designed  numerical strategies addressing specific applications, such as motion  segmentation, structure from motion, classification of face images,  medical imaging and the characterization of cosmological models in  astrophysics.  The PIs will share their findings through publications  and software, all available online to the scientific and engineering  communities. <br/>"
"0913757","Efficient numerical techniques of two-phase transport model in the cathode of hydrogen polymer electrolyte fuel cell","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/14/2009","Pengtao Sun","NV","University of Nevada Las Vegas","Standard Grant","Junping Wang","08/31/2013","$90,000.00","","pengtao.sun@unlv.edu","4505 S MARYLAND PKWY","LAS VEGAS","NV","891549900","7028951357","MPS","1271","0000, 6890, 9150, 9263, OTHR","$90,000.00","This proposal is awarded using funds made available by the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>This project is to develop advanced numerical techniques in order to perform efficient, accurate<br/>and state of the art simulations for two-phase transport model in the cathode of hydrogen<br/>proton exchange membrane fuel cell (PEMFC). The computational efficiency and accuracy for<br/>solving two-phase transport PEMFC model depends crucially on the partition of mesh for precisely<br/>capturing the anisotropic interface of single- and two-phase zones, the design of proper<br/>discretization schemes and efficient iterative methods for solving a highly unstable nonlinear<br/>system due to the discontinuous and degenerate diffusion coefficient. The PI proposes to develop<br/>anisotropic adaptive mesh techniques, and advanced algorithms in both discretization<br/>and iteration level in order to design a better discretized model which can be solved more efficiently and accurately by iterative methods on an optimal mesh. More precisely, for anisotropic<br/>adaptive mesh method, the PI proposes an a posteriori error estimator based on error equaldistribution<br/>by equalidistributing edge length of finite element in Hessian matrix-metric. For the<br/>discontinuous and degenerate diffusion coefficient, the PI proposes Kirchhoff transformation to<br/>skillfully reformulate the original PEMFC model to a linear Poisson's equation, and Newton's<br/>method to efficiently solve the resulting inverse Kirchhoff transformation. In particular, for the<br/>case of wet gas channel in PEMFC, in which Kirchhoff transformation brings the discontinuity<br/>back to the resulting Kirchhoff's variable on the interface of gas channel and gas diffusion<br/>layer, the PI proposes Dirichlet-Neumann alternating iterative domain decomposition method<br/>to resolve this interfacial boundary problem. On the discretization level, the PI will design a<br/>combined finite element-upwind finite volume method to overcome the dominant convection in<br/>gas channel of PEMFC without losing the benefits of FEM. For nonlinear iteration schemes,<br/>the PI will employ either Picard's or Newton's method to linearize nonlinear PEMFC model,<br/>combining with specifically preconditioned Krylov-type solver. The PI hopes to develop more<br/>efficient and accurate numerical simulations for two-phase transport model in the cathode of<br/>hydrogen PEMFC by uniting modern numerical techniques of adaptivity and multilevel solvers<br/>with standard numerical methods.<br/><br/>Fuel cells have been called the key to abundant energy from secure and renewable sources,<br/>e.g., fuel cells promise to replace the internal combustion engine in transportation due to their<br/>higher energy efficiency and zero or ultralow emissions. Hydrogen proton exchange membrane<br/>fuel cell (PEMFC) is presently considered as a potential type of fuel cells for such application.<br/>Since PEMFC involves electrochemical reactions, current distribution, two-phase flow transport<br/>and heat transfer, a comprehensive mathematical modeling of multiphysics system and high performance<br/>computing combining with the advanced numerical techniques shall make a significant<br/>impact in the development of fuel cell technology. However, because of the complexity of the underlying<br/>mathematical model, current numerical techniques are far from being satisfactory due<br/>to poor performances on both efficiency and accuracy. Hence, advanced numerical techniques<br/>are urgently required to significantly improve the efficiency and accuracy of fuel cell simulation.<br/>The proposed numerical techniques in this project will challenge a number of critical numerical<br/>difficulties, which are caused by large discontinuity, degeneracy, nonlinearity, dominant convection<br/>and anisotropy, by designing and analyzing the efficient numerical methods toward fast<br/>convergence and precise solutions. The PI will utilize the proposed efficient numerical methods<br/>to eventually develop an efficient and robust in-house code for PEM fuel cell simulations<br/>by achieving one to two orders of magnitude improvement on the existing commercial fuel cell<br/>packages in computational performance. The PI hopes that the proposed numerical techniques<br/>and numerical package for PEMFC will lead to a significant progress and likely breakthrough in<br/>the field of computational fuel cell technology, substantially impacting the commercialization of<br/>fuel cells and further helping in the transition to hydrogen economy."
"0915242","Rigid motion steerability for multiscale stochastic models of 3D-textures applied  to soft tissue segmentation/identification in 3D-biomedical images","DMS","COMPUTATIONAL MATHEMATICS, COFFES","09/01/2009","08/24/2011","Emanuel Papadakis","TX","University of Houston","Continuing Grant","Leland Jameson","08/31/2013","$490,712.00","Ioannis Kakadiaris, Robert Azencott","mpapadak@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271, 7552","0000, 9263, OTHR","$0.00","Modern medicine and biology have been enormously benefited from the advancement of imaging. New devices and acquisition methods enabled the first images of viruses. Resolution levels for diagnostic imaging are now at the order of a few hundred microns and in 3D; e.g. MRI or CT scans. Despite of all these advances some information in medical images is latent and extracting it is often a tedious task. Achieving finer resolution levels does not automatically make every tissue visible to the eye of the practitioner. The expansion of the imaging frontiers not only increases grossly the volume of the available data but also makes to want to extract more information from an image. Thus, there is an ever growing demand for the development of reliable, automated or semi-automated image analysis tools. With this goal in mind the interdisciplinary group of investigators in this project aims in making theoretical and algorithmic contributions that can lead to the development of such tools.<br/><br/>The problem motivating this project is how to identify or segment soft-tissues that are of interest to medical practitioners or biologists with high spatial accuracy in 3D-images. To our detriment, most of the time tissues of diagnostic interest have great variability, small volume, low contrast and are corrupted by non-standard noise. Based on the premise that soft-tissues are associated with 3D-textures, the investigators approach soft-tissue discrimination/identification as segmentation/identification of the 3D-textures resulting from the tissues of interest. Notable efforts have been made to solve this problem in 2D but in 3D it is practically untouched. To achieve high spatial accuracy in the segmentation/identification of 3D-textures the investigators will build novel probabilistic models for 3D-rigid motion invariant texture signatures. This will reduce or may even eliminate classification errors due to the positioning of a tissue in the 3D-space. To extract such signatures we will characterize and thoroughly study multiscale data representations that are covariant (steerable) with respect to 3D-rigid motions. A major challenge of this project is to extract 3D-rigid motion invariant texture signatures with reasonable length and adopt probabilistic models governing the classification of these signatures in a computationally manageable manner. The envisioned tools will be tested in (3D) CT-angiography scans and 3D-confocal microscopy images of pyramidal neurons. In the first case we wish to segment various soft tissues such as cardiac muscle, epicardial fat, lumen and calcium while in the second we wish to identify dendrites in a noisy background.  The investigators aim in developing an algorithmic platform for soft-tissue segmentation based on novel 3D-data representations rather than a customized application. This research program requires the development of novel mathematical ideas both in mathematical analysis and in probability theory. These new mathematical concepts and methods will endow the envisioned algorithms with a unique ability native to human vision but not yet achieved in computer and robotic vision: the identification of structures and patterns independently of their position in the 3D-space. Indeed, tissues must be correctly identifiable by any automated image analysis system regardless of their position in the 3D-space or in the human body. A system with this ability will be able to circumscribe tissue boundaries with the same high accuracy in every direction in the 3D-space. This algorithmic platform can be adopted for a wide variety of imaging applications in medicine and biology, such as CT-angiography used to diagnose stenosis in coronary arteries or contrast CT for the detection of liver cancer. Detecting abnormalities in the walls of coronary arteries especially of their regions proximal to the ascending aorta will help prevent the most life-threatening infarctions and possibly monitor the treatment of the atherosclerotic plaque without the frequent use of the grossly invasive intravascular ultrasound probes. Identifying cancerous lesions in the liver at their early stages of development can significantly increase the chances of survival in this type of cancer. Capturing accurately the structure of dendrites and of their protruding attachments called spines in images acquired with 3D-confocal microscopes is a prime time goal as spines seem to hold the key of understanding the biological basis of depression and bipolar disorder.<br/><br/>"
"0915237","The immersed interface method (IIM) for interfaces immersed in fluids","DMS","COMPUTATIONAL MATHEMATICS","07/01/2009","07/04/2009","Sheng Xu","TX","Southern Methodist University","Standard Grant","Leland Jameson","06/30/2012","$191,774.00","","sxu@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","0000, 6890, 9263, OTHR","$191,774.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The key objective of this research is to combine the strength of the immersed interface method (IIM), hierarchical grids, and the level set method for accurate and and efficient simulation of interface problems.<br/>In particular, the investigator and his students develop the hierarchical-grid IIM for fluid-solid interfaces and the level set IIM for fluid-fluid interfaces. Hierarchical grids provide fine resolution for resolving flow and geometry at a solid and allow a large domain for treating open far-field boundary conditions. The hierarchical-grid IIM is particularly suitable for simulating nature's flyers or swimmers in moving frames. In the level set method, a fluid-fluid interface is captured as a level set of a scalar function which is evolved by a partial differential equation (PDE).<br/>The level set method is very robust for capturing interfaces subject to topological changes. Jump conditions occur at a fluid-fluid interface. They also appear at a fluid-solid interface when the solid is represented as a (singular) force concentrating at the interface through the delta function. The main idea of the IIM is to directly incorporate necessary jump conditions into a numerical scheme. How to achieve high accuracy if available jump conditions are limited?<br/>How to incorporate jump conditions if they are coupled? How to determine the force to enforce the motion of a moving rigid solid?<br/>In this research, the investigator and his students answer these questions. In particular, they apply generalized Taylor expansions to construct high-order finite differences with limited jump conditions, use augmented-variable approaches to implement coupled jump conditions, and develop a boundary condition capturing approach to model free-moving rigid objects. The intellectual merit of this research is reflected in three aspects: the comprehensive development and implementation of various numerical methods, the generation of new approaches for computational modeling of interfaces in fluids, and the extensibility of the ideas produced in this research to a broad range of other interface problems.<br/><br/>Fluid-solid and fluid-fluid interface problems are very rich in nature and technological applications. Insect flight is a beautiful example of fluid-solid interface problems. Insect flight is fascinating not only because it is beautiful to human eyes but also because its unconventional aerodynamics has great technical importance, especially in helping design flapping-wing micro air vehicles (MAVs). One well-known example of fluid-fluid interface problems is in oil recovery. The idea is to push out the oil trapped in the ground by flooding with water. A technical problem for oil recovery is to find means to suppress the fingering instability at water-oil interfaces. To study such interface problems, computational fluid dynamics<br/>(CFD) has become a very important role. CFD can provide very detailed flow data, which are difficult or impossible to obtain experimentally. The CFD data help understand flow physics and help construct and validate reduced analytical models.<br/>Because of the variety of flows and associated complexity, development and improvement of CFD techniques is still in high demand. In this research, the investigator and his students develop CFD techniques to achieve high-fidelity simulation of various interface problems. They improve some existing CFD methods, combine the strength of each, and develop new ones.<br/>This research has impact on unveiling unconventional aerodynamics and hydrodynamics of nature's flyers and swimmers. It also has impact on lubricated transport, air-driven liquid cooling, and laser welding. Its educational impact includes: (1) blending research-related topics into existing and new curricula; (2) training graduate students for simulation-based research; and (3) engaging undergraduate students in the research. In particular, building a user input module for simulating nature's flyers and swimmers is a nice summer research opportunity for undergraduate students."
"0915222","Fast integral equation methods for moving boundary problems in parabolic PDEs","DMS","COMPUTATIONAL MATHEMATICS","08/15/2009","08/10/2009","Johannes Tausch","TX","Southern Methodist University","Standard Grant","Junping Wang","07/31/2012","$210,259.00","Vladimir Ajaev","tausch@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","0000, 9263, OTHR","$0.00","Fast algorithms, such as the Fast Multipole Method, wavelets, or FFT-based convolutions are by now well established methods for solving boundary integral formulations of elliptic partial differential equations.  These methods have been successfully applied in potential theory, viscous flow, linear elasticity and wave propagation.This project seeks to broaden the range of fast boundary integral solvers to problems governed by parabolic differential equations. In this case the integral operators involve a convolution over the history of problem, which increase CPU and storage requirements to impractical levels unless fast methods are used.  Specifically, we will investigate Nystrom methods for the discretization of boundary integral equations and develop fast algorithms based on Chebyshev interpolation of the heat kernel. We expect to evaluate thermal layer potentials in nearly optimal time.  The numerical methods can be applied to problems governed by the heat equation or transient Stokes flow. Because of unconditional stability and better asymptotic scaling these methods have the potential to replace the conventional finite element- or finite difference methods as the workhorse algorithm.<br/><br/>The newly developed numerical techniques will be applied to the problem of laser melting of a metal film on a substrate and to the interaction of fluid interfaces with micro- and nanostructured surfaces.  These problems are of current interest for technological applications. The simulations of laser-induced melting of metal films will lead to potential advances in fabrication of micro- and nanochannels for lab-on-a-chip devices. Such portable devices can be used for detection of a range of substances from chemical pollutants to biological weapons. The simulations of interaction of fluid interfaces with micro- and nanostructured surfaces are important for the development of the so-called self-cleaning surfaces, which are considered among the most promising applications of nanotechnology. The proposed project will also promote learning through research and interdisciplinary scientific collaborations."
"0913982","Collaborative Research: Numerical Methods for Fully and Implicitly Nonlinear Equations","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","06/10/2011","Roland Glowinski","TX","University of Houston","Standard Grant","Junping Wang","08/31/2012","$177,924.00","Alexandre Caboussat","roland@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","0000, 9263, OTHR","$0.00","The main goal of this project is to further investigate the numerical solution of fully nonlinear elliptic equations such as Monge-Ampre?s, in order to extend previous work done with the support of NSF Grant DMS-0412267. The main findings of these previous investigations are that, after regularization of the data (which is not always necessary), well-chosen least-squares formulations in appropriate Hilbert spaces lead to robust solution methods able to compute classical solutions, or generalized ones if classical solutions do not exist. The objectives of the present project are: (i) To improve the performances of the iterative methods used to solve the least-squares problems. This will require the development of novel algorithms to solve the many (one per grid point) low dimensional nonlinear eigenvalue problems obtained from the decomposition of the least squares problems, since this results in a number of small but intricate constrained eigenvalue problems. (ii) To demonstrate the effectiveness of these new algorithms on a variety of test problems (Monge-Ampre, Pucci, Gaussian curvature, sigma-2, in dimension 2, 3, and even 4 for the Pucci problem, using parallelization). (iii) To determine whether the remarkable homogenization properties observed in two-dimensions for some of these fully nonlinear elliptic equations when a coefficient in the operator varies periodically or randomly in space persist in higher dimensions. (iv) To apply, ultimately, the above methodology (or close variant of it) to the solution of some implicitly nonlinear partial differential equations from non-smooth differential geometry that model folding phenomena.<br/><br/>What motivates these investigations is the fact that fully nonlinear elliptic equations play an important role in areas as diverse as material sciences, nonlinear elasticity, fluid mechanics, atmospheric sciences, nonlinear elasticity, shape design in electrical and structural engineering (antennas, car shape,?), finance, applied and theoretical physics, differential geometry and others. The related mathematical problems have generated a large literature. In contrast these problems have the reputation to be difficult from a computational standpoint explaining why the computational and applied mathematicians have not made significant progress on their numerical solution. One of the goals of this project is to close the gap between the various communities concerned with fully nonlinear elliptic equations so that each of them will learn from the others, setting an example of interdisciplinary science. Such an effort will also benefit science and engineering professionals and students, via publications, dedicated web sites and post-graduate courses, lectures at conferences, and of course direct involvement for some graduate students. It will also stimulate the contributions of other scientists to these important areas. Since computational methods developed previously by the Principal Investigators and their associates are currently used in many areas of Science and Engineering, Academia and Industry, one can expect a similar endeavor for the results and products originating from this collaborative project.<br/>"
"0914788","Computational methods for the suspensions of deformable and rigid particles and their applications to modelling of blood flows","DMS","COMPUTATIONAL MATHEMATICS","08/15/2009","08/07/2009","Tsorng-whay Pan","TX","University of Houston","Standard Grant","Junping Wang","07/31/2014","$340,454.00","Roland Glowinski, Ronald Hoppe","pan@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","0000, 9263, OTHR","$0.00","This project focuses on developing computational methods for simulating the suspensions of deformable and rigid particles and their applications to modeling the microcirculation of blood flow and cell separation in microchannel flow. It is computationally challenging to simulate directly the three-dimensional motion and dynamics of hydrodynamically interacting red blood cells in a fluid. We will use a spring network model with its associated energy potential to model the membrane of the red blood cell. <br/>We propose to combine the spring network model with the immersed boundary methods, finite element methods and operator splitting techniques to simulate the cell-fluid and cell-cell interactions in three dimensions. <br/>We also want to combine the above proposed methodology with the distributed Lagrange multiplier/fictitious domain methods, which are closed related to the immersed boundary methods, to simulate the suspensions of cells and solid particles. Through the computational methodologies proposed by the PIs, efficient three-dimensional simulations of the red blood cells in microvessels will be performed to study the rheology of red blood cells in microcirculation, the margination dynamics of solid particles in microvessels and the cell separation in microchannels.<br/><br/>The microcirculation, which takes place in the smallest blood vessels (i.e., arterioles, capillaries, and venules), is responsible for regulating blood flow in individual organs and for exchange between blood and tissue. Since blood contains about 40-45% red blood cells by volume, as well as platelets and leukocytes, the interactions of these formed elements play a crucial role in determining blood characteristics. Because of their large volume fraction and their aggregation capacity, red blood cells are the most important  determinant of blood flow characteristics. While theories of suspension rheology generally focus on homogeneous flows in infinite domains, the important phenomena of blood flows in microcirculation depend on the combined effects of vessel geometries, cell deformabilities, wall compliance, flow shear rates, and many micro-scale chemical, physiological, and biological factors. We concentrate on the rheological aspects of flow in microcirculation involving deformable cells, cell-cell interactions and vessel geometry, which is particularly challenging theoretically and computationally. The first main application is to study the margination dynamics of solid particles in microvessels. The intravascular delivery of rigid particles for biomedical imaging and therapy is being recognized as a powerful and promising tool in cardiovarscular and oncological applications. These particles can be loaded with drug molecules and contrast agents and transported by the blood flow through the circulatory system. They are generally decorated with ligand molecules which are able to interact specifically with antigens expressed over diseased cells (or target cells). The effect of particle shape, size and density on margination propensity is needed to be analyzed in order to find the optimal design. The second main application is to study cell separation in microchannel flow. The main focus is to gain inside in the separation of healthy and sick red blood cells due to their deformability and size on a microfluidic biochip platform. Separation of soft objects is of enormous relevance in medical and biological applications, since very often a loss in deformability comes along with diseases such as malaria, diabetes mellitus or cancer, just to name a few. Hence, a microfluidic system to sort or separate cells would be of tremendous importance for both diagnostic and dialysis applications."
"0944521","Southwest Conference on Integrated Mathematical Methods in Medical Imaging; February 2010; Tempe, Arizona","DMS","COMPUTATIONAL MATHEMATICS","10/01/2009","09/06/2009","Anne Gelb","AZ","Arizona State University","Standard Grant","Junping Wang","09/30/2010","$30,520.00","","annegelb@math.dartmouth.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1271","0000, 7556, 9263, OTHR","$0.00","The proposal is for funding to help defray the expenses of participants, especially minorities, women, graduate students, postdocs, and junior faculty, at Southwest Conference on Integrated Mathematical Methods in Medical Imaging that will take place February 6 and 7, 2010, at Arizona State University, Tempe, Arizona.<br/><br/>The main focus of the conference is to enhance the mathematical understanding of the consequences of modern data collection strategies used in magnetic resonance imaging (MRI) with respect to generating high fidelity images. <br/>The conference will bring together researchers from the mathematical sciences and engineering as well as practitioners from the biomedical community. It will engage researchers from all career stages, including undergraduates from ASU and other Southwestern institutions who will be invited to fully participate in the conference. Graduate and undergraduate students from ASU, the University of Arizona and other Southwestern institutions will also be encouraged to give poster presentations and attend talks. The organizers anticipate representation from some of the many Arizona medical research hospitals, Mayo Clinic, BNI, and Banner Health, who are actively engaged in brain imaging research, as well with researchers from the University of Arizona.<br/>"
"0852533","Saint Mary's University of Minnesota Women in Mathematics Colloquium Series","DMS","INFRASTRUCTURE PROGRAM, COMPUTATIONAL MATHEMATICS","07/01/2009","05/15/2009","Kevin Dennis","MN","Saint Mary's University of Minnesota","Standard Grant","Junping Wang","06/30/2011","$10,890.00","","kdennis@smumn.edu","700 TERRACE HTS","WINONA","MN","559871320","5074524430","MPS","1260, 1271","0000, 7556, 9263, OTHR","$0.00","The support from this grant provides the funds for the Mathematics and Statistics Department at Saint Mary's University of Minnesota to continue an annual colloquium highlighting successful women mathematicians and to support undergraduate travel to national conferences. Both activities support the department's goal to produce students who can effectively communicate mathematics to laypeople and to experts in the field. Both the colloquium speakers and undergraduate presenters at national conferences provide role models for Saint Mary's students by giving them the chance to observe and interact with successful mathematics professionals and peers. The colloquium speakers especially will provide examples to our students of successful female mathematicians. This series encourages the recent upward trend of women in mathematics at Saint Mary's University and introduces undergraduate and high school women to role models who are not only successful in their mathematical profession, but also in their lives. This project intends to keep inspiring women to pursue terminal degrees in mathematics and also to enhance the Saint Mary's mathematics community. <br/><br/>A major objective of this project is to continue a successful annual colloquium for the advancement and encouragement of women in mathematics. By bringing well-known and outstanding women scientists specializing in mathematics to lecture and talk about their areas of expertise, students are engaged and inspired by the opportunities and challenges presented for women in the ever-progressing field of mathematics. This series provides undergraduate women with the opportunity to spend time listening and conversing with successful women in the field in hopes that they will be encouraged to go on to graduate studies and consider careers in mathematics. Additionally, since the symposium encourages participation from local high schools, it leaves high school women with a positive and hopeful outlook on the field and fosters their inclination to pursue math majors in college."
"0915100","Numerical Multilinear Algebra in Signal Processing and Environmetrics","DMS","COMPUTATIONAL MATHEMATICS","07/01/2009","06/15/2009","Carmeliza Navasca","NY","Clarkson University","Standard Grant","Junping Wang","06/30/2013","$182,142.00","","cnavasca@uab.edu","8 CLARKSON AVE","POTSDAM","NY","136761401","3152686475","MPS","1271","0000, 6890, 9263, OTHR","$182,142.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The PI and her collaborators will develop tensor-based numerical methods for applications primarily to signal processing and environmetrics, and secondarily to image processing, data mining and scientific computing. In the past, these subjects benefited from advances in numerical linear algebra. In analogy, numerical multilinear algebra aims to find useful decompositions of a given tensor into sums of simpler tensors, such as simple tensor products. This leads to efficient data compression. By using a new tensor norm called the trace-class norm for tensors, one can substantially improve de-noising, compression, and reconstruction of signals. This norm has attracted much attention in matrix rank minimization because of compressed sensing applications. Also, the PI will develop  new regularization methods for tensor decomposition. Many of the problems in tensor applications are ill-posed, such as reconstruction of multi-channel signals in the presence of noise.  New numerical stable methods will be studied in the framework of inverse and ill-posed problems. <br/><br/>Mathematics has applications to environmetrics.  The EPA and other agencies use air quality models, to decide how to regulate emissions. These models include receptor modeling, source apportionment and pollution identification. The PI and her collaborators will work on new computations methods to provide more accurate estimation and analysis of environmental data. Tensor-based signal processing also has many applications in biomedical imaging, such as magnetic resonance imaging diagnosis and nuclear magnetic resonance spectroscopy in tumor detection. Better data analysis tools will be facilitated by new higher-order tensor decompositions. A key component of the research project is collaboration with other scientists as well as students. Through summer research programs for undergraduates, as well as inclusion of graduate students, they will train a new generation of applied mathematicians in the methods of tensor based analysis.<br/>"
"0914773","Collaborative research: Theoretical and experimental approaches to search problems in group theory","DMS","COMPUTATIONAL MATHEMATICS, Information Technology Researc","09/15/2009","09/11/2009","Alexander Ushakov","NJ","Stevens Institute of Technology","Standard Grant","Junping Wang","08/31/2012","$131,772.00","Alexei Miasnikov","sasha.ushakov@gmail.com","1 CASTLEPOINT ON HUDSON","HOBOKEN","NJ","07030","2012168762","MPS","1271, 1640","0000, 9263, OTHR","$0.00","The objective of this proposal is to address various search problems in group theory. <br/>Decision problems  in group theory have been studied for over 100 years now, since Dehn put forward, <br/>in the beginning of the 20th century, the three famous decision problems now often referred to as <br/>Dehn's problems: the word problem, the conjugacy problem, and the isomorphism problem. In general, <br/>decision problems are problems of the following nature: given a property P and an input O, find out <br/>whether or not the input O has the property P. On the other hand, search problems are of the following <br/>nature: given a property P and an input O with the property P, find a proof (sometimes called a ""witness"") <br/>of the fact that O  has the property P. This is a substantial shift of paradigm, and in fact, studying <br/>search problems often gives rise to new research avenues in mathematics or computer science, very different <br/>from those prompted by addressing the corresponding decision problems.<br/><br/>The potential broader impacts of the proposed research are extensive; the impact on the general area <br/>of information security can be singled out. The difficulty of several well-studied problems, e.g. <br/>integer factorization and the discrete logarithm problem underlie most current public-key cryptographic <br/>protocols used in real-life applications.  Developing public-key protocols based upon other search problems, <br/>e.g. the conjugacy search problem whose difficulty has been well studied by group theorists, is prudent from <br/>the standpoint of robustness, particularly if factorization or related developments threaten the security of <br/>current protocols. The complexity of non-abelian infinite groups is a promising fertile ground for new <br/>protocols and there is a great deal of preliminary work required such as that proposed here.<br/>"
"0845760","CAREER: Predicting global climate change through fluctuation-dissipation: A practical computational strategy for complex multiscale dynamics","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","03/18/2014","Rafail Abramov","IL","University of Illinois at Chicago","Standard Grant","Junping Wang","08/31/2015","$472,997.00","","abramov@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1271","1045, 1187, 1303, 6890, 9263, EGCH","$472,997.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The proposal describes a practical computational framework for the response of a complex chaotic nonlinear multiscale dynamical system to changes in external forcing parameters. It is based on the PI's recent successful efforts to create a numerical approach for the fluctuation-dissipation theorem to predict linear response of a nonlinear chaotic forced-dissipative dynamical system to an external perturbation with improved skill, based on a precise geometric response formula for systems with chaotic attractors. While the method developed is observed to perform well for relatively simple dynamical systems, complex realistic climate models are more computationally expensive, have nonlinear interactions on multiple scales, and sometimes include stochastically parameterized processes. Here the PI proposes several numerical strategies to adapt the new approach for complex multiscale nonlinear forced-dissipative, and, possibly, stochastically parameterized dynamical systems with many variables and highly non-Gaussian equilibrium state, as well as reduce its computational cost. Successful implementation of proposed algorithms should help create a novel computational framework for global climate change prediction. The proposal also suggests development of a set of graduate-level courses with strong emphasis on the basic subjects of chaotic nonlinear dynamics, atmospheric and oceanic physics, and computational weather and climate prediction, which are the key topics needed to become an interdisciplinary weather and climate research scientist. These courses will give graduate students an opportunity to interact directly with the PI's research and learn a variety of advanced theoretical approaches and numerical methods directly from the PI through graduate advising.<br/><br/><br/>The ability of the linear fluctuation-dissipation approach to identify the ranges of parameter perturbations which produce catastrophic climate response can be helpful in determining potentially harmful types of anthropogenic intervention into the Earth's global climate cycle. This data may provide additional information to help define economic, political and legislative initiatives to preserve our environment and to develop advanced technologies for more reliable environmentally-friendly renewable power systems. In addition, such an approach is also suitable for the inverse climate change problem, where the geological evidence of past climate changes is used to compute the range of physical forcing parameters which triggered these climate changes, which can help to determine the cause of these climate changes on the planetary scale.  The set of courses under the PI's development may potentially evolve into a consistent interdisciplinary educational program on the graduate level to train future climate and weather research scientists with heavy mathematical and computational bias, which could eventually be adopted as a basic educational standard for climate and weather research. Implementation of this program will reduce the burden on national weather and climate research centers and laboratories which currently spend substantial efforts on the training of their employees at the postdoctoral level."
"0914802","Algorithms and Software for Singular Polynomial Systems","DMS","COMPUTATIONAL MATHEMATICS","07/01/2009","03/04/2010","David Marker","IL","University of Illinois at Chicago","Standard Grant","Junping Wang","06/30/2012","$158,365.00","","marker@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1271","6890, 9179, 9263, SMET","$158,365.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>In this project the PI will develop robust symbolic-numerical methods for solving singular polynomial systems and decomposing singular varieties. These algorithms would obtain information about a singular variety that the current regular methods can not deliver: in particular, discover embedded components of the solution set. This approach will lead to numerical algorithms that would solve problems that are intractable by symbolic primary decomposition routines. A major part of this project is software implementation. Macaulay2, a free open-source computer algebra system created by Grayson and Stillman, will make a platform for an efficient implementation of our algorithms. A package will be written in the Macaulay2 language with computationally intensive routines implemented in C++ in the<br/>Macaulay2 kernel. Since the basic routines for homotopy continuation scale well, parallel implementation of the aforementioned algorithms will be carried out where appropriate.<br/><br/>Polynomial systems are ubiquitous in various mathematical models in science and engineering. Many, if not the majority, of the polynomial systems arising in the real world contain singular solution components. The produced software will help a broad range of scientists and engineers confronted with polynomial systems in their work. The students appointed as research assistants through this project will gain invaluable experience in mathematical research combined with programming and development of algorithms.<br/><br/>"
"1001521","LTB: Generalized Variational Integrators for Large-Scale Scientific Computation","DMS","COMPUTATIONAL MATHEMATICS","09/04/2009","01/19/2010","Melvin Leok","CA","University of California-San Diego","Standard Grant","Junping Wang","08/31/2011","$131,061.00","","mleok@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","0000, 9263, OTHR","$0.00","Geometric integration is concerned with the construction of numerical methods that preserve the geometric structure of a continuous dynamical system. Many problems arising in science and engineering, such as solar system dynamics and molecular dynamics, are highly nonlinear, sensitive to small perturbations, and have underlying geometric structure that affects the qualitative behavior of solutions. The chaotic properties of these dynamical systems render prohibitively expensive the accurate computation of particular trajectories for long-time integration. As such, it is instead desirable to study numerical methods that preserve the geometry of a problem as they yield more qualitatively accurate simulations. The goal of this project is to generalize variational integrators based on a discrete Hamilton's principle to larger-scale problems arising from astrodynamics, molecular dynamics, and computational mechanics. This will involve incorporating methods from large-scale scientific computation, such as adaptivity, spectral methods, multi-resolution hierarchical techniques, and domain decomposition, while retaining the geometric preservation properties of variational integrators for Hamiltonian ODEs and PDEs.<br/><br/>Computer simulations of complex physical systems have become an increasingly important complement to traditional experimental techniques as a tool for validating and guiding theoretical developments in science, as well as practical advances in technology and engineering. This research will improve our ability to accurately and efficiently compute the long-time behavior of complex systems, which is a fundamental aspect of the rational design of pharmaceuticals and high-performance composite materials. In addition, it has the potential to accelerate the pace of technological development by allowing the rapid prototyping of new and innovative industrial designs directly on the computer."
"0915220","Optimization, Differential Equations and Applications","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/18/2009","Philip Gill","CA","University of California-San Diego","Standard Grant","Junping Wang","08/31/2011","$290,000.00","Randolph Bank, Michael Holst","pgill@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","0000, 9263, OTHR","$0.00","The investigators conduct research on several fundamental topics in computational science, which include optimization methods, numerical optimal control, parameter estimation, inverse problems, evolution partial differential equations with constraints, and parallel adaptive finite element methods.  A feature common to all of these topics is the parallel implicit solution of optimization problems with ordinary and partial differential equations constraints.<br/>Many fundamental physical systems are described by (generally nonlinearly) constrained ordinary or partial differential equations of motion (also generally nonlinear) in the sense that exact solutions to the equations of motion always satisfy a given set of constraints.  Examples include Maxwell's equations, the incompressible Navier-Stokes equations, the Yang-Mills equations, and Einstein's equations, among others.  Accurate discretizations of differential equation constraints lead to very large sparse constrained optimization problems, where much of the structure reflects the discretization.  The problems are highly nonlinear, with reliability of the optimization being the dominant factor in the formulation of successful methods.<br/><br/>The optimization of functions subject to differential equation constraints arises in many contexts in engineering and scientific computation, since physical reality is often expressed through models involving ordinary and partial differential equations. The investigator's research program is motivated and guided by some challenging applications of computational science in engineering and science. These applications include the design of large neurobiological network models, off-shore petroleum exploration, the numerical modeling of gravitational waves, and trajectory planning for spacecraft and unmanned autonomous vehicles.<br/>In addition, the investigators are active in the development and dissemination of computer software that embodies the advanced computation techniques developed as part of their research program.  Software developed by the investigators provides an effective method of technology transfer and provides US scientists and engineers with instant access to state-of-the-art methods.  Within the Center for Computational Mathematics, the Investigators offer a program of instruction and research that emphasizes the role of computational science in the formulation, modeling, and solution of problems from diverse and changing areas.  The program of research conducted by the investigators helps to attract advanced graduate students into the area of computational science, which plays a vital role in the study of systems arising in manufacturing, engineering and the natural sciences."
"0915160","Collaborative Research:  Multi-manifold data modeling: theory, algorithms and applications","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","06/06/2011","Ery Arias-Castro","CA","University of California-San Diego","Continuing Grant","Leland Jameson","08/31/2014","$110,190.00","","eariasca@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","0000, 9263, OTHR","$0.00","The object of this proposal is the analysis of existing methods, and the development of new ones, for the task of multi-manifold learning, where the data is assumed to be comprised of low-dimensional structures. The main focus will be in studying the potential of spectral methods for clustering and modeling of low-dimensional surfaces embedded in high dimensions; in designing new spectral-based approached to the task of detection of low-dimensional objects in point clouds; and the analysis of popular manifold learning algorithms, especially in terms of robustness to outliers. A number of applications will be specifically addressed, for example, motion segmentation, structure from motion, classification of face images, segmentation of diffusion tensor images and the characterization of cosmological models in astrophysics. <br/><br/>Modern high-dimensional datasets often exhibit low-dimensional structures. Such situations arise in image processing; e.g., in target tracking, where a typical trajectory defines a curve through successive frames; and also in medical imaging, e.g., in the examination of vascular networks. The study of the galaxy distribution, which contains filamentary and sheet-like structures, is another example. Traditional methods are known to be ineffective in this context and the last decade has seen a massive amount of research aiming at improving on these classical tools. A number of approaches for multi-manifold modeling have been suggested, mostly by computer scientists and engineers. Applied mathematicians and statisticians have different perspectives to <br/>offer and their contribution is needed, not only in designing new algorithms but also (and perhaps especially) in providing theoretical foundations, which researchers in the field have been asking for. The research in this proposal will address both issues, developing rigorous mathematical theory combined with carefully designed numerical strategies addressing specific applications, such as motion segmentation, structure from motion, classification of face images, medical imaging and the characterization of cosmological models in astrophysics. The PIs will share their findings through publications and software, all available online to the scientific and engineering communities."
"0838692","A Sage/SciPy Developer Workshop: Special Functions and Computational Number Theory Meet Scientific Computing; Austin, TX","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","02/01/2009","02/02/2009","Fernando Rodriguez-Villegas","TX","University of Texas at Austin","Standard Grant","Junping Wang","01/31/2010","$9,000.00","Irene Gamba","villegas@math.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1264, 1271","0000, 9263, OTHR","$0.00","This workshop is an instance of SAGE Days (number 11 to be precise), a<br/>periodic gathering where developers of the software package SAGE, a<br/>free and open software that supports research and teaching in algebra,<br/>geometry, number theory, cryptography, and related areas, address<br/>current issues of the software, program for very long hours, either<br/>improving existing code or adding new code, and plan the next steps to<br/>be taken. For example, this workshop includes working on adding to<br/>SAGE the capability to work with modular forms over function<br/>fields. In addition to the above standard features of such a meeting,<br/>this particular workshop includes a strong emphasis in the interaction<br/>betwen algebra and number theory (the area of the bulk of the SAGE<br/>developers and researchers in attendance) and computer science,<br/>supercomputing (from TACC, the Texas supercomputing center) and<br/>applied Mathematics (from ICES, the Institute for Computational<br/>Engineering and Sciences) at the University of Texas at Austin, as<br/>well as researchers working outside academia (from Entought, an Austin<br/>based company).<br/><br/><br/>The main goals of this workshop are: (i) to support the development of<br/>the free and open source software SAGE by gathering some of its<br/>developers as well as researchers that use it in their own research;<br/>(ii) to bring together the SAGE commnunity and various research<br/>groups at the host location, the University of Texas at Austin, that<br/>while might have very different research focus share many common<br/>computational problems and needs. To give just one example, being<br/>able to solve large linear algebra problems accurately and fast is a<br/>fundamental problem that underlies essentially any type of scientific<br/>computation imaginable. The availability of high level software is<br/>nowadays of crucial and increasing importance for research in<br/>Mathematics, both in applied and pure Mathematics. SAGE is an example<br/>of a be very powerful computer package that is at the cutting edge<br/>(in terms of the sophistication of its capabilities as well as its<br/>speed) of existing tools for research in Mathematics and its<br/>applications; particularly, in those areas related to algebra, number<br/>theory, geometry and cryptography. The workshop is planned with the<br/>SAGE development model in mind, distinguished by an emphasis on<br/>openness, community, cooperation, and collaboration."
"0844775","CAREER: Linear Matrix Inequality Representations in Optimization","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","07/04/2009","Jiawang Nie","CA","University of California-San Diego","Standard Grant","Junping Wang","08/31/2015","$500,445.00","","njw@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","0000, 1045, 9263, OTHR","$0.00","This proposal investigates the linear matrix inequality representations of convex sets and their applications in optimization problems. The work involves different kinds of mathematical tools like algebraic geometry, convex analysis, differential geometry, numerical analysis, optimization theory, and real algebra. The investigator not only studies the fundamental mathematics on the scope and depth of linear matrix inequality representability, but also work on designing new algorithms and software solving hard optimization problems.<br/>The following five main topics will be focused in this project: linear matrix inequality representations of rigid convex sets, semidefinite programming representations of convex semialgebraic sets, second order cone programming representations of convex semialgebraic sets, semidefinite programming representations of nonnegative multivariate polynomials, and linear matrix inequality methods for solving nonconvex optimization problems and polynomial systems.<br/> <br/> <br/>A basic problem of science and engineering is finding a global minimum of a function of many variables. As a metaphor one might think of a complicated terrain of  mountains and valleys which stretches for hundreds of miles and one must find the lowest point in the lowest valley. The difficulty is that one can not see the map and one only knows a mathematical formula for the terrain and in most applications (like electronics, networks, biochemistry) there are many variables instead of three. Many algorithms will find the lowest point of a particular valley but none are known which effectively find the lowest valley itself. This NSF research is to develop global optimization algorithms for various situations. One is the class of problems where the data is given by polynomials. Another is to determine and parameterize convex problems very efficiently; in convex situations one has only one valley. These pursuits require integration of techniques from numerical mathematics, real and complex algebraic geometry, convex analysis, differential geometry, numerical analysis, and optimization theory, a wide range of mathematics. Jiawang Nie has personal experience with several areas of applications including sensor networks and systems control and this informs his mathematics and techniques. Other important features of this proposal are integrating research and education, developing new mathematical courses, training undergraduate and graduate students on using the latest mathematical tools, advising postdoctoral scholars on how to create novel research results.<br/>"
"0915223","Collaborative Research: Computational Methods for Coupled Wave, Current, Sediment Transport and Morphological Evolution","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","07/11/2011","Clinton Dawson","TX","University of Texas at Austin","Continuing Grant","Leland Jameson","08/31/2013","$279,188.00","","clint@ices.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","0000, 9263, OTHR","$0.00","The goal of this project is to significantly advance the capability to accurately predict sediment transport and seabed morphology in three-dimensional coastal and estuarine environments. To achieve this goal, state-of-the-art algorithms for fully coupled wave, current, sediment transport and bed morphology will be developed, analyzed, and implemented.  The development of this system will require a better mathematical and physical understanding of the tightly coupled nature of the various processes involved. In addition, it will require computational strategies which address the accurate and efficient coupling of interdependent processes that exhibit a wide range of spatial and temporal scales?both within the fluid motion itself and between the fluid and bed motion.  Robust and highly parallelizable algorithms for the various processes will be developed based on discontinuous Galerkin finite element methods, and the coupling of these algorithms will be carefully investigated in order to maintain numerical accuracy, efficiency, and local mass conservation of sediment and fluid phases.<br/><br/>Studying and predicting the morphodynamics of the coastal zone requires a detailed knowledge of winds, waves, currents, sediment transport and, ultimately, the resulting morphological changes of the seabed that occur as a result of these processes. The erosion and deposition of bed sediment can have a major detrimental impact on the coastal population, infrastructure and environment.  For example, during Hurricane Katrina, four major levee breeches that occurred during the storm were a result of foundation-induced failures caused by scour.  The transport of sediment is closely tied to a number of other issues in the coastal zone, including water quality and related ecological concerns, beach and shoreline erosion, and the maintenance of navigation channels and harbors through dredging activities.<br/>Accurate estimates of expected sediment transport and bed morphological changes can aid greatly in the long-range planning and management of coastal and estuarine environments.  In this project, the investigators will develop a fully coupled wave, current, sediment transport and bed morphology model system.  Such a system will significantly advance the capability to accurately predict sediment transport and seabed morphology in coastal and estuarine environments.<br/>From this research, a better scientific understanding of the complex interrelations among hydrodynamic, transport and morphodynamic processes in the coastal zone will emerge, which can lead to more informed decision-making that will help protect the coastal population and infrastructure. The developed software of the project will also provide a computational infrastructure that can be used in many other applications within the area of computational modeling. Furthermore, the technology developed under this project will be disseminated to government agencies such as NOAA and the US Army Corps of Engineers.<br/>"
"0914840","Dynamic Visibility and Inverse Source Problems in Unknown Environments with Complicated Topology.","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","06/06/2011","Yen-Hsi Tsai","TX","University of Texas at Austin","Continuing Grant","Leland Jameson","08/31/2013","$240,043.00","","ytsai@math.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","0000, 9263, OTHR","$0.00","This proposal contains a research program that systematically covers a novel  source discovery problems involving dynamic visibility, the Poisson equation, the heat  equation, and the wave equations posed in complicated, non-simply connected domains.  These problems are motivated by robotic path planning applications. Assuming sparse and sequential measurements of data, the PI and collaborators propose new robotic path algorithms along which new measurements can be added in order to efficiently determine plausible source locations and to reach particular vantage points such that the plausible source locations are under visual surveillance. The PI further studies situations in which obstacles in the domains are partially known through the visibility along the robotic path.<br/><br/><br/>Consider the situation in which a robot, sent into an unknown environment, is supposed to discover the location of a signal source and place it under its line-of-sight in an  efficient manner. The unknown environment contains non-penetrable solid obstacles and  should be avoided along the robot's path. In this environment, the properties of the signal, such as the signal strength, are assumed to satisfy certain  mathematical equations.  The robot gathers measurements from two different sensors: a range sensor that  gives distance from the robot to the surrounding obstacles, and a sensor that measures  the signal strength that is being emitted from the yet-to-be-located source.  While measurements can be taken anywhere,  the PI and the collaborators are interested in having the robot take very few measurements with its sensors.  This consideration is particularly relevant to efficient surveillance and anti-terrorism applications. The goal is to design an robust algorithm that determines how the robot should navigate through the environment and where along its path it should take measurements so  that the ob jective of discovering signal sources and their surveillance can be achieved  efficiently."
"0846501","CAREER: Fast Algorithms for Oscillatory Integrals","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","09/13/2009","Lexing Ying","TX","University of Texas at Austin","Standard Grant","Junping Wang","03/31/2013","$415,017.00","","lexing@math.stanford.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","0000, 1045, 9263, OTHR","$0.00","Many challenging computational problems related to high frequency wave phenomena can be formulated mathematically as integral equations and transforms with oscillatory kernels. The PI proposes to leverage the ideas of multidirectionality and butterfly computation to develop fast and accurate algorithms for oscillatory integral equations and transforms, with targeted applications in acoustic and electromagnetic scattering, numerical wave propagation, and reflection seismology. The proposed research activities include: (1) fast and parallel algorithms for the $N$-body problems and the boundary integral equations of high frequency acoustic and electromagnetic scattering, (2) an optimal complexity algorithm for computing Fourier integral operators with applications in high frequency wave propagation and seismic migration, (3) an optimal complexity algorithm for sparse Fourier transform where the spatial and Fourier samples are supported only on a low dimensional manifold, and (4) an optimal complexity algorithm for partial Fourier transform where the frequency summation is restricted to a spatial dependent domain. The education part of includes the following components (1) mentoring students and postdocs through participating the proposed research activities, (2) curriculum development through developing a new course that focuses on the fast algorithms in multiscale and multidirectional computation and publishing survey papers on these topics, and (3) organizing summer schools and lecture series that aim to present students with recent developments in computational mathematics.<br/><br/>Through developing robust and accurate numerical algorithms with optimal complexity, this research will significantly improve our ability of understanding large scale physical problems of oscillatory nature. The algorithms to be developed in the proposed research activities will have direct applications in acoustic and electromagnetic scattering, reflection seismology, and medical imaging. The PI will also work closely with researchers from industrial and government laboratories to disseminate ideas and deliver operational softwares for realistic challenging applications. The development of a new generation of numerical algorithms and softwares requires researchers to understand different aspects of computational mathematics. The research and educational components will integrate together to (1) help train a new generation of researchers who master algorithmic design, mathematical analysis, and software development, and (2) promote the awareness and interests in computational mathematics among undergraduates and underrepresented groups (female and minority students).<br/>"
"0914825","Numerical Methods for Inverse Problems of the Linear Boltzmann Transport Equation","DMS","COMPUTATIONAL MATHEMATICS","07/15/2009","07/10/2009","Kui Ren","TX","University of Texas at Austin","Standard Grant","Leland Jameson","06/30/2013","$180,584.00","","kr2002@columbia.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","0000, 6890, 9263, OTHR","$180,584.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The objective of this project is to develop fast and robust numerical reconstruction methods for linear and nonlinear inverse transport problems appeared in many application areas. The main approach proposed is to incorporate a priori information into optimization-based algorithms that have been developed in the past. The a priori information includes both knowledge on the unknowns to be reconstructed and knowledge on numerical methods for forward transport problems. More precisely, the proposed research include: (1) to develop methods that utilize a priori information to reduce the number of unknowns in the reconstruction, and to reconstruct features in the object of interests; (2) to accelerate the reconstruction process by analyzing the structure of the forward transport problem so that larger set of data can be used in the reconstruction process; and (3) to develop efficient Bayesian computational methods for uncertainty quantification in transport-based imaging problems.<br/> <br/>The proposed research lies between numerical mathematics and applications. From computational point of view, developing fast, robust and accurate reconstruction algorithms for ill-posed transport-based inverse problems is very challenging, not only because of the advanced numerical optimization, numerical partial differential equations techniques it involves, but also because of the fact that a deep understanding of the theory of ill-posed inverse problems is required. Many ideas developed in this proposal should have straightforward applications in numerical solutions of other model-based inverse problems. From application point of view, inverse transport problems find applications in various areas such as medical imaging (mainly optical tomography and optical molecular imaging), detection and imaging in random media, and atmospheric optics. The proposed research can potentially has long-term impacts in those application areas, improving both the stability and the accuracy of those imaging methods.  Some of the ideas and techniques developed in this project will be incorporated into a graduate level class on numerical methods for inverse problems which presumably will benefit graduates who are interested in applying mathematical and computational techniques to solve real world problems.<br/>"
"0921015","Modeling and Simulation of Microbial Fuel Cells","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","09/01/2009","08/30/2009","David Chopp","IL","Northwestern University","Standard Grant","Mary Ann Horn","08/31/2013","$252,888.00","","chopp@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1271, 7334","0000, 7237, 9263, OTHR","$0.00","This project would use mathematical modeling and computer simulations to help evaluate the potential capabilities of a commercial scale implementation of microbial fuel cells.  The mathematical modeling we will employ is a continuum model using a combination of the level set method with the extended finite element method for solving a coupled system of reaction diffusion equations that incorporates the diffusion of various elements such as oxygen, substrates, and byproducts, as well as the growth of biomass consisting of multiple bacterial species attached to a fixed surface, or substratum.  We have successfully applied this strategy to other biofilm systems such as Pseudomonas aeruginosa, a common bacterium often associated with mortality in people with cystic fibrosis, and heterotroph/autotroph symbiotic systems present in activated sludge water treatment processes.  In this project, we will develop the necessary reactions and growth processes to simulate the microbial fuel cell system and use it to study various control strategies for optimizing energy production.<br/><br/>Microbial fuel cells are an attractive potential alternative energy source that are currently only at an experimental stage.  These systems take waste water streams, e.g. pig manure, and convert them directly into electricity without the use of combustion.  At the same time, the water is being cleaned of harmful elements such as ammonium, that is one stage of a comprehensive water treatment process currently in use.  It is estimated that microbial fuel cells have the potential to generate as much as 25% of the current worldwide power demand, all while using a negative cost energy source in waste water.   <br/>Experimental systems are currently limited to bench scale reactors in closed systems that have a limited lifespan. To take these systems to the commercial scale, these systems need to be better understood for potential power per cost to demonstrate that they are feasible on that scale."
"0915178","Development of fast and high-order accurate time-domain PDE solvers for complex geometries","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","08/02/2009","Mark Lyon","NH","University of New Hampshire","Standard Grant","Junping Wang","07/31/2012","$47,003.00","","mark.lyon@unh.edu","51 COLLEGE RD SERVICE BLDG 107","DURHAM","NH","038242620","6038622172","MPS","1271","0000, 6890, 9150, 9263, OTHR","$47,003.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/><br/>The project develops fast and high-order accurate time-domain Partial Differential Equation (PDE) solvers for complex geometries, in particular focusing on unconditionally stable methods that utilize advanced Fourier series techniques for boundary conditions involving normal derivatives. A recently developed methodology uses a fast Fourier continuation method which effectively extends a non-periodic function to a periodic function over a larger domain. This high-order accurate approximation allows for the fast evaluation of the function and its derivatives by means of Fast Fourier Transforms. This Fourier Continuation is utilized within an Alternating Direction scheme in the new methodology for PDE solution, FC-AD, which is high-order accurate, is unconditionally stable, and can be directly applied to complex domains without prohibitive domain mappings or coordinate transformations. These FC-AD techniques have the potential to be transformative and produce solutions to problems that would otherwise be intractable, in particular, for any type of wave propagation problem (Radar/Sonar, Ultrasound, Communications ...) where traditional techniques are limited by ?pollution? type errors. Due to the alternating directions though, the application of the FC-AD techniques to problems with normal derivatives provides a significant challenge that this project seeks to overcome. The boundary conditions will be incorporated using fast boundary integral equation methods at each time step on a sub-problem and then incorporated back into the full solution. The resulting coupled methods will represent a broad class of general high-order solvers that are applicable to a range of PDEs including non-linear problems, where boundary integral methods alone will not work. The development will include the consideration of several important applications, including the computation of stress concentrations.<br/><br/>Significant problems remain unsolved because current algorithms are not yet efficient enough to obtain sufficiently accurate solutions. The new computational tools being developed in this project provide a new more efficient means to solve some of the basic equations used in scientific models and to be able to solve them efficiently on the complex geometries encountered in real life. Due to the FC-AD?s ability to solve PDEs directly on complex geometries without domain mappings or the costly mesh generation of standard techniques, it has the significant potential to make a contributions in many scientific fields. By providing numerical solutions with better accuracies, a greater understanding of the underlying physical processes can be developed, better predictions can be made from the models, and engineering designs can be further optimized to provide greater safety or reduce costs. Potential application areas include: Radar/Sonar, Biomedical applications including cancer treatments, Communications, Stress and Failure analysis, and Fluid Dynamics applications including pollution transport and aspects of climate change."
"0946656","Conference on Computational and Mathematical Methods in Science and Engineering, CMMSE 2010; May 2010, Madison, Wisconsin","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/13/2009","Bruce Wade","WI","University of Wisconsin-Milwaukee","Standard Grant","Rosemary Renaut","08/31/2010","$22,016.00","Shi Jin","wade@uwm.edu","3203 N DOWNER AVE","MILWAUKEE","WI","532113153","4142294853","MPS","1271","0000, 7556, 9263, OTHR","$0.00","The Principle Investigator and his colleagues propose to organize the international conference Computational and Mathematical Methods in Science and Engineering (CMMSE 2010) on May 26--28, 2010 at the University of Wisconsin-Madison, Madison, Wisconsin.  This is the tenth in a series of conferences, with previous meetings held in Chicago, Illinois and Milwaukee, Wisconsin, USA; Kastoria, Greece; Alicante (twice), Gijon, Madrid, and Murcia, Spain; and Uppsala, Sweden.<br/>CMMSE 2010 is organized on the principle that excellent applied mathematics of the Keynote Plenary Speakers and a core of  10-20 Special Sessions attracts participants from a wide variety of related specialties to share their ideas and results for the latest advances of computational and applied mathematics.  Presentations center around the  themes of the special sessions, and there will be poster sessions, opportunities for emerging researchers and a session on future directions led by experts in the various fields of research. <br/>Extensive efforts will be extended to attract significant participation from graduate students and emerging researchers, especially women and minorities.  Graduate students and young researchers will have opportunities to present their work and learn about open problems, thus enhancing their careers.<br/><br/><br/>In the midst of the various successful annual conferences there remains a niche for unifying, cross-cutting, interdisciplinary gatherings, where specialists can have exposure to diverse fields, a chance to meet new people in or near their individual areas of research, and participate in special sessions different from, but still close to, their own interests.  The CMMSE conference series fulfills that important role in the field of computational mathematics. Computational mathematics, numerical analysis, and mathematical modeling have come to be regarded as an incredibly valuable part of the scientific process, complementing in an essential way theory and experiment.  Computational simulation truly enhances the study of  complex systems of  science and engineering to enable<br/>breakthroughs for modern technology.   In seeking better and better results from simulations, the computational mathematician or<br/>scientist needs not only  capacity to carry out simulations, but also innovative ideas and foundational analysis for algorithms. <br/>This conference represents a rapidly growing multidisciplinary approach in scientific research with connections to business, economics, the sciences, engineering, mathematics and computer science through academia as well as industry."
"0915006","Time-Spectral Method for Unsteady Viscous Flow on Moving and Deformable Grids with the High-order Spectral Difference Method","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","08/02/2009","Antony Jameson","CA","Stanford University","Standard Grant","Leland Jameson","07/31/2012","$474,394.00","","antony.jameson@tamu.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1271","0000, 9263, OTHR","$0.00","This project extends the high-order spectral difference (SD) method for three-dimensional compressible viscous flows to systems with moving boundaries and deformable grids, and also to combine it with the time-spectral (TS) method to treat periodic unsteady flows. Compared to conventional time accurate methods, the SD-TS method has the potential to significantly reduce the computational cost of simulating time periodic flows. The extension to moving boundaries is needed to enable application of the high order SD methodology to perform accurate simulations of numerous devices in energy and transportation systems, such as wind turbines, rotorcraft and autonomous flapping wing micro air vehicles. Recent development of the SD method by the principal investigator and his colleagues has confirmed it accuracy, robustness and efficiency in dealing with high Reynolds number turbulent flows. The SD method offers a great flexibility in choosing optimal spatial discretization by varying the polynomial order. A baseline SD code has been developed based on quadrilateral/hexahedral grid elements. An element splitting algorithm has also been developed to partition each triangular/tetrahedral element into three or four quadrilateral/hexahedral elements. This enables the use of general grids with mixed elements. For time-dependent high-Reynolds number problems, implicit Lower-Upper Symmetric Gauss-Seidel (LU-SGS) time stepping approach has been developed in conjunction with a p-multigrid method to speed up convergence of the SD solver.  In order to achieve the above goal of treating devices as complex as a rotating wind turbine, the proposed research must address several major challenges. The main tasks being undertaken in the ongoing research by the investigator and his colleagues are 1) Extension of the SD method to moving and deformable grids by transforming the Navier-Stokes equations on a moving physical domain to a fixed reference domain by a blended mapping technique; 2) Parallelization of the three-dimensional solver using MeTis for domain decomposition and MPI for message passing; 3) Development of non-conforming hexahedral elements with hanging nodes to allow geometric flexibility and variable order; 4) Implementation of the Time Spectral method to reduce the computational cost of simulating periodic time dependent flows. <br/><br/>The numerical simulation techniques being developed in this project are crucial to advancing technology in a wide range of energy and transportation systems, with significant potential for reducing environmental impact. Many such systems require simulations of flows with moving boundaries. An immediate target of the research is to improve the state of the art in wind turbine design. The importance of sustainable energy both to reduce U.S. dependence on imported oil supplies and to reduce environmental damage due to fossil fuels is by now widely recognized. Wind power is a resource with tremendous untapped potential. Existing commercial flow simulation codes use low order methods which are too numerically dissipative to allow accurate tracking of the vortex wake which are crucial to wind turbine performance. The high order methods which will result from this project will provide a basis for the systematic future development of superior wind turbine designs. Potential applications to transportation systems which could have significant economic and environmental benefits include drag reduction of road vehicles, both passenger cars and trucks, and improvements in the efficiency and reduction of the acoustic signature of gas turbines and rotorcraft, both of which incorporate moving blades.<br/>"
"0921004","Collaborative Research: Mathematical Studies and Refinements of a Reduced Ion Channel Model and a Nonlocal Dielectric Model","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","09/01/2009","09/11/2009","Dexuan Xie","WI","University of Wisconsin-Milwaukee","Standard Grant","Mary Ann Horn","08/31/2012","$265,417.00","","dxie@uwm.edu","3203 N DOWNER AVE","MILWAUKEE","WI","532113153","4142294853","MPS","1271, 7334","0000, 7237, 9263, OTHR","$0.00","Ion channels are critical for biological function and also represent remarkable electronic devices of independent interest. They facilitate and regulate the flow of charged ions in and out of cells, and they do so with precise behaviors that are not fully understood. The more accurate models and more efficient numerical algorithms will significantly advance the research on ion channels and other important proteins. In this project, the PIs plan to study models for two ion channels (sodium and calcium channels) to understand the role of dielectric models in predicting the mechanism of ion channels. In particular, the PIs will build upon an existing model that uses a very simple, almost naive, yet remarkably effective, model of dielectrics. They then will analyze this model mathematically to further reveal its properties and range of applicability. Moreover, they will study a more sophisticated model of dielectrics, commonly known as a non-local model. In addition, this project will provide multi-disciplinary training opportunities to both graduate and undergraduate students.  <br/>     Fast computation of numerical solutions of ion channel models including nonlocal continuum solvent models is essential for quantitative understanding of critical physiological processes and cellular metabolism and energetics, which is central to improving our understanding of health and diseases. The new ion channel models, nonlocal continuum solvent models, and efficient numerical algorithms developed in this project will be a considerable contribution to mathematical biology, computational physiology, and computational mathematics. They will play important roles in studying various ion channels, computing continuum electrostatics for solvent-solute systems, and simulating large scale protein systems. <br/>"
"0914514","Applied Numerical Mathematics of Structured Multiscale Models of Biological Systems","DMS","COMPUTATIONAL MATHEMATICS","10/01/2009","09/21/2009","Bruce Ayati","IA","University of Iowa","Standard Grant","Leland Jameson","09/30/2014","$67,920.00","","bruce-ayati@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1271","0000, 9263, OTHR","$0.00","The investigator proposes to develop computational methods and software to solve systems of partial differential equations that arise in multiscale models<br/>of various biological systems.  The behavior of these systems depends not only on time and space, but also on physiological traits such as size or age. The methods decouple time from the physiological variables. In doing so they, unlike previous methods, prevent the often fine-resolution time discretization from inflating the<br/>number of nodes in the physiological variables.  This family of methods remains the one with the highest-order accuracy due to the fact that approximation error is the only meaningful  source of error in the computations. Collaboration with biologists is a critical component of this research plan. The software will be developed and used in the context of models developed and parameterized with biofilm researchers at Montana State University and cancer biologists at Vanderbilt University, with the expectation that they will have a significant and near-term impact on understanding the mechanisms of biofilm growth and tumor invasion. This collaboration is also vital to ensure the relevance and usefulness of the methods and software; methods developed in an application vacuum tend to lack utility.<br/><br/>The investigator proposes the development of computer software and algorithms -- the mathematical rules that determine how the software works -- to study how<br/>biofilms develop and how cancer tumors, in middle and later stages of their development, invade nearby tissue.   Biofilms are bacteria that live in a material casing of their own creation.   They are involved in many of the most difficult to treat infections and in industrial fouling, and are used in wastewater remediation.   The software will handle complicated situations where individual cells within the biofilm or tumor differ in some important respect.   For example, different bacteria in a biofilm will have different abilities to grow and divide, based on the number of divisions already undertaken.  The different stages in the cell-division cycle of cancer cells are linked to the physically larger complete tumor.  This is important, for example, in studying the overall effects of chemotherapy when using drugs that affect cells differently depending on what part of the cell-division cycle they are in.  The mathematics behind the algorithms in this proposal are significantly more advanced than what came before and remain the state of the art, resulting in software that can be used effectively with the computers of today and the near future.  They can also be expected to be useful for problems other than biofilms and tumors.   An important part of this research proposal is the collaboration between the investigator and colleagues at Montana State University in Bozeman, MT and Vanderbilt University in Nashville, TN.<br/>"
"0915183","Anderson Acceleration for Fixed-Point Iteration","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/13/2009","Homer Walker","MA","Worcester Polytechnic Institute","Standard Grant","Junping Wang","08/31/2012","$210,000.00","","walker@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","MPS","1271","0000, 6863, 9263, OTHR","$0.00","This research will focus on acceleration methods for fixed-point iteration that are based on a method introduced by D. G. Anderson in 1965. This method has been used widely and with considerable success within the computational physics, chemistry, and materials communities as a means of accelerating the self-consistent field iteration used in electronic structure computations.  However, it has been untried or underexploited in many other important applications in which it seems likely to be equally successful. Moreover, it has received relatively little attention from mathematicians and numerical analysts, despite there being many significant unanswered mathematical questions. The goals of this research are to analyze the convergence of the method, to explore its effectiveness across a broad range of important applications, and ultimately to develop extensions with improved global convergence and stability properties.<br/><br/>The method to be investigated is at present an important method for accelerating computations used in materials science, for example, to determine properties of ""designer"" materials in nanotechnology applications. In practice, the method is usually very effective but occasionally fails. The first goal of this research is to develop a theoretical understanding of the method that explains this behavior and points the way to methods that are more consistently successful.  <br/>The method is general in nature and can potentially be much more broadly applied. The second goal of this research is to determine the effectiveness of the method in a variety of important applications, ranging from statistical estimation to computational modeling of complex physical phenomena. If successful, this research can result in much faster computational methods for challenging tasks such as extracting information from extremely large data sets and simulating fluid flow coupled with chemical reactions."
"0914809","The Flat Norm for Shapes and Images","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","07/15/2011","Kevin Vixie","WA","Washington State University","Continuing Grant","Leland Jameson","08/31/2012","$275,963.00","Thomas Asaki","vixie@speakeasy.net","240 FRENCH ADMINISTRATION BLDG","PULLMAN","WA","991640001","5093359661","MPS","1271","0000, 9263, OTHR","$0.00","In this project, the Principal Investigator and his collaborators pursue an integrated program of research and education aimed at elucidating and exploiting their recent discovery connecting the flat norm from geometric measure theory and the Chan-Esedoglu (CE) variational functional from image analysis. The several threads in the research program cover application of the new multiscale flatnorm to the study of shapes, integrated with supporting theoretical and computational work. The central discovery making this program possible is the realization by Vixie and his collaborator Simon Morgan that (1) minimizers of the CE functional translate directly to minimizers for the flatnorm decomposition, (2) this identification generalizes both the flat norm and the CE functional and (3) the minimal values are the same.  This opened the way for (1) denoising of non-boundary and/or higher codimension sets (a generalization of the CE functional), (2) a multiscale flat norm (a generalization of the flatnorm), and 3) a practical way to calculate the flat norm through methods for the CE functional. Taken together, they present us with new methods to analyze shape and image data. This research is a part of a larger program promoted by the PI and collaborators in which insights and innovations in geometric measure theory and geometric analysis are exploited in the pursuit of solutions to various data challenges.<br/><br/>In less technical terms, the investigator and his collaborators are developing new ways to represent and characterize image and image-like data. The technical aspects of the research program are both very interesting and amenable for the training new generations of mathematicians who are capable of both serious technical contributions and practical work on pressing problems related to the information and data sciences. The mathematics, arising from an area originally invented to study minimal surface problems related to question ""What shape will a bubble be if we dip this wire into a soap solution?"", turns out to have practical applications to the understanding of shapes and images. The investigators focus on both the theoretical developments that support practical applications and the development of practical algorithms for computing the multiscale flatnorm, the centerpiece of this project. Applications of this new multiscale flatnorm extends into many areas like image and shape denoising, shape recognition and the detection of anomalies in image streams. It is generally accepted that intelligent, efficient extraction of information from massive data streams is a very important, largely unsolved problem. Examples of such data streams include hyperspectral imagery from satellites and image streams from extended time high resolution microscopic observations of dynamic, living systems, to name just two. The tools being developed in this project promise new ways to understand and analyze such large scale data sets.<br/>"
"1011738","Efficient Numerical Methods for Viscous Incompressible Flows","DMS","COMPUTATIONAL MATHEMATICS","11/18/2009","04/16/2010","Jian-Guo Liu","NC","Duke University","Continuing Grant","Junping Wang","06/30/2012","$121,236.00","","jliu@phy.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1271","0000, 9263, OTHR","$0.00","This project will address the development of a class of novel and very efficient numerical methods for incompressible flows based on a reformulation of the Navier-Stokes equations that has been proved well-posed and provides boundary conditions for the viscous contribution to pressure in terms of vorticity circulation.  The main insight is that the total creation of boundary vorticity can be computed exactly by a commutator between the Laplacian and Helmholtzprojection operators. Moreover, this term is dominated by the viscosity term, hence we can treat it explicitly to gain efficiency and <br/>stability. The major advance is that the method can be formulated in standard continuous finite element space, and there is no compatibility condition needed for the velocity and pressure approximation spaces. Hence the standard fast solver can be applied to Navier-Stokes equation directly.<br/><br/>Accurate, efficient simulations of 3D flows in complicated domain are still  major challeges for many scientific and engineering problems. The resolution of the flows near physical boundaries is essential to the accurate prediction of the body force such as the lift and drag, shedding of vortex and boundary layer separation. The success of this project will have an important impact on many branches of science and engineering. It will give more accurate predictions of body force which will result in better energy efficiency for transportation related applications. It will provide fast and reliable simulations for scientific research and engineering application.<br/>"
"0914778","Collaborative Research: Theoretical and experimental approaches  to search problems in group theory","DMS","COMPUTATIONAL MATHEMATICS, Information Technology Researc","09/15/2009","09/11/2009","Vladimir Shpilrain","NY","CUNY City College","Standard Grant","Junping Wang","08/31/2012","$73,495.00","","shpilrain@yahoo.com","160 CONVENT AVE","NEW YORK","NY","100319101","2126505418","MPS","1271, 1640","0000, 9263, OTHR","$0.00","The objective of this proposal is to address various search problems in group theory. <br/>Decision problems  in group theory have been studied for over 100 years now, since Dehn put forward, <br/>in the beginning of the 20th century, the three famous decision problems now often referred to as <br/>Dehn's problems: the word problem, the conjugacy problem, and the isomorphism problem. In general, <br/>decision problems are problems of the following nature: given a property P and an input O, find out <br/>whether or not the input O has the property P. On the other hand, search problems are of the following <br/>nature: given a property P and an input O with the property P, find a proof (sometimes called a ""witness"") <br/>of the fact that O  has the property P. This is a substantial shift of paradigm, and in fact, studying <br/>search problems often gives rise to new research avenues in mathematics or computer science, very different <br/>from those prompted by addressing the corresponding decision problems.<br/><br/>The potential broader impacts of the proposed research are extensive; the impact on the general area <br/>of information security can be singled out. The difficulty of several well-studied problems, e.g. <br/>integer factorization and the discrete logarithm problem underlie most current public-key cryptographic <br/>protocols used in real-life applications.  Developing public-key protocols based upon other search problems, <br/>e.g. the conjugacy search problem whose difficulty has been well studied by group theorists, is prudent from <br/>the standpoint of robustness, particularly if factorization or related developments threaten the security of <br/>current protocols. The complexity of non-abelian infinite groups is a promising fertile ground for new <br/>protocols and there is a great deal of preliminary work required such as that proposed here.<br/>"
"0915107","Regularization Methods for Positron Emission Tomography","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/26/2009","Johnathan Bardsley","MT","University of Montana","Standard Grant","Leland Jameson","08/31/2011","$87,404.00","","bardsleyj@mso.umt.edu","32 CAMPUS DR","MISSOULA","MT","598120003","4062436670","MPS","1271","0000, 9150, 9263, OTHR","$0.00","It has been well-established in the positron emission tomography (PET) literature that statistical imaging algorithms for tracer density reconstruction are superior to fully analytic methods, such as filtered back projection. However, statistical imaging methods require some form of regularization, which is usually implemented in one of two ways: via the truncation of an iterative method applied to the maximum likelihood estimation problem, or by solving a penalized maximum likelihood (PML) problem. The investigator's focus in this proposal is on the later approach. PML methods are attractive because they allow for the incorporation of prior information via the choice of the regularization function. One of the investigator's main objectives in this proposal is the development of edge-preserving regularization functions for PET stemming from discretized diffusion operators. Edge-preservation in PET is important due to the fact that tracer densities change abruptly at tissue boundaries. However, a main issue with the PML approach is the need for methods for choosing the regularization parameter. A second main objective of this proposal is the development of regularization parameter choice methods for PET. Finally, a statistical analysis of the resulting reconstructions is a goal of the work.<br/><br/>Most Americans are familiar with the terms 'X-ray', 'CAT scan', and 'MRI'. Indeed, the use of machines to view the interior of the human body is so commonplace in this day-and-age that the challenging mathematics and computation behind their use is largely unknown and unappreciated. In this proposal, the investigator seeks to improve upon existing computational methods for the medical imaging modality known as positron emission tomography (PET). In PET, which is often used in cancer diagnostics, the patient ingests a substance called a tracer that concentrates in various regions throughout the body. The tracer then radioactively decays, resulting in the emission of photons (i.e. light) outside of the body. The PET machine counts the emitted photons and then reconstructs the amount of tracer at each location within the patient. In this proposal, the investigator focuses on the development of computational methods for PET that yield higher resolution reconstructions than current approaches.<br/>"
"0915068","Multiscale Modeling and Approximation in Novel Geometric and Nonlinear Settings","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","09/01/2009","Thomas Yu","PA","Drexel University","Standard Grant","Junping Wang","08/31/2012","$175,573.00","","yut@drexel.edu","3141 CHESTNUT ST","PHILADELPHIA","PA","191042816","2158956342","MPS","1271","0000, 9263, OTHR","$0.00","Multiscale data representation has been proven to be one of the most effective methods for representing data. Such methods are of major current interests not only in applied mathematics but also in computer science and engineering (especially the computer graphics and scientific simulation communities), and it is the job of applied mathematicians to answer (interrelated) questions such as: When do these methods work?<br/>How to fix them when they break? How to bring these methods to novel settings ? etc..<br/>The proposed projects develop various multiscale representations of data in novel geometric and nonlinear settings; such representations do for such data what wavelets were able to do for images and signals. The resulted multiscale representations are the key to data compression, feature extraction, noise removal and a number of other signal processing tasks that are key to informational technologies (computer graphics, computer-aided design, wireless communication, etc..), medical imaging technology (MRI and other kinds of radiology), military signal processing(sonar and radar etc.)<br/><br/>Our goal of analysis and synthesis of many new types of data fits right into the broad and fundamental goal of finding  efficient ways to organize and manipulate enormous and complex volumes of high-dimensional data.<br/>Such data analysis problems have gotten so ubiquitous and sophisticated throughout science, medicine, engineering that the need of applying abstract mathematical techniques becomes fruitful and inevitable.<br/>The project provides interdisciplinary research and training opportunities for graduate students, and stimulates collaboration among computational mathematicians, engineers and scientists."
"0914910","Diffeomorphic Deformation of Textured Shapes with Applications in Medical Image Analysis","DMS","COMPUTATIONAL MATHEMATICS","08/15/2009","08/11/2009","Yan Cao","TX","University of Texas at Dallas","Standard Grant","Leland Jameson","07/31/2013","$273,650.00","","yan.cao@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1271","0000, 6890, 9263, OTHR","$273,650.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The investigator and his colleagues will continue the study on diffeomorphic deformation of shapes. This framework supplies a rigorous mathematical metric on the shape space, a valuable quantitative measurement of the similarities between shapes. In the proposed project, the investigator and his colleagues will investigate diffeomorphic deformations of textured shapes. This study is to address the problems caused by boundary conditions in the current setup and response to the need of matching regions of interest directly in medical applications. Two problems will be studied: one is to build a rigorous mathematical framework for diffeomorphic deformations of textured shapes, another is to develop computational efficient algorithms for finding the shortest path connecting the textured shapes through the space of diffeomorphisms.<br/><br/>Changes in the anatomical structures occur during the development and aging process, as well as during many disease and injury process.<br/>Detecting and monitoring the anatomy changes are crucial in disease diagnosis, treatment and prevention. The results of this research will provide new image analysis tools for quantitative studies of variations of biological shapes, detecting and tracking changes of individual anatomy structures. They can also be applied to computer vision in object tracking, recognition and classification."
"0913017","Algebraic hierarchical matrix preconditioners for two- and three-dimensional saddle point problems","DMS","COMPUTATIONAL MATHEMATICS","07/01/2009","07/29/2011","Allan Mills","TN","Tennessee Technological University","Standard Grant","Junping Wang","09/30/2013","$137,149.00","","amills@tntech.edu","1 WILLIAM L JONES DR","COOKEVILLE","TN","385050001","9313723374","MPS","1271","0000, 6890, 9150, 9263, OTHR","$137,149.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>This project deals with the development, analysis and implementation<br/>of novel techniques for the solution of large, sparse linear systems<br/>of equations of saddle point type. Despite much recent progress,<br/>the solution of large systems of equations remains one of the main<br/>bottlenecks in many numerical simulations. Applications include<br/>fluid dynamics, magnetohydrodynamics, image processing, and many more.<br/>This project deals with the further development of the <br/>technique of hierarchical (H-)matrices. H-matrices <br/>provide an efficient technique for computations<br/>involving approximations to fully populated matrices.<br/>The standard construction of an H-matrix is based<br/>on the underlying geometry of the application. Similar to the generalization<br/>of geometric to algebraic multigrid methods, the PI proposes to <br/>develop an algorithm for the algebraic construction of H-matrix <br/>preconditioners for saddle point problems. Sequences of three-dimensional <br/>Oseen problems that result in the numerical solution of the Navier-Stokes <br/>equations will serve as the major application of the novel techniques.<br/>An additional topic to be considered in this project is<br/>the application of H-techniques in only recently developed<br/>Kronecker product preconditioners. The techniques of this proposal <br/>have a high potential to lead to robust and scalable blackbox solvers <br/>for large, linear systems arising in the simulation of scientific and <br/>technical problems of increasing size and complexity.<br/><br/><br/>H-matrices have first been introduced in 1998 and <br/>since then entered into a wide range of applications.<br/>The approach of H-matrices is of significant importance <br/>within its own field of numerical analysis and also with respect to<br/>practical large-scale computing challenges that scientists are <br/>currently facing. Examples for applications include models for <br/>magnetic fusion, accelerator design, electrochemical processes <br/>or the growth of ceramic nanostructures.<br/>Progress in the areas targeted in this proposal will have a <br/>positive impact on science and engineering by allowing for faster<br/>and more accurate computer simulations."
"0914974","Collaborative Research: RUI: Multilinear Algebra Computations with Higher-Order Tensors","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","06/12/2009","Carla Martin","VA","James Madison University","Standard Grant","Junping Wang","07/31/2013","$113,229.00","","martincd@jmu.edu","800 S MAIN ST","HARRISONBURG","VA","228013104","5405686872","MPS","1271","0000, 6890, 9229, 9263, OTHR","$113,229.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>Many real world applications need to compress, sort, and otherwise <br/>manipulate large volumes of multidimensional data arrays (i.e., higher-<br/>order tensors), so there is an increasing need for theoretical and <br/>computational tools to deal with multiway data.  The subject of <br/>multilinear algebra and tensors has gained increasing prominence in <br/>the past decade due to a surge in applications such as approximation <br/>of Newton potentials and stochastic PDEs, image compression and <br/>deblurring, network traffic analysis, biological assay interpretation, <br/>unmixing of signals, and many more.  Such applications with higher-order<br/>tensors involve factorizations of the tensor.  There are multiple <br/>possible extensions of matrix factorizations to higher-order tensors <br/>(e.g. extensions of the matrix SVD), with some more amenable to certain<br/>applications.  The investigators are advancing the state-of-the-art in <br/>both theoretical and computational multilinear algebra via several newly<br/>developed tensor constructions based on new notions of tensor <br/>multiplication, orthogonality and diagonalizability.  Algorithms with <br/>compression schemes based on these new constructions are being <br/>implemented by the investigators and tested on several datasets from <br/>various applications including handwritten digit identification, <br/>genomics, the spectral unmixing problem, video compression, and computer<br/>image recognition.<br/><br/>Current applications in the sciences can involve <br/>analysis, classification, searching and compression of large volumes of <br/>data that is ""multidimensional"" in nature.  Consider, for example, the <br/>problem of facial recognition, used to identify a terrorist from within <br/>a database of images of known terrorists. The database can be considered <br/>multidimensional data in the sense that for each individual there <br/>corresponds a specific viewpoint, illumination, and facial expression. <br/>It is critical in this scenario to have an accurate and fast algorithm <br/>to match an unknown image against a database of images of known <br/>terrorists. Another example where a multidimensional representation is <br/>useful is genomic data.  Here, DNA microarray two-dimensional tabular <br/>data from different experiments is concatenated into a multidimensional <br/>array.  Recently published results have indicated that so-called <br/>'factorizations' of this multidimensional data can be used to discover <br/>new molecular-level interactions.  Hence, along with advances in <br/>computer architecture to store large datasets must come mathematically <br/>sound models for the compression and/or analysis of such data.  <br/>Development of new concepts and ideas is therefore required to deal with<br/>the different geometries that arise in the multidimensional case.  The <br/>investigators are contributing directly to this effort by developing <br/>innovative mathematical theory for multidimensional objects that is <br/>consistent with two-dimensional proven ideas.  With theoretical <br/>constructs in place, the investigators are able to create computational<br/>tools and algorithms to analyze and compress multidimensional data.  A <br/>significant component of the proposal is the involvement of <br/>undergraduates, graduate students, and researchers.  The investigators <br/>are leveraging the strengths of both universities in a novel, <br/>inter-institutional vertical integrative experience for all students <br/>(undergraduate and graduate) involved in the research.  This arrangement<br/>allows students at all levels, mentored by leading researchers in the <br/>field, to advance the state-of-the-art in the analysis and compression<br/>of multidimensional data."
"0932948","Uncertainty Quantification for Systems Governed by Partial Differential Equations; May 2010; Edinburgh, Scotland","DMS","COMPUTATIONAL MATHEMATICS","10/01/2009","09/20/2009","Max Gunzburger","FL","Florida State University","Standard Grant","Junping Wang","09/30/2010","$44,100.00","","gunzburg@fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1271","0000, 7552, 7556, 9263, OTHR","$0.00","In deterministic modeling, complete knowledge of input parameters is assumed. This leads to simplified, tractable computations and produces simulations of outputs that correspond to specific choices of inputs. However, most physical, biological, social, economic, financial, etc. processes involve some degree of uncertainty. Uncertainty quantification (UQ) is the task of determining statistical information about the outputs of a process of interest, given only statistical (i.e., incomplete) information about the inputs. The particular focus of the workshop are processes governed by partial differential equations (PDEs). It has long been recognized that mathematical models need to account for such uncertainties. However, the science of UQ in many application areas is still in its infancy. There is much current activity in disparate areas of mathematics, statistics, science, and engineering that is relevant to UQ. However, fundamental and challenging mathematical issues remain unsolved, in particular combating the ""curse of dimensionality"" attendant to solving problems in high dimensions remains an unresolved issue. The workshop is meant to help ameliorate this situation. The workshop brings together experts in all areas of mathematics and statistics relevant to UQ as well as scientists and engineers working in application areas. The objectives of the workshop are as follows: to review developments in this rapidly developing field; to bring together internationally leading experts working in relevant fields of mathematics, statistics, and other areas and enable an effective dialogue between them; to expose industrial researchers to the recent developments in the field and mathematical scientists to the important problems facing industry; to promote communication between the various relevant mathematical disciplines (e.g., numerical analysis, probability theory, statistics, high-performance computing); to encourage junior researchers to work in the field; and to strengthen interactions between researchers coming from different areas of research. The workshop commences with three short courses that are meant to get everyone up to speed on the disparate aspects of UQ and stochastic PDEs considered in the workshop. Although the short courses are of value for everyone attending the workshop, they are especially valuable for junior researchers. The workshop closes with a session devoted to a discussion of future directions in stochastic PDE and UQ research with a special emphasis on the outstanding open problems that need to be solved in order to make stochastic PDE-based UQ a tool that is easily, routinely, and readily available to those in government and industry that have to make decisions in environments involving risk and uncertainty. <br/><br/>Uncertainty quantification (UQ) is the process of accurately assessing the uncertainties in predictions made by scientists and engineers about physical, biological, social, economic, financial, military, etc. processes. For example, predicting hurricane paths, the structural integrity of a bridge or airplane, future prices of financial instruments, and the lifetime to failure of military equipment are all subject to uncertainty. Thus, accurately quantifying that uncertainty is of paramount importance to engineers in the design process, to government officials when making policy decisions including those related to homeland security and military strategies, to response teams assessing dangers and remedies in natural and man-made disaster situations, and in many other settings. The workshop objective is to advance the state of the art of the science of UQ. The objective is met by bringing together mathematicians, statisticians, engineers, and scientists from universities and industry to exchange ideas and to develop new methodologies. A significant and effective transfer of knowledge to the users of scientific UQ is also affected. The organizers of the workshop are committed to include a diverse, with respect to rank, gender, age, and ethnicity, set of participants in the workshop. There is also a well-formulated plan for the timely and effective dissemination of information about developments occurring at the workshop. The workshop closes with a session devoted to a discussion of future directions in UQ research with a special emphasis on the outstanding open problems that need to be solved in order to make stochastic PDE-based UQ a tool that is easily, routinely, and readily available to those in government and industry that have to make decisions in environments involving risk and uncertainty."
"0915045","Studies in Poromechanics and Electro-Poromechanics","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","07/14/2013","Asheber Abebe","AL","Auburn University","Standard Grant","Junping Wang","08/31/2013","$175,777.00","","abebeas@auburn.edu","321-A INGRAM HALL","AUBURN","AL","368490001","3348444438","MPS","1271","0000, 6890, 9150, 9263, OTHR","$175,777.00","This proposal is awarded using funds made available by the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The mathematical problems arising from poromechanical models are challenging. Even linear models are mathematically and computationally demanding because they require the simultaneous solution of the equations of elasticity and those of fluid mechanics. More complex models may also account for electromagnetic, chemical, or thermal effects, in which case Maxwells equations, equations describing chemical reactions, or an energy equation must also be solved simultaneously. Difficulties arise from the fact that equations for common models have some degree of degeneracy and more complex models may involve nonlinear equations, multiple spatial scales, complicated boundary conditions, and nonlinear interactions. The investigator and his students will study poromechanical models analytically (existence and uniqueness), develop and rigorously analyze finite element based methods for approximating solutions of various model problems in poromechanics, and derive a-priori and a-posteriori error estimates. They will advance the underlying mathematical theory and the science of computer simulation of large-scale, complex, coupled, multi-scale phenomena.<br/><br/>Poromechanics is the science of energy, motion, and forces and their effect on porous material and in particular the swelling and shrinking of fluid-saturated porous media. Modeling and predicting the mechanical (or the electro-chemo-thermo-mechanical) behavior of fluid-infiltrated porous media is of great importance since many natural substances, for example, rocks, soils, clays, shales, biological tissues, and bones, as well as man-made materials such as foams, gels, concrete, water-solute drug carriers, and ceramics are all elastic porous media. The studies conducted by the investigator and his students have applications in a variety of unrelated fields, from geomechanics (ground failure of water-saturated sediments, electroseismic prospecting for oil and natural gas, or underground storage of hazardous waste) to pharmacology, material science, and biomechanics (in particular, the study of bones, corneal swelling, hydrated tissues, and intervertebral discs).<br/>Students involved in the project will be trained in the mathematical modeling and computer simulation of large-scale, complex, coupled, multi-scale phenomena."
"0914554","Numerical solutions of time-dependent stochastic partial differential equations","DMS","COMPUTATIONAL MATHEMATICS, COFFES","09/01/2009","08/26/2009","Yanzhao Cao","AL","Auburn University","Standard Grant","Leland Jameson","08/31/2013","$138,874.00","","yzc0009@auburn.edu","321-A INGRAM HALL","AUBURN","AL","368490001","3348444438","MPS","1271, 7552","0000, 9150, 9263, OTHR","$0.00","This project is to study  numerical solutions for time-dependent stochastic partial differential equations (SPDEs). In particular the investigator  will construct fast, practical numerical algorithms.<br/>At the same time, A solid theoretical basis for these algorithms based on proper error analysis will be provided. The project intends to concentrate on stochastic parabolic partial differential equations as our model problem. However, it is expected that the algorithms and analysis will be extended to many other types of time<br/>dependent SPDEs.   A concerted and comprehensive effort will be made<br/>to develop efficient, accurate, and robust computational methodologies, which from the very beginning incorporate uncertainty effects. The research has three major components: 1. Study of finite element approximations for high dimensional parabolic SPDEs with a forcing term involving either multiplicative colored noise or a random field; 2. Investigation of fast collocation methods to numerically evaluate statistical moments of parabolic SPDEs  with random boundary input data; 3. Construction of enhanced Monte Carlo methods, using sensitivity derivatives, for SPDEs with random parameters such as diffusion coefficients.<br/><br/>Scientists have discovered that there is a significant amount of uncertainty in all physical systems; not just when something is measured, but even when an attempt is made to describe how the system changes. No computer calculation can possibly consider every slight variation in the measurements and dynamics of a system under study. And yet it is known that, at least in some cases, small amounts of uncertainty can lead to significant, and even disastrous, errors in the computed results. The proposed research intends to make an effort to understand, quantify and control the effect of uncertainties through numerical computations. The underling mathematical equations describe basic physical phenomena such as heat transfer, diffusion processes and fluid flow dynamics. An important bonus of this research is the involvement of a group of undergraduate and graduate students, some of them from under represented groups.  It is expected that their participation in this project will expose them to scientific research, induce them to pursue further training, and to consider a scientific career as a future."
"0914336","Efficient Algorithms for Electronic Structure Analysis","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/02/2011","Weinan E","NJ","Princeton University","Continuing Grant","Junping Wang","08/31/2013","$400,000.00","","weinan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1271","0000, 9263, OTHR","$0.00","The main objective of the present proposal is to develop efficient algorithms for electronic structure analysis that are applicable for both metals and insulators. This will be done by developing multipole representation of the Fermi operator, which is a fundamental object in electronic structure analysis and more generally quantum theories of matter. In addition, the PI also proposes to develop efficient algorithms for representing and computing the Green's functions that arise in this context. A second component of the project is the numerical analysis of the algorithms in electronic structure analysis. Topics to be studied include the accuracy of linear scaling algorithms, convergence and convergence rates of self-consistent iterations. This will be done by developing and using simple but canonical model problems that capture the essential aspects of the problem but allow explicit analytical  calculations.<br/><br/><br/>Electronic structure analysis is at the foundation of chemistry and material science, as well as some aspects of biology. Our ability to understand chemical reactions and fundamental aspect of materials relies heavily on efficient numerical algorithms for solving models from quantum chemistry or density functional theory. Existing algorithms are much more effective for insulators than for metals. This is particularly true for the recently developed linear scaling algorithms which relies heavily on the exponential decay property of the wave functions or density matrices, a property that holds for insulators but not for metals. The present project is aimed at bringing powerful mathematical tools to bear on the problem of electronic structure analysis. The proposed work will explore the mathematical features of the electronic structure problem in a way that has never been done before. By doing so, new insights and new algorithms will result that greatly advance our ability to analyze the electronic structure of matter."
"0914559","Stable and efficient computation of the CS decomposition","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/17/2009","Brian Sutton","VA","Randolph-Macon College","Standard Grant","Junping Wang","08/31/2013","$107,951.00","","bsutton@rmc.edu","204 HENRY ST","ASHLAND","VA","230051502","8047527268","MPS","1271","0000, 6890, 9263, OTHR","$107,951.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The CS decomposition is a matrix decomposition related to the eigenvalue and singular value decompositions. It applies to any unitary matrix partitioned into a 2-by-2 block structure, and the effect is to simultaneously diagonalize all four blocks. By orthogonality, the entries of the resulting diagonal blocks must be the cosines and sines of some angles, leading Stewart to coin the term CS (cosine-sine) decomposition for his 1977 discovery. The decomposition can reveal the canonical angles between linear subspaces, studied as early as 1875 by Jordan, and the canonical correlations between two sets of observed variables, introduced by Hotelling in 1936. The current proposal focuses on computational aspects of this matrix decomposition. The first goal is the stable computation of the full form of the decomposition, as originally formulated by Stewart. As described above, this may be called a 2-by-2 CSD. Prior methods compute a reduced form, the 2-by-1 CSD, which in a sense is half of the full 2-by-2 form. Attempts at extending existing methods for the 2-by-1 CSD to the full 2-by-2 CSD have not been successful. A proof of stability for a recent algorithm described by the investigator, designed from the ground up to compute the full 2-by-2 CSD, is sought. Another goal of the project is to analyze the stability and efficiency of the algorithm when specialized to the 2-by-1 CSD and the GSVD, comparing with existing methods. A third goal is the investigation of an alternative algorithm based on a divide-and-conquer scheme, in addition to the QR iteration-approach of the investigator's first algorithm. Finally, possible applications enabled by the stable computation of the 2-by-2 CSD will be explored.<br/><br/>A matrix decomposition is a mathematical tool for breaking a matrix, a rectangular array of numbers, into a product of simpler matrices. These simpler matrices often reveal important structure. The eigenvalue matrix decomposition can distinguish between stability and instability in engineering applications and can reveal the prominence of web pages when performing Internet searches. The singular value matrix decomposition can infer the roles of genes from indirect observations and provides a key component in facial recognition. The subject of this project is another matrix decomposition, the CS (cosine-sine) decomposition. This decomposition can be used to compare and contrast underlying factors present in two data sets. An example is provided by Hotelling, who proposes the analysis of heritable traits by comparing and contrasting mother rat and daughter rat. The computation of the CS decomposition has proved more difficult than that of the eigenvalue and singular value decompositions. This project seeks to improve the accuracy and speed of computation. In addition, this project will consider a more general form of the CS decomposition than earlier studies, opening the possibility of new applications.<br/>"
"0914873","High-performance computations with rational generating functions","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/26/2009","Matthias Koeppe","CA","University of California-Davis","Standard Grant","Leland Jameson","08/31/2013","$142,779.00","","mkoeppe@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271","0000, 9263, OTHR","$0.00","This is a research project in Discrete Computational Mathematics.  The investigator and his students will use and significantly advance the technology of short rational generating functions introduced by Barvinok (1994).  In short, this technology allows to efficiently count the integer points in families of polytopes, which has numerous applications.  Research in this project will involve methods from mathematical optimization, discrete geometry, convexity, and computer-based search.  The PI has written and maintains the open-source software ""LattE macchiato"", which is one of the two state-of-the-art implementations of short rational generating functions.  Past and current research of the PI has established, by introducing algorithms and proving theorems on their computational complexity, that short rational generating functions have a large computational potential that goes beyond what has been achieved so far in practice.  This potential relates to applications in many fields, including combinatorics, mathematical optimization (nonlinear mixed integer programming), and algorithmic game theory.  The general theme of the research project is to find new theorems on decomposition schemes with special properties, generalizing the signed decomposition of simplicial cones in the primal or dual space.  The PI's research will involve, among other techniques, computer-based search for optimal decomposition schemes.  There is a deep theoretical interest in this, independent of algorithmic consequences.  On the basis of the new decompositions and other new techniques (such as exploiting symmetry), the PI expects to develop and implement new efficient algorithms, including a parallel implementation for use on multi-core systems and clusters. The overall goal is to expand the range of applicability of short rational generating function methods significantly.  The success of these methods will be measured by specific computational challenges listed in this proposal, which are intractable by current computer software.<br/><br/><br/>Mathematical optimization is the science of making the best decisions possible, when resources are limited.  In the past 10 years it has become clear that a wide array of key technologies, ranging from biotechnology to chemical engineering and healthcare, depend on the ability to solve a complicated type of mathematical optimization problems, called ""nonlinear mixed-integer programs"".  The PI and collaborators have shown recently that a mathematical technique called short rational generating functions is very powerful to solve these optimization problems, surpassing all other known techniques, at least in mathematical theory.  However, there still is a huge gap between theory and practice.  This research project is about fundamental research on this technique of short rational generating functions, to help bridge the gap.  This will lead to new mathematical insights and more efficient algorithms and new, more powerful open-source mathematical software.  This project also has a strong educational impact.  The PI plans to train several undergraduate and graduate students in this research area.  One component of the training will be to create new course material on the topics of this proposal, and to use it for new classes for undergraduate and graduate students. The second component of the training consists of direct involvement of students in this research project, involving theoretical work, computer experimentation, and software implementation, all of which lead to undergraduate and graduate theses.<br/>"
"0915150","Computational Methods for Multiscale Turbulent Reacting Flows","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/07/2009","Tarek Echekki","NC","North Carolina State University","Standard Grant","Junping Wang","08/31/2013","$217,464.00","","techekk@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1271","0000, 9263, OTHR","$0.00","The objective of the proposed research is to develop computational methods within a multiscale modeling framework for turbulent reacting flows. The framework is based on hybrid coarse-grained and fine-grained(resp.) simulations based on large-eddy simulations (LES) and a stochastic low-dimensional model, the one-dimensional turbulence (ODT) model. The ODT model is designed to capture the coupling between turbulent and molecular transport and chemistry/heat release. The framework has been developed by the PI for combustion to directly model subgrid scale physics associated with turbulence-chemistry interactions; but, it can easily have broader applications for multiscale driven flows involving 3D deterministic solutions with low-dimensional stochastic solutions. The work proposed here extends the model formulation to address the development of multiscale computational tools to couple the two solution schemes. The approaches involve physics-based matching of filtered subgrid scale stresses between LES and ODT as well as the implementation multiresolution methods based on the wavelet transform. Data assimilation strategies will be formulated and implemented for managing the transfer of statistics from the stochastic solution to the high-order deterministic LES solution. Finally, strategies to accelerate the integration of chemical kinetics and molecular transport will be investigated to improve the computational efficiency of the hybrid LES-ODT formulation. The various formulations will be validated with direct numerical simulations (DNS).<br/><br/>The proposed effort addresses a novel simulation framework to study combustion flows. The combustion of fossil and novel fuels make up the lion?s share of energy consumption in the US and abroad; and the ability to simulate combustion flows, with expanding computational resources and know-how, can eventually replace building expensive prototypes of engines, and speed up the design-to-production process, and help mitigate the generation of pollutants and greenhouse gases. Combustion simulation is complex, because an account of complex physics from the molecular scale to the device scale is required; the investigator and his colleagues will develop multiscale strategies to simulate combustion flows through hybrid simulations that are designed to addresses physics at different ranges of scales. The combined outcome is a bootstrapping process for these simulations that results in computationally efficient strategies to represent the broad ranges of scales in combustion flows. Much of the effort requires insight to the physics of combustion flows and energy and environmental implications; but, other aspects of the effort draw upon advances in information technology (including access to supercomputing resources, advances in software) and computational mathematics."
"0913491","Study on Algorithms and Applications of Centroidal Voronoi Tessellations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/19/2009","Lili Ju","SC","University South Carolina Research Foundation","Standard Grant","Junping Wang","08/31/2012","$180,000.00","Xiaoqiang Wang","ju@math.sc.edu","915 BULL ST","COLUMBIA","SC","292084009","8037777093","MPS","1271","0000, 6890, 9150, 9263, OTHR","$180,000.00","This proposal is awarded using funds made available by the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>Centroidal Voronoi tessellations (CVTs) are special Voronoi tessellations having the property<br/>that the generators of the Voronoi tessellations are also the centroids, with respect to a <br/>given density function, of the corresponding Voronoi cells. In this project, we will continue <br/>to investigate algorithms for computing CVTs and CVT-based applications for scientific <br/>and engineering problems. Topics of the proposed project include: study of single limit-point <br/>convergence analysis for the Lloyd's algorithm; development and analysis of nonlinear <br/>conjugate gradient methods for computing CVTs; study and implementation of parallel CVT/CVDT <br/>mesh generation on the distributed systems; improving existing CCVT-based techniques for <br/>surface meshing; incorporating these meshing schemes in adaptive solutions of partial <br/>differential equations, especially for the convection-dominated problems; and further <br/>investigation and improvement of the edge-weighted CVT model and corresponding algorithms for <br/>image segmentation that combines the intensity information in the color space of the image <br/>and the local edge information in the physical space.<br/> <br/>CVT-based methodologies have been proven to be very useful in diverse applications in the past <br/>decade, including but not limited to, image processing, vector quantization and data <br/>analysis, resource optimization, optimal placement of sensors and actuators for control, cell <br/>biology and territorial behavior of animals, high-quality point sampling, mesh generation and <br/>optimization, numerical partial differential equations, climate and atmospheric science, <br/>model reduction, computer graphics and vision, mobile sensing networks, logistics system <br/>design, and etc. The application list is still growing. The proposed project has a <br/>comprehensive coverage of algorithm design and analysis, implementation and applications of <br/>CVTs to diverse problems in science and engineering. Mathematical tools are used to analyze <br/>these techniques to give guidelines for their applicability; practical considerations <br/>including parallel implementation issues are addressed to make the algorithms competitive in <br/>real applications and large scale computations. The proposed investigation will offer new <br/>insight into the  understanding of the elegant Lloyd's algorithm and it will also lead to <br/>exploration of transformative concepts and renovation of computational algorithms for many <br/>important applications involving mesh optimization, adaptive algorithms, energy minimization<br/>and image processing based on the CVT methodologies. In addition, this project will also offer <br/>a unique educational opportunity for graduate students with interests in computational and <br/>applied mathematics, engineering and information technology by having them participate in an <br/>interdisciplinary research program."
"0914957","Collaborative Research: RUI: Multilinear Algebra Computations with Higher-Order Tensors","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/14/2009","Misha Kilmer","MA","Tufts University","Standard Grant","Junping Wang","07/31/2012","$221,216.00","","misha.kilmer@tufts.edu","169 HOLLAND ST FL 3","SOMERVILLE","MA","021442401","6176273696","MPS","1271","0000, 6890, 9263, OTHR","$221,216.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>Many real world applications need to compress, sort, and otherwise <br/>manipulate large volumes of multidimensional data arrays (i.e., higher-<br/>order tensors), so there is an increasing need for theoretical and <br/>computational tools to deal with multiway data.  The subject of <br/>multilinear algebra and tensors has gained increasing prominence in <br/>the past decade due to a surge in applications such as approximation <br/>of Newton potentials and stochastic PDEs, image compression and <br/>deblurring, network traffic analysis, biological assay interpretation, <br/>unmixing of signals, and many more.  Such applications with higher-order<br/>tensors involve factorizations of the tensor.  There are multiple <br/>possible extensions of matrix factorizations to higher-order tensors <br/>(e.g. extensions of the matrix SVD), with some more amenable to certain<br/>applications.  The investigators are advancing the state-of-the-art in <br/>both theoretical and computational multilinear algebra via several newly<br/>developed tensor constructions based on new notions of tensor <br/>multiplication, orthogonality and diagonalizability.  Algorithms with <br/>compression schemes based on these new constructions are being <br/>implemented by the investigators and tested on several datasets from <br/>various applications including handwritten digit identification, <br/>genomics, the spectral unmixing problem, video compression, and computer<br/>image recognition.<br/><br/>Current applications in the sciences can involve <br/>analysis, classification, searching and compression of large volumes of <br/>data that is ""multidimensional"" in nature.  Consider, for example, the <br/>problem of facial recognition, used to identify a terrorist from within <br/>a database of images of known terrorists. The database can be considered <br/>multidimensional data in the sense that for each individual there <br/>corresponds a specific viewpoint, illumination, and facial expression. <br/>It is critical in this scenario to have an accurate and fast algorithm <br/>to match an unknown image against a database of images of known <br/>terrorists. Another example where a multidimensional representation is <br/>useful is genomic data.  Here, DNA microarray two-dimensional tabular <br/>data from different experiments is concatenated into a multidimensional <br/>array.  Recently published results have indicated that so-called <br/>'factorizations' of this multidimensional data can be used to discover <br/>new molecular-level interactions.  Hence, along with advances in <br/>computer architecture to store large datasets must come mathematically <br/>sound models for the compression and/or analysis of such data.  <br/>Development of new concepts and ideas is therefore required to deal with<br/>the different geometries that arise in the multidimensional case.  The <br/>investigators are contributing directly to this effort by developing <br/>innovative mathematical theory for multidimensional objects that is <br/>consistent with two-dimensional proven ideas.  With theoretical <br/>constructs in place, the investigators are able to create computational<br/>tools and algorithms to analyze and compress multidimensional data.  A <br/>significant component of the proposal is the involvement of <br/>undergraduates, graduate students, and researchers.  The investigators <br/>are leveraging the strengths of both universities in a novel, <br/>inter-institutional vertical integrative experience for all students <br/>(undergraduate and graduate) involved in the research.  This arrangement<br/>allows students at all levels, mentored by leading researchers in the <br/>field, to advance the state-of-the-art in the analysis and compression<br/>of multidimensional data."
"0915219","RUI: New Variational Models for Denoising, Decomposition, and Deblurring","DMS","COMPUTATIONAL MATHEMATICS","07/01/2009","06/19/2009","Stacey Levine","PA","Duquesne University","Standard Grant","Leland Jameson","06/30/2014","$187,875.00","","sel@mathcs.duq.edu","600 FORBES AVENUE","PITTSBURGH","PA","152820001","4123961537","MPS","1271","0000, 9229, 9263, OTHR","$0.00","The investigator, collaborators and students will develop new models for image processing, show the models are mathematically sound, determine geometric properties of their solutions, develop accurate and efficient numerical schemes, and directly apply these models to problems in the sciences. The models will address four fundamental problems in image processing: edge-preserving denoising, decomposition, deblurring, and image fusion. The proposed formulations will be based on variational methods and partial differential equations, which provide an appropriate framework for studying the mathematical foundations and physical interpretation of the models.<br/>These include models involving convex linear growth functionals, the Besov semi-norm, and negative Sobolov norms, which can retain desirable geometric image properties such as edges and textures, while avoiding the introduction of false artifacts. One of the challenges addressed here will be finding appropriate discrete representations of the continuous problem that retain desirable geometric features and are tractable.<br/><br/>Digital images are now used in almost every area of science and technology. The models developed in this project will be used to solve real world problems in areas such as medical imaging and material science. However, the models will be formulated in enough generality to potentially be applied to a wide array of applications in the sciences. Software developed in this grant will be made publicly available. The investigator regularly teaches courses on image processing, and leads workshops on image processing for middle school students and high school women and minorities. This project will also support undergraduate researchers. Thus this work will promote the training of young scientists, as well as provide educational opportunities to underrepresented groups.<br/>"
"0915118","Collaborative Research: Computational Methods for Coupled Wave, Current, Sediment Transport and Morphological Evolution","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","07/11/2011","Ethan Kubatko","OH","Ohio State University Research Foundation -DO NOT USE","Continuing Grant","Leland Jameson","08/31/2013","$223,849.00","","kubatko.3@osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1271","0000, 9263, OTHR","$0.00","The goal of this project is to significantly advance the capability to accurately predict sediment transport and seabed morphology in three-dimensional coastal and estuarine environments. To achieve this goal, state-of-the-art algorithms for fully coupled wave, current, sediment transport and bed morphology will be developed, analyzed, and implemented.  The development of this system will require a better mathematical and physical understanding of the tightly coupled nature of the various processes involved. In addition, it will require computational strategies which address the accurate and efficient coupling of interdependent processes that exhibit a wide range of spatial and temporal scales?both within the fluid motion itself and between the fluid and bed motion.  Robust and highly parallelizable algorithms for the various processes will be developed based on discontinuous Galerkin finite element methods, and the coupling of these algorithms will be carefully investigated in order to maintain numerical accuracy, efficiency, and local mass conservation of sediment and fluid phases.<br/><br/>Studying and predicting the morphodynamics of the coastal zone requires a detailed knowledge of winds, waves, currents, sediment transport and, ultimately, the resulting morphological changes of the seabed that occur as a result of these processes. The erosion and deposition of bed sediment can have a major detrimental impact on the coastal population, infrastructure and environment.  For example, during Hurricane Katrina, four major levee breeches that occurred during the storm were a result of foundation-induced failures caused by scour.  The transport of sediment is closely tied to a number of other issues in the coastal zone, including water quality and related ecological concerns, beach and shoreline erosion, and the maintenance of navigation channels and harbors through dredging activities.<br/>Accurate estimates of expected sediment transport and bed morphological changes can aid greatly in the long-range planning and management of coastal and estuarine environments.  In this project, the investigators will develop a fully coupled wave, current, sediment transport and bed morphology model system.  Such a system will significantly advance the capability to accurately predict sediment transport and seabed morphology in coastal and estuarine environments.<br/>From this research, a better scientific understanding of the complex interrelations among hydrodynamic, transport and morphodynamic processes in the coastal zone will emerge, which can lead to more informed decision-making that will help protect the coastal population and infrastructure. The developed software of the project will also provide a computational infrastructure that can be used in many other applications within the area of computational modeling. Furthermore, the technology developed under this project will be disseminated to government agencies such as NOAA and the US Army Corps of Engineers.<br/>"
"0915238","Efficient Solution of Advection Dominated PDE Constrained Optimization Problems","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/13/2009","Matthias Heinkenschloss","TX","William Marsh Rice University","Standard Grant","Junping Wang","08/31/2013","$264,825.00","","heinken@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","0000, 9263, OTHR","$0.00","The aim of this proposal is to develop, analyze and implement optimization<br/>algorithms that integrate multilevel iterative solvers, adaptive mesh refinement methods, <br/>and so-called `all-at-once' methods for the solution of optimization problems <br/>governed by advection dominated partial differential equations (PDEs). <br/>The presence of strong advection in the governing PDE creates many challenges for<br/>the numerical solution of these optimization problems, beyond the challenges already<br/>encountered in the numerical simulation of single advection dominated PDEs and beyond<br/>the many difficulties in solving PDE constrained optimization problems with weak or no advection. <br/>One reason  for the additional challenges arising in the optimization context is the presence of the <br/>so-called adjoint equation, which is also an advection dominated PDE, but with advection <br/>given by the negative  of the advection in the governing equation. This can cause significant <br/>and perhaps unexpected differences in the behavior of discretization schemes and iterative <br/>solvers when they are extended from the application to single advection dominated PDEs <br/>to the solution of PDE constrained optimization problems.  This research will analyze <br/>the sources of these differences, their impact on the quality of computed solution<br/>and on the efficiency of solution algorithms. Another goal is to devise modifications of <br/>multilevel iterative solvers, adaptive mesh refinement methods, and so-called KKT solvers to<br/>make them robust against the presence of advection.<br/><br/><br/>Many important real-life applications  such as the shape optimization of technological devices, <br/>the optimal control of systems, and the identification of parameters in environmental<br/>processes lead to optimization problems governed by systems of advection dominated<br/>partial differential equations. This research addresses mathematical and algorithmic<br/>issues that are crucial for the reliable and efficient solution of these problems. It will lead<br/>to a better theoretical understanding of the solution properties of these optimization problems <br/>as well as to new computational tools for their  reliable and efficient solution. Furthermore, <br/>it will provide training opportunities for students in an important area of computational science."
"0914596","Hybridizable Discontinuous Galerkin Methods for Partial Differential Equations and Theoretical Questions in Finite Elements","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","08/02/2009","Johnny Guzman","RI","Brown University","Standard Grant","Junping Wang","07/31/2013","$189,826.00","","Johnny_Guzman@brown.edu","1 PROSPECT ST","PROVIDENCE","RI","029129127","4018632777","MPS","1271","0000, 6890, 9150, 9263, OTHR","$189,826.00","This proposal is awarded using funds made available by the American Recovery and Reinvestment Act of 2009 (Public Law 111-5), <br/><br/>The main part of this project will focus on developing and analyzing discontinuous Galerkin (DG) methods for problems arising in structural mechanics and fluid flow. In particular, the P.I. will analyze hybridizable discontinuous Galerkin (HDG) methods for plate bending problems, elasticity equations and convection-diffusion equations.  One advantage of HDG methods is that they can approximate all the variables of interest in an optimal way while using equal-order approximations for all the variables. More importantly, many of the global degrees of freedom can be eliminated by the use of Lagrange multipliers, making the final linear system smaller than linear systems arising in standard DG methods.  Another component of the project is the investigation of DG methods for multiscale problems.  The P.I. hopes to develop higher-order DG methods using non-polynomial basis functions.  A final project will be answering theoretical questions in finite elements. The P.I. will prove pointwise error estimates for finite element methods applied to the Stokes problem on general convex polyhedral domains.  Then, the P.I. will prove error estimates for higher-order streamline diffusion methods on layer-adapted meshes.   <br/><br/>Numerical simulations play a central role in modern engineering.  For example, they are crucial in the design of airplanes, automobiles, and oil platforms, to name a few.  They allow industries to test structures using computers without ever building an actual physical model. One of the reasons this is possible is that very efficient and reliable numerical methods have been developed over the years.  However, to meet new computational challenges, researchers are working on improving existing algorithms and on the development of new competitive ones. In this project, the P.I. will work on developing a new, promising family of numerical methods called hybridizable discontinuous Galerkin methods. In order to gain a deeper understanding of these numerical methods and related ones, the P.I. will also investigate mathematical aspects of such methods. <br/>"
"0915047","Collaborative Research: A Computational Framework for Assessing the Observation Impact in Air Quality Forecasting","DMS","COMPUTATIONAL MATHEMATICS, Atmospheric Chemistry","07/15/2009","02/22/2012","Adrian Sandu","VA","Virginia Polytechnic Institute and State University","Standard Grant","Junping Wang","09/30/2013","$418,616.00","","sandu@cs.vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271, 1524","0000, 4444, 6890, 9188, 9263, EGCH, OTHR","$418,616.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>This research develops the algorithmic and computational framework needed for a judicious assessment of the observation impact in air quality modeling. Novel algorithms in the framework of model-constrained optimization will allow to account for the data location in the time-space domain, observation type, instrument type, and data interaction in the presence of multiple observing systems. Specifically, the research is focused on: development of high-order adjoint-based observation impact techniques consistent to four-dimensional variational data assimilation schemes; assessment of forecast sensitivity to observation error variances and estimation of the forecast impact of uncertainties in the specification of the input error statistics; development of efficient computational approaches to allow for practical implementations of the observation impact algorithms; validation of the novel techniques through observing system experiments and assessment of the potential impact of new observing systems.<br/><br/>The ability to accurately represent the distribution of atmospheric pollutants in relation to various anthropogenic activities is essential for chemical weather forecasting to protect the population, for answering science questions related to the future of our planet, and for designing sound environmental policies. An accurate representation of the chemical composition of the atmosphere requires a close integration of models and observations through data assimilation.<br/>Data assimilation is the process by which model predictions utilize measurements to produce an optimal representation of the state of the atmosphere. As more observations are becoming available and new measurement networks are being planned, it is of critical importance to develop the capabilities to best utilize the data, to better manage the sensing resources, and to design more effective field experiments and networks to support atmospheric chemistry and air quality studies. This research develops the computational tools required to optimize the information provided by the existing observing systems and to provide guidance for future improvements to the observational network and instruments design. The new developments will also help the design process of new field experiments and of new chemical monitoring networks."
"0914813","A novel framework for fluid/structure interaction in subject-specific surgical simulations involving elastic cardiac geometries","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","09/04/2009","Joseph Teran","CA","University of California-Los Angeles","Standard Grant","Junping Wang","08/31/2012","$197,136.00","","jmteran@ucdavis.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1271","0000, 9263, OTHR","$0.00","One of the most important applications of computational fluid dynamics has been simulation of blood flow. However, practical difficulties have limited the types and applicability of simulations performed, thus preventing numerical modeling of blood flow from reaching its full potential. Extreme computational expense, reduced order of accuracy due to complex geometry and lack of regularity in solutions have restricted the scale and scope of blood flow simulations. One exciting application currently outside the scope of existing methods is the simulation of surgeries designed to repair diseased and malfunctioning heart valves. The complexity of the valvular geometry and flow patterns in their vicinity complicate considerably the development of reliable and predicative numerical models. The ability to deliver patient specific prognoses demands an algorithm that can accurately resolve these flows. However, cardiac geometry is highly complicated and must be represented with both volumetric and membraneous components, either of which might also exhibit intricate irregularities due to the patient's valvular disease. The solid/fluid coupling algorithm must have sufficient geometric flexibility to resolve these features and to adapt to the changes induced by the virtual surgery. Ultimately, to provide meaningful results the solid/fluid algorithm must deliver a certain level of accuracy and stability without sacrificing adaptability. Existing methods for fluid-solid interaction cannot guarantee this level of functionality. The general case sees geometric flexibility traded for higher order accuracy. Also, practical demands create the need for stable algorithms with minimal time step restrictions as the desire to accurately predict postoperative behavior comes with the inherent need to run simulations over longer time intervals. The challenging nature of providing the functionality needed for effectively simulating valvular surgeries requires addressing all these issues simultaneously and existing methods cannot do this. The primary contribution of the proposed research will be the development and application of a tractable second-order numerical method capable of coupling a viscous incompressible fluid with thin and volumetric geometrically complex elastic solids represented with Lagrangian meshes. The fluid will be modeled by a cartesian Eulerian grid in which the solid representations are embedded to avoid the prohibitive cost of re-meshing the computational domain at each time-step in the simulation. Regular grids will be used wherever possible. Geometric flexibility and the ability to impose a variety of boundary conditions on arbitrary moving surfaces throughout the fluid domain are key to accomplishing the stated goals and will be a primary guide in developing the higher-order accurate Navier-Stokes solver.<br/><br/>The benefits of patient-specific computational fluid dynamics simulations of blood flow near healthy and diseased heart valves can potentially revolutionize the treatment of certain pathologies. Such functionality could allow the surgeon to design new procedures tailored to the individual, to determine whether or not surgery is needed by numerically predicting postoperative results and could even be used to train surgical residents in state-of-the-art techniques. This effort will focus on the development of a numerical method for examining blood flow through such surgically altered tissues in the challenging case of corrective valvular surgery. Specifically, we target improvements in treatment for Tetralogy of Fallot and mitral valve repair. Patients born with Tetralogy of Fallot require artificial replacement valves with inherently finite lifespan and accurate determination of the time to replace these valves to correct for pulmonary regurgitation is a matter of life and death. With procedures such as mitral valve repair, the difficult choice lies in determining exactly which type of correction best suits a particular individual. The determination of when to make these critical decisions and many related others could potentially be improved with the successful application of this effort.     <br/>"
"0914524","Sparse and Regularized Optimization","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","05/20/2009","Stephen Wright","WI","University of Wisconsin-Madison","Standard Grant","Junping Wang","08/31/2013","$274,000.00","","swright@cs.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1271","0000, 9263, OTHR","$0.00","Most algorithmic research in optimization has focused on models<br/>consisting of a single objective together with a number of<br/>constraints, all defined precisely and deterministically, where an<br/>exact solution is required. This paradigm is inadequate in many<br/>applications. First, there is often uncertainty in the model and data;<br/>this is being dealt with by a recent upsurge of work in stochastic and<br/>robust optimization. Second, users often require a simple approximate<br/>solution rather than a more complicated exact solution. When the<br/>problem is formulated in the appropriate space, simplicity is often<br/>manifested as sparsity - the vector of variables has relatively few<br/>nonzeros. Inclusion of nonsmooth regularization terms in the<br/>formulation can steer the model toward sparse solutions. This proposal<br/>focuses chiefly on algorithms and theory for sparse and regularized<br/>optimization, and on application of the methods to such important<br/>areas as compressed sensing, machine learning, computational<br/>statistics, and image processing. The project also takes a<br/>higher-level view, cross-fertilizing algorithmic ideas across<br/>different application areas, and devising and analyzing algorithms in<br/>general settings that encompass many specific applications. <br/><br/><br/>Optimization methods can be used to solve a great variety of practical<br/>problems, such as design of cancer treatment plans, removing noise and<br/>blur from images and videos, identifying genomic and environmental<br/>risk factors for diseases, and reconstructing pictures, signals, and<br/>other data sets from limited random samples. Precise mathematical<br/>formulations of these optimization problems are available, and exact<br/>solutions can often be obtained, but what is needed in many cases is a<br/>simple, approximate solution that is easy to compute, understand, and<br/>apply.  To take one example: Many images can be stored by taking a<br/>small number of random combinations of the pixels that make up the<br/>image. The optimization algorithm that reconstructs the original<br/>picture from the samples should look for the simplest picture that is<br/>roughly consistent with the random observations; this image is likely<br/>to appear more natural than complicated images that give an exact<br/>match to the data. This project investigates how the mathematical<br/>statements of problems like this one, and the mathematical methods<br/>that solve them, can be modified to produce simple solutions."
"0915066","Fast Spectral-Galerkin Methods and their Applications","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/02/2011","Jie Shen","IN","Purdue University","Continuing Grant","Junping Wang","08/31/2013","$329,052.00","","shen@math.purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271","0000, 9263, OTHR","$0.00","As computer simulations are playing an ever increasing role in many branches of science and engineering and are rapidly replacing much of the expensive prototyping and testing phases in manufacturing and in science explorations, fast and robust numerical methods are becoming an indispensable tool for many scientists and engineers.  The focus of this project is to design auurate, fast and robust spectral methods for solving a large class of partial differential equations, and apply them to investigate several important problems of current interest. The proposed research will result in fast and accurate numerical algorithms for a class of partial differential equations with applications in acoustic and electromagnetic scattering, complex fluids and materials science.  In particular, the proposed numerical algorithms will make it possible to directly solve some important high-dimensional equations which are currently not treatable with existing numerical algorithms.<br/><br/>It is expected that the proposed numerical simulations will enable us to handle challenging problems having stringent accuracy and/or memory requirements with a reasonable cost in CPU and turn-around time, contribute towards better understandings of the complex physical and mathematical problems, and provide valuable information for the design of advanced materials and on the rheological and hydrodynamic properties of complex fluids.  Another important goal of this project is to engage graduate and undergraduate students in learning necessary skills of computational and applied mathematics so that they can pursue a successful career in sciences and engineering.<br/>"
"0937025","International Symposium on Mathematical Programming 2009; Chicago, IL; August 2009","DMS","COMPUTATIONAL MATHEMATICS","07/01/2009","06/09/2009","Stephen Wright","WI","University of Wisconsin-Madison","Standard Grant","Rosemary Renaut","06/30/2010","$20,000.00","Mihai Anitescu","swright@cs.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1271","0000, 7556, 9263, OTHR","$0.00","This project supports travel by graduate students, postdoctoral<br/>associates, and young faculty members from US institutions to the<br/>International Symposium in Mathematical Programming (ISMP), to be held<br/>on August 23-28, 2009, in Chicago, Illinois. This year's ISMP is the<br/>twentieth conference in a series that began in 1949 and is held about<br/>every three years. ISMP is the major conference for the optimization /<br/>mathematical programming community and is a must-attend event for<br/>researchers in the area worldwide. The attendance has grown from about<br/>40 participants in 1949 to over 1400 at more recent gatherings. About<br/>every second or third ISMP is held in the US. Further information on<br/>ISMP 2009 can be found at http://www.ismp2009.org. ISMP provides the<br/>main forum for the exchange of ideas in optimization, and in<br/>particular for cross-fertilization between the discrete, continuous,<br/>and stochastic aspects of the discipline. The major professional<br/>prizes in optimization, which are decided by juries assembled by MPS<br/>and co-sponsoring societies AMS and SIAM, are awarded during the<br/>opening ceremony of ISMP. Particularly notable is the Tucker Prize,<br/>which is awarded to the best student thesis written during the three<br/>years preceding ISMP. It has been a long-standing tradition to hold a<br/>technical session at ISMP during which the three student finalists<br/>present their work; this session is always a highlight of the meeting.<br/><br/><br/>The project provides reimbursement of travel costs for at least 25<br/>graduate students and young researchers from US institutions to attend<br/>ISMP 2009, to a level of up to $800 per participant. ISMP is the<br/>premier conference in optimization, held triennially. Optimization is<br/>a field of mathematical and computational research that concerns<br/>itself with finding the best way to operate a system subject to<br/>restrictions on the choices that can be made. It has applications in a<br/>great many areas of science, engineering, and commerce, which are<br/>illustrated by the following three examples. First, optimization is a<br/>vital tool in designing radiation treatment plans for cancer patients,<br/>where the goal is to irradiate and thus destroy tumors while sparing<br/>healthy tissue. Second, optimization is used in designing mechanical<br/>parts for vehicles and machines that have minimum weight and cost<br/>while meeting certain strength and robustness requirements. Third,<br/>optimization is used in budget planning for businesses, to allocate<br/>resources in a way that maximizes expected profit in an uncertain<br/>economic environment. Support of attendance by young US researchers at<br/>ISMP will be of great benefit to their careers and research<br/>programs. It will eventually have important positive benefits for US<br/>industry, science, and technology. Applications for support will be<br/>solicited and reviewed by a committee of optimization<br/>researchers. Awardees will be chosen to ensure breadth and diversity<br/>of individuals and institutions represented, and to ensure<br/>participation of women, minorities, and persons with disabilities in<br/>ISMP 2009.<br/>"
"0914827","Computational Methods for Studying Heterogeneous Pulse-Coupled Network Dynamics","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/23/2009","Aaditya Rangan","NY","New York University","Standard Grant","Junping Wang","07/31/2013","$270,000.00","","rangan@cims.nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1271","0000, 1719, 6890, 9263, OTHR","$270,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The goal of this proposal is to develop analytical and computational tools capable of addressing the architecture-specific dynamics associated with heterogeneous pulse-coupled dynamical systems. There are two major components to this proposal.  First, the development of a diagrammatic subnetwork expansion will provide a systematic way of analyzing dynamic observables (such as activity rates or correlations) for the long-time dynamics associated with any pulse-coupled network. Due to the pulse-coupling of the network dynamics, each term within this diagrammatic expansion corresponds to a causal sequence of events spanning a subnetwork of the original network.  Second, the development of numerical algorithms for solving delay-differential population-dynamics equations will provide a natural method for computing the terms within the subnetwork expansion, as well as for solving more general delay-differential equations.<br/><br/>Pulse-coupled networks are a very general, and important, type of dynamical system which are often studied within the physical sciences. Indeed, any system which can be represented using a network of connected nodes which interact through instantaneous bursts of information can be formally described as a pulse-coupled network. For example, the internet, networks of neurons within the brain, and many neural networks used in computer science can all be thought of as pulse-coupled networks. One of the major questions associated with the study of any pulse-coupled network is ""what does it do?"" How can a complicated pulse-coupled network's dynamics be understood? The techniques to be developed within this proposal will provide a framework for analyzing the dynamics of pulse-coupled networks, casting a detailed picture of the relationship between any given pulse-coupled network's structure (i.e., how the network is built) and how that network behaves. The proposed research will aid in understanding the function of many important pulse-coupled networks studied in a variety of fields, ranging from image-processing and pattern detection to robotics and circuit design to neuroscience."
"0914954","Domain Decomposition Methods: Algorithms and Theory","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/18/2009","Olof Widlund","NY","New York University","Standard Grant","Junping Wang","08/31/2012","$209,874.00","","widlund@cs.nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1271","0000, 9263, OTHR","$0.00","The development of numerical methods for large algebraic systems is<br/>central in the development of efficient codes for computational<br/>fluid dynamics, elasticity, and electromagnetics. <br/>Many other tasks in such codes parallelize relatively easily.<br/>Algebraic system solvers therefore remain very important now that<br/>an increasing number of parallel and distributed computing systems, with<br/>a substantial number of fast processors, each with a relatively large memory,<br/>are becoming widely available. A very desirable feature of domain<br/>decomposition algorithms is that they respect the memory hierarchy of<br/>modern parallel and distributed computing systems, which is essential<br/>for approaching peak floating point performance. This is important<br/>since the cost of communication often can dominate for large computer<br/>systems. The domain decomposition methods are also relatively easy to implement <br/>and they have an increasingly solid theoretical basis, which shows that the<br/>rates of convergence of these preconditioned Krylov space methods are <br/>independent of the number of subdomains and grows only very slowly with <br/>the dimension of the subproblems allocated to individual processors. <br/>In each iteration step, local problems representing the restriction of the <br/>original problem to a potentially large number of subregions are solved <br/>exactly or approximately. The subregions, which can be allocated to <br/>individual processors of a parallel computer, form a decomposition of the <br/>entire domain of the problem. In addition, the inclusion of a coarse  <br/>component often substantially increases the efficiency of the preconditioner<br/>and can dramatically reduce the CPU time. This project will combine <br/>mathematical analysis with the design and numerical testing of algorithms.<br/>Each class of applications, e.g., elasticity, incompressible fluid flow,<br/>and electromagnetics, requires special considerations and,<br/>in particular, the design of an appropriate coarse solver, for the problem <br/>at hand,  is crucially important. Among the applications to be considered <br/>are incompressible Navier Stokes equations, Reissner-Mindlin plates, <br/>Maxwell's equations, nonlinear elastic contact problems, and those arising <br/>in forced vibrations and acoustics. Work will also continue on developing <br/>analytic tools, which also are applicable to very irregular subdomains <br/>such as those obtained from mesh partitioning software.<br/><br/>The overall goal of this work is to provide improved computational methods<br/>for the engineering and scientific community. A special emphasis is on<br/>methods that can be used effectively on modern parallel and distributed<br/>computer systems; these systems have many processors and fast networks<br/>for the communication between the processors. In many design problems, such<br/>large scale computing resources are required in order to take complicated<br/>geometry and rapidly varying displacements or velocities into account and<br/>standard computing systems have often proved to be inadequate. Led by the<br/>US national laboratories and the computer manufacturers, large scale parallel<br/>computing systems are being developed rapidly and these systems are by now<br/>also available to practicing engineers, who, e.g., test building design<br/>under the impact of earthquakes, prior to certification and construction, <br/>or machine parts under realistic operating conditions, prior to making <br/>prototypes. This work requires access to software systems and ultimately <br/>to reliable methods to approximate complicated scientific or engineering <br/>models. In many applications, accurate predictions often require massive <br/>amounts of data to describe the geometry and material properties accurately<br/>enough. The design of methods to extract the solution of such problems <br/>requires different algorithms for different applications such as the<br/>design of buildings, the propagation of electromagnetic waves, or <br/>fluid flow in oil fields. This project is focused on mathematical analysis<br/>of these issues and the design of improved methods. Experience of such <br/>efforts in the past clearly indicates that insight gained from such work <br/>can greatly improve the efficiency and reliability of computational practice. <br/>This work is a collaborative effort with leading developers of methods <br/>and software systems at the SANDIA National Laboratories at Albuquerque, NM, <br/>and at the University of Essen, Germany. <br/>"
"0900277","Quantitative Design of Experiments to Predictably Alter Intracellular Signaling Dynamics","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","09/01/2009","08/27/2012","Ann Rundell","IN","Purdue University","Continuing Grant","Mary Ann Horn","08/31/2015","$1,216,337.00","Marietta Harrison, Gregery Buzzard","rundell@ecn.purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271, 7334","0000, 4075, 7303, 9263, OTHR","$0.00","The objective of this project is to establish nonlinear dynamics and control theory techniques that facilitate the design of experimental strategies to predictably manipulate the intracellular signaling pathway dynamics.  The development of this type of dynamical analysis and control theory-inspired experiment design has been hindered by the lack of adequately accurate mathematical models and incomplete knowledge of the intracellular signaling pathways.  Recent developments and preliminary experiments using nonlinear model predictive controller design with sparse grid-based optimization have set the stage for the development of effective control strategies despite these uncertainties.  The refinement of computationally efficient sparse grid-based techniques to rapidly screen model structures and parameters for compatibility with the available experimental data will elucidate data-consistent signaling dynamics, facilitate experiment design to discern between data-compatible mechanisms, and support robust controller design.  The extension of current robust controller design techniques to select controller parameters that minimize the effects from uncertainties in the model parameters and structure and small perturbations in the controller parameter values will maximize the likelihood that the resulting controller actions, when realized in the laboratory, will achieve their desired objectives.  The robust controller design techniques developed to minimize the effects of large degrees of uncertainty in the model parameters and structure will be immediately applicable to control a number of highly uncertain systems.<br/> <br/>Intracellular signaling pathways transmit and coordinate signals within the cell to direct fundamental cellular processes such as proliferation, differentiation, migration and apoptosis.  The ability to predict signaling network dynamics will ultimately provide us with ways to alter cellular responses in a controlled manner.  This ability will find application in medicine and pharmacy, agriculture, and biotechnology.  The intracellular signaling pathways involve numerous chemical species that participate in a complex sequence of events orchestrated by interactions, crosstalk and feedback.  The complexity of these networks hinders the ability of unaided intuition to efficiently design experiments to manipulate the signaling events in a desired manner.  This interdisciplinary research combines and extends expertise and techniques from control theory, computational mathematics, and cell biology to derive approaches to quantitatively design experiments.  Undergraduates and graduate students will participate in the research and primary dissemination of the research results will be through peer-reviewed journal publications, conference presentations, a technical workshop, course material, and the Web.<br/>"
"0914785","Efficient Stochastic Oracle Based Algorithms for Stochastic Programming and Large Scale Convex Optimization","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/14/2009","Alexander Shapiro","GA","Georgia Tech Research Corporation","Standard Grant","Junping Wang","08/31/2013","$327,417.00","Arkadi Nemirovski","ashapiro@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271","0000, 9263, OTHR","$0.00","The proposed research is aimed at  developing  computational techniques for handling two classes of complex convex problems. The first is Stochastic Programming, where objective and the constraints are given implicitly, as expectations over stochastic uncertain data, and thus are difficult to compute.<br/>The second class is formed by large-scale well-structured deterministic convex programs like those of  Linear or Semidefinite Programming, the difficulty coming from huge problem sizes; e.g., LP's with just few thousands of variables and constraints and  dense constraint matrices (arising, e.g., in Compressed Sensing and Machine Learning), or typical Semidefinite programs of similar sizes, are often beyond the grasp of the state-of-the-art solvers. The proposed research is  intended to treat both these problem classes simultaneously, reducing them to variational inequalities with monotone operators represented by unbiased easy-to-compute stochastic oracles. These oracles arise naturally in the context  of Stochastic Programming and can be constructed (e.g., via randomizing matrix-vector multiplications) in the case of large-scale LP's and SDP's. In order to  solve the resulting stochastic variational inequalities it is proposed to employ  recently developed algorithms (Robust Mirror Descent Stochastic Approximation and Stochastic Mirror Prox) which, under favorable circumstances,  exhibit nearly dimension-independent and theoretically unimprovable in the large scale case rate of convergence.<br/><br/><br/><br/>The last two decades have witnessed dramatic progress in techniques for solving convex optimization problems -- progress which by some  estimates led to 1.000.000-fold  performance growth, the factor  nearly equally contributed by advances in algorithms and by increase in hardware power.<br/> In spite of this progress, a significant gap between what is needed  by applications and what can be routinely solved by the  state-of-the-art convex programming algorithms still persists,  especially when the problems to be processed are intrinsically  difficult. The proposed research is aimed at bridging the above gap  by developing novel, more powerful than the existing alternatives,  computational techniques based on systematic replacement of difficult  to compute in the large scale case quantities, like matrix-vector  products involving huge matrices, with relatively easy to compute  randomized estimates of these quantities. Coupled with carefully  designed stochastic type algorithms, this allows to solve huge  problems, unaccessible by currently available deterministic algorithms,  in a reasonable time with a reasonable accuracy by observing only a  small portion of the data. This can be of paramount importance, e.g., for various applications of machine learning techniques where the amount of available data by far too large for direct processing. <br/>Some  preliminary theoretical and numerical results are very encouraging and  suggest that the proposed avenue of research is<br/>highly promising, and if successful   the proposed research will advance <br/>optimization theory and practice by enriching   the algorithmic know-how<br/>related to processing complex optimization problems.<br/>"
"0915240","0-1 Semidefinite Programming:  Modeling, Theoretical Foundation, Resolution and Applications","DMS","COMPUTATIONAL MATHEMATICS","07/01/2009","06/12/2009","Jiming Peng","IL","University of Illinois at Urbana-Champaign","Standard Grant","Junping Wang","06/30/2013","$225,000.00","","jopeng@central.uh.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1271","0000, 6890, 9263, OTHR","$225,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/><br/>0-1 Semidefinite Programming (0-1 SDP) is a new optimization model<br/>that covers several classes of challenging nonlinear integer<br/>programming problems. 0-1 SDPs arise frequently from numerous<br/>applications in various domains such as learning, communications and<br/>facility location. However, in spite of its broad range of<br/>applications and the computational challenge it poses, little is<br/>known with respect to the new optimization model except few results<br/>scattered in the literature. In this project, the PI and his<br/>research group will study the theoretical foundations of the 0-1 SDP<br/>model including identifying polynomially solvable cases of the model<br/>and exploring its theoretical limitations from an optimization<br/>perspective, develop resolution techniques for this new class of<br/>problems such as effective exact algorithms and scalable<br/>approximation algorithms that can deal with large size problems, and<br/>apply the new modeling and resolution techniques to problems from<br/>various disciplines.<br/><br/>The primary goal of the project is to study a new optimization<br/>model that can be applied to a broad range of domains such as<br/>learning and engineering design, and to develop reliable and scalable<br/>resolution tools for such a model.   Creative modeling techniques are<br/>crucial for capturing the problem semantics in many applications. For<br/>example, in multi-agent learning, every agent might have a<br/>different objective and solution to the same problem. It is<br/>important to integrate various opinions and solutions from the<br/>agents to have a more comprehensive understanding and achieve a<br/>global objective. The new optimization model in this project<br/>provides a powerful approach for multi-agent learning. Reliable<br/>and scalable computational methods are essential for learning from<br/>massive and noisy data set."
"0914478","Enabling Long-Time Accuracy in Turbulent Flow Simulations","DMS","COMPUTATIONAL MATHEMATICS","07/15/2009","07/10/2009","Leo Rebholz","SC","Clemson University","Standard Grant","Leland Jameson","06/30/2012","$256,583.00","","rebholz@clemson.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1271","0000, 6890, 9150, 9263, OTHR","$256,583.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The principal investigator (PI) proposes to research mathematical models and numerical methods for enabling long time accuracy in turbulent fluid flow simulations. The first aspect is the development, through mathematical and numerical analysis, of high accuracy approximate deconvolution regularization models and related algorithms. Analysis of these models and their methods will lead to i) the development of better models with increased accuracy and more efficient algorithms, and ii) discretization strategies and stabilization techniques that will improve stability and accuracy over longer time intervals. The second aspect is the development of an enhanced-physics based scheme for computing solutions to the 3d incompressible Navier-Stokes equations on general domains. By conserving helicity in addition to mass, momentum and energy, long-time accuracy will be achieved through the additional physical fidelity offered by the scheme. To confirm expectations, large-scale long time simulations will be performed on a variety of domains and boundary conditions. The enhanced physics based scheme will be extended to approximate deconvolution models, which are a rare breed of models that conserve energy and helicity in their continuous forms. Additionally, extension of the enhanced-physics based scheme to an energy and potential enstrophy conserving scheme for the shallow water equations will be explored.<br/><br/>The proposed research will lead to more accurate, more physically meaningful, computable approximations to 3d turbulent flow, which in turn will enable long-time accuracy of computed solutions. The need to accurately simulate turbulent fluid &#64258;ow is paramount for the design of planes, cars, and devices (including medical) that transport fluids.<br/>Even for designs where more complex &#64258;ows need simulated (e.g.<br/>multiphase such as in nuclear reactors), the fundamental difficulty is the same as for turbulent flow, and so progress in single phase turbulence is directly relevant.  Accuracy over long-time intervals, as well as the portability offered by physics-based models/discretizations, will greatly reduce the need for expensive experimental data and substantially accelerate the design process."
"0914595","Direct and Inverse Scattering Problems in Near-Field Optics Modeling","DMS","COMPUTATIONAL MATHEMATICS","08/15/2009","08/03/2009","Peijun Li","IN","Purdue University","Standard Grant","Junping Wang","07/31/2012","$84,852.00","","lipeijun@math.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1271","0000, 7237, 9263, OTHR","$0.00","The research objective of this proposal is to examine mathematical issues and develop computational methods for solving important classes of direct and inverse scattering problems that arise in near-field optics modeling. The approach is to treat first the direct scattering problem and then the inverse scattering problem, and increase the complexity of the modeled system as far as possible. The proposed research concerns the following topics: (1)  based on a global model for scanning tunneling microscopy, develop an adaptive coupling of the finite element and boundary integral method with error control by an a-posteriori error estimate to solve the direct problem; (2) extend the adaptive coupling of the finite element and boundary integral method to the vector theory of electromagnetic scattering; (3) develop an adaptive treecode algorithm to accelerate the boundary integral evaluations and thus to provide more efficient direct solvers; (4) develop a novel continuation method to solve the inverse problem at a fixed wavenumber. The proposed research will result in a suite of nice modeling and computational techniques, suitable for qualitative and quantitative study of various experimental configurations in<br/>near-field optical systems. Particularly, these techniques will contribute towards better understandings of the complex physical and mathematical problems in near-field optics, and provide valuable information for industry to design and fabricate new optical devices.<br/><br/>Near-field optics has developed dramatically in recent years as an effective approach to breaking the diffraction limit and obtaining images with subwavelength resolution, which leads to vast applications in modern science and technology, including biology, chemistry, materials science, and information storage. Guided by the increasingly accurate and realistic numerical simulations, the significant advances of the near-field optical microscopies have led to integration and miniaturization of optical devices, and many original and reproducible measurements in the vicinity of complex lithographically designed nanostructures. Reciprocally, the practical applications and scientific developments have driven the need for rigorous mathematical models and analysis to describe the scattering of complicated structures, and to accurately compute electromagnetic vector fields and thus to predict the performance of a given structure in near-field and nano optics, as well as to carry out  optimal design of new structures. The research lies at the interface of mathematics, physics, engineering, and materials science. It has significant potential for advancing the frontiers of applied and computational mathematics, and for evolving new mathematics and science.<br/><br/>"
"0907832","Analysis, Computation, and Application of Completions of Constrained Dynamical Systems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2009","05/31/2011","Stephen Campbell","NC","North Carolina State University","Continuing Grant","James Curry","08/31/2013","$300,000.00","","slc@math.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Abstract:  A differential algebraic equation (DAE) is an implicit system of differential equations in state and control variables. An integer quantity called the index is one measure of how different a DAE is from being an explicit ordinary differential equation (ODE). Many problems are most naturally initially modeled as a DAE particularly those that are analyzed and simulated using computer generated mathematical models. DAEs occur in optimization when the original problem is a DAE or because of the activation of constraints. Because of DAEs importance, a variety of numerical techniques have been developed in recent years to simulate and analyze DAEs although most of these methods only work on specific classes of problems with special structure and low index. Unfortunately many widely used software packages cannot accept DAE models, or if they can accept them, the models must be index one or have special structure. One way to address both the problem of general DAE integrators and wider use of DAE models in current software, is to embed the DAE into an ordinary differential equation (ODE) called a completion of the DAE. The idea of embedding the DAE solutions into an ODE has been around for some time. However, traditional approaches either left the new additional dynamics under determined or again only worked for special classes of systems. A more general and automated type of embedding, such as that to be investigated in the proposed project, would greatly extend the applicability of many current software packages. In this project the investigators and his colleagues and students  will work on the analysis, computation, and application of completions of general DAEs.  Analysis backed algorithms will be constructed to generate completions with desired extra dynamics. The proposed research represents a major extension and modification of this idea to general nonlinear DAEs and the development of new results on controlling the nature of these additional dynamics. The obtained results will be of considerable independent interest but the proposal will focus on first applying the new results to simulation and control. The proposed research will result in improved algorithms and new theoretical understanding of numerical methods for differential algebraic equations and their use in control and simulation.<br/><br/>Increasingly in many problems in science and industry the process or machine is described in terms of what are called differential equations.  This  mathematical model is then used to simulate the process on a computer and to predict what the real system would do, to design more efficient processes, and to improve performance.  However, the problems of interest in science and engineering today  are becoming increasingly complex so that today mathematical models are often formed by  combining together several different mathematical models.  A mathematical model of a biotechnological  system, for example, might include equations describing fluid flow, chemical reactions, and mechanics.  These composite mathematical models are often very complex and are sometimes not in a form that is ready to be solved with current computer software.  It can then take considerable human effort, and sometimes a loss of accuracy, to convert these complex mathematical models to a form that can be readily used.  The conversion process not only takes time but can lead to errors.  The investigators of this project will develop theory backed mathematical procedures and algorithms that will facilitate and automate the conversion of the original complex mathematical models to models that can be used with existing software and algorithms.  This will reduce the time between model development and being able to use the model, improve the accuracy of the models, reduce the introduction of errors, and thereby speed up the design of industrial processes, and the analysis of many physical systems.  While the differential equations studied in this project occur widely, in the testing of the algorithms the investigators will initially  draw primarily on problems from the mechanical engineering and aerospace communities and the development of more efficient and reliable  manufacturing processes."
"0915023","Adaptive Kernel-free Boundary Integral Method for Elliptic PDEs","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","10/14/2010","Wenjun Ying","MI","Michigan Technological University","Standard Grant","Junping Wang","10/31/2010","$51,324.00","","wjying@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","MPS","1271","0000, 6890, 9263, OTHR","$183,444.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>This project is motivated by the principal investigator's research on modeling the electrical wave propagation in the heart, for which a system of variable coefficient and anisotropic elliptic-parabolic partial differential equations (PDEs) must be solved. A major challenge in the research is to take into account the moving boundary of the geometrically complicated domain (beating heart) during the simulation. With the standard finite element or finite volume method to solve the moving boundary problems, the need to frequently regenerate body-fitted unstructured volume grids as the domain boundary evolves usually makes the simulation very expensive. This project aims to develop an efficient and second-order accurate algorithm for solving the general variable coefficient and anisotropic elliptic PDEs on complex domains with moving boundaries. Recently, the principal investigator developed a kernel-free boundary integral (KFBI) method for solving elliptic PDEs in two space dimensions (2D). The KFBI method is a structured grid based boundary integral method. The structured grids involved are not required to be aligned with the domain boundary. The KFBI method does not need the analytical expression for the kernel of the integral operator. It is applicable for solving general elliptic PDEs on complex domains with moving boundaries. This project will further develop the KFBI method for solving variable coefficient and anisotropic elliptic PDEs on complex domains in both 2D and three space dimensions (3D). To further improve the efficiency, an adaptive version of the KFBI method will be developed as part of the project. During his doctoral and post-doctoral studies, the principal investigator has also developed an adaptive mesh refinement (AMR) algorithm with body-fitted grids for elliptic/parabolic PDEs on complex but stationary domains in both 2D and 3D. The body-fitted grid based AMR algorithm, however, is not suitable for moving boundary problems. The combination and further development of the KFBI method and the AMR technique will overcome this issue and significantly improve both the efficiency and the robustness of the algorithm. <br/><br/>The kernel-free boundary integral method is a second-order accurate sharp interface method. The proposed research is a pioneering effort in applying the second-order accurate sharp interface method in combination with an adaptive mesh refinement technique to solve general elliptic partial differential equations, whose coefficients are spatially variable and anisotropic, on complex domains with moving boundaries. In addition, the proposed research has clear broad impacts to engineering applications. The outcome of the project will make it possible to perform more efficient and accurate modeling and simulation of clinically important phenomena, such as the cardiac electrical dynamics for heart diseases, tumor generation, tumor-induced angiogenesis and the intra-tumoral infusion of drugs, just to name a few. <br/><br/>"
"0915110","Collaborative Research: Non-negative Matrix Factorizations for Data Mining: Foundations, Capabilities, and Algorithms","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/18/2009","Tao Li","FL","Florida International University","Standard Grant","Junping Wang","08/31/2014","$149,981.00","","taoli@cs.fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","MPS","1271","0000, 9263, OTHR","$0.00","Nonnegative matrix factorization (NMF) factorizes an input nonnegative matrix into two nonnegative matrices of lower rank. It was recently discovered that NMF has unique ability to solve challenging data mining and machine learning problems.  The advantage of NMF over existing unsupervised learning methods are (1) NMF can model widely varying data distributions, (2) NMF performs both hard and soft clustering simultaneously. (3) Many other data mining problems such as semi-supervised clustering problems can be reformulated as NMF problem. Building upon these foundations, the investigators propose to establish a NMF-based comprehensive framework for data mining: (a) Provide deeper understanding of NMF's clustering capability;<br/>(b) Extend data mining capability of NMF for solving various data mining and machine learning problems; (c) Develop fast numerical algorithms which incorporate the state-of-the-art developments from numerical optimization for various matrix factorization models; (d) Develop novel and rigorous proof strategies to prove the correctness and convergence properties of the numerical algorithms; (e) Apply and evaluate these new algorithms in real-world applications.<br/><br/><br/>The proposed work creates a new paradigm of analyzing vast amount of data and discovering new knowledge from the data by transforming established matrix computational methodologies. This new technology can automatically group news articles into meaningful categories, discover protein modules in protein networks, extract weather patterns in climate data, segment pictures into distinct objects, detect communities on the Web, and enable many other scientific discoveries and new technologies creation. On a fundamental level, the proposed work establishes that a simple matrix factorization in fact solves challenging data mining problems. This research reinforces the importance of mathematics in today's data centric world and encourages students to learn mathematics."
"0914107","Algebraic and Geometric Computation with Applications","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/13/2009","Jesus De Loera","CA","University of California-Davis","Standard Grant","Junping Wang","08/31/2013","$194,455.00","","deloera@math.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1271","0000, 9263, OTHR","$0.00","The goal of this project is to develop algorithms and software in both algebraic-symbolic computation and computational geometry. The algorithms will be useful in a wide variety of problems of discrete mathematics, optimization, and computer science. First, regarding algebraic computation, the PI will use ideas from commutative algebra, real algebraic geometry, large-scale linear algebra, semidefinite analysis, and combinatorics to develop very fast algorithms that can solve large, but highly-structured systems of polynomial equations that carry an extra rich combinatorial structure (e.g., they are defined from a graph and their solutions correspond to colorings or matchings, or their group of automorphisms is very large). Although solving arbitrary systems of non-linear equations is a very hard problem in general, we have been able to provide fast solutions for unusually large polynomial systems with thousands of variables. The second area of work is more related to the geometry of convex polyhedra. It is common that software in Operations Research and Optimization requires a quick way to decide when a polyhedron is integrally feasible. Relying on computational geometry analysis, convex analysis and probability, the PI intends to develop fast heuristics for detecting the existence of a lattice point inside a polyhedron as well as for counting of all its lattice points. The PI hopes to incorporate these ideas to mathematical programming software.<br/><br/><br/>Software and Algorithms that solve systems of equations and inequalities or that decide whether a system of equations has a solution with integer numbers are extremely useful in all areas of mathematics, engineering, and beyond. Most of the problems we will consider are directly related to problems where one wishes to find an optimal arrangement or make best decisions with scarce resources. Examples include the problem of selecting the least expensive network connecting given sites or selecting the least expensive tour for visiting a given set of locations. Since such optimization problems appear in all areas of society (data mining, finances and economics, transport scheduling and circuit design, to name a few) but are very difficult to solve, researchers have explored special structures to be able to solve them in practice (e.g., in the case of the<br/>TSP) or settle for algorithms that compute near-optimal solutions. In our project we use non-traditional tools based on recent mathematical progress as a way to approach these very difficult problems. Last, but not least, the project also has a strong educational component for this project. The PI is committed to integrate this new knowledge within curriculum development, undergraduate research projects, training of graduate students and postdocs, and the development of new open source software tools for computational optimization. Several graduate and undergraduate students will play important roles in the project's development.<br/><br/>"
"0915214","Image Reconstruction In Diffuse Optical Tomography With Sparsity Constraints","DMS","COMPUTATIONAL MATHEMATICS, International Research Collab, EPSCoR Co-Funding","09/01/2009","08/03/2011","Taufiquar Khan","SC","Clemson University","Continuing Grant","Junping Wang","08/31/2013","$182,334.00","","tkhan13@uncc.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1271, 7298, 9150","0000, 5936, 5979, 9150, 9263, OTHR","$0.00","In this project, the PI will investigate Sparsity Constrained Regularization (SCR) for solving the Diffuse Optical Tomography (DOT) inverse problem. Two recent algorithms are available for the implementation of SCR in DOT: the Generalized Conditional Gradient Method with Sparsity Constraint (GCGM-SC) and the Generalized Semi-smooth Newton's Method with Sparsity Constraint (GSNM-SC). The efficacy of GCGM-SC has been demonstrated with both theoretical and numerical results for linear and some non-linear problems. GCGM-SC, which has also been derived for linear problems, minimizes a given functional, not necessarily convex, with sparsity constraint with respect to a given basis. In this project, the computational aspect of the DOT inverse problem will be investigated to (i) develop a reconstruction algorithm for DOT using GCGM-SC, (ii) devise a strategy for computing adaptive basis such as finite elements to take full advantage of the sparsity constraint idea, (iii) extend GSNM-SC to nonlinear problems and compare GSNM-SC to Tikhonov regularization, and (iv) perform convergence analysis of the proposed GSNM-SC and GCGM-SC in DOT. The interdisciplinary research project seeks to integrate education and research and foster an international exchange of students. The educational activities in the project will include: (i) international research experience for  students, (ii) student exchange between Clemson University and the University of Bremen, Germany, (iii) the incorporation of much of the research activities into teaching activities to help students bridge course materials with research, (iv) exposure of high school students to applied mathematics research, and (v) attraction of underrepresented groups to pursue applied<br/>mathematics.<br/><br/>Diffuse Optical Tomography (DOT) is a method for imaging a highly scattering medium using near infrared and visible light. For example, optical tomography of biological tissue has potential applications for the early detection of breast cancer. One major advantage of DOT is that it is less expensive and non-invasive as compared with x-ray mammography. DOT imaging technology also shows great promise as a tool for initiating discoveries in physics, biology, and medicine. However, despite its great potential, it has yet to be commercially or medically successful as the instability in image reconstruction results in the blurring of any resulting image. To overcome these difficulties, we will investigate novel mathematical techniques for image reconstruction. Research applications will vary from cancer detection in biomedical imaging to land mine detection in remote sensing to imaging objects in the ocean. Research results are also expected to contribute to scientific knowledge in neutron transport, transport in atmospheric science, photothermal spectroscopies and microscopies, laser pump probes, diffuse photon density waves and new tomography technologies, such as optical, electronic and thermal imaging, and biomedical diagnostics."
"0901385","Interactions between Representation Theory, Quantum Field Theory, Category Theory, and Quantum Information Theory; August 2009, Tyler, TX","DMS","TOPOLOGY, COMPUTATIONAL MATHEMATICS","05/01/2009","12/22/2008","Kazem Mahdavi","TX","University of Texas at Tyler","Standard Grant","Joanna Kania-Bartoszynska","04/30/2010","$10,000.00","Leonard Brown, Deborah Koslover","kmahdavi@uttyler.edu","3900 University Boulevard","Tyler","TX","757990001","9035655670","MPS","1267, 1271","0000, 7556, 9263, OTHR","$0.00","This conference is on Interactions between Representation Theory, Quantum Field Theory, Category Theory, Mathematical Physics, and Quantum Information Theory.  The conference will bring together experts in the areas of Topology, Hopf  Algebras, Quantum Topology, Category Theory, Representation Theory, Mathematical Physics, and Quantum Information Theory. These fields are interrelated and have motivated each others' development.   <br/><br/>The conference will be of benefit to American mathematics, mathematical researchers, and quantum computational researchers. Cross-disciplinary interactions are widely recognized for their value in stimulating new ideas and advances and will thus not only increase the knowledge of the participants, but could result in the new invention and new discovery."
"0914986","Modulation Splines","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","07/16/2014","Amos Ron","WI","University of Wisconsin-Madison","Standard Grant","Junping Wang","08/31/2015","$496,644.00","","amos@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1271","0000, 9263, OTHR","$0.00","This proposal centers on the introduction and analysis of a new class of spline functions,  the first such endeavor since the introduction of polynomial and exponential box splines in the mid 1980's. Coined in the proposal  ""modulation splines"", the novel class represents a dramatic departure from all classical spline paradigms: While modulation splines are ""splines"", i.e., smooth compactly supported piecewise-analytic functions over polyhedral domains, they are new even in one dimension.<br/>In the short term, it is proposed to develop the basic theory and fundamental properties of modulation splines, with emphasis on 2D constructions. In the longer term, it is envisioned that modulation splines will serve as the backbone of a new hierarchical anisotropic multivariate data representation methodology, representation that is different and complementary to the prevailing Fourier and wavelet ones. As is the case with all spline theories and constructions, the project lies at the interface between mathematical analysis and computational science. The impetus for the project, however, comes from a topic in non-commutative algebra known as Macdonald polynomials, a topic that is related to Lie algebras, and to group representations. As such, the project provides further evidence to the unlimited potential in the intraconnectivity within mathematical science, and is anticipated to provide a channel of cross-fertilization among analysis, computation, and algebra.<br/><br/>The project's core strength vis-a-vis NSF's broader merit criteria is the intrinsic significance of the research area: data representation in general, and spline approximation in particular, are critical disciplines in science, and progress in these areas may have a widespread multiplier effect in a broad range of disciplines. In fact, the theory and practice of  ""spline functions"" stands out as one of the most significant contributions of the mathematical community to science and technology.  Splines have become indispensable tools in computer-aided design and manufacturing of cars and airplanes, in the production of printers' typesets, in automated cartography, in the production of movies, and in many other areas, often concealed at the core of elaborate software packages. In addition to their direct utility for the representation of curves and surfaces, splines, in one as well as several variables, are the preferred backbone for the wavelet representation, and are the prevailing choice for smoothing subdivision algorithms in computer-aided geometric design. The recognition of the impact of the mathematical research of splines culminated earlier in this decade in the awarding of a Medal of Science to Carl de Boor by the U.S. president."
"0914345","Tractability of High Dimensional Problems for Quantum and Classical Computers","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","06/05/2011","Joseph Traub","NY","Columbia University","Continuing Grant","Leland Jameson","08/31/2012","$473,383.00","Henryk Wozniakowski","traub@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1271","0000, 9263, OTHR","$0.00","New proof techniques and new optimal algorithms and complexity<br/>results will be obtained under the grant. Many important scientific and<br/>engineering problems have continuous mathematical formulations. Since<br/>digital computers can only deal with numbers, continuous problem have to<br/>be discretized for input into the computer. Hence the information about<br/>the continuous mathematical problem is partial and contaminated and only<br/>an E-approximation can be obtained. Because the information about the<br/>continuous problem is partial and contaminated<br/>one can use adversary arguments pioneered by the investigators and their<br/>colleagues to obtain computational complexity and optimal algorithms for<br/>important problems. This may be contrasted with discrete problems such as<br/>integer factorization, where one has to settle for conjectures about the<br/>complexity hierarchy. A central issue is to determine for which settings<br/>and spaces a problem is tractable; that is, its complexity is not<br/>exponential. There is a huge literature on the computational complexity of<br/>d-dimensional problems. Most of these papers and books obtain results<br/>which are sharp with respect to 1/E but have, unfortunately, unknown<br/>dependence on d. But to determine if a problem is tractable, we need to<br/>know the dependence on both 1/E and d. This requires new proof techniques.<br/><br/>        Many important scientific and engineering problems involve a large<br/>number of variables. Equivalently they are said to be high dimensional.<br/>Examples of such problems occur in quantum mechanics, molecular biology,<br/>and economics. For example, the Schrodinger equation for p particles has<br/>dimension d = 3p; systems with a large number of particles are of great<br/>interest in physics and chemistry. This problem can only be solved<br/>numerically. In decades of work scientists have found that the problems<br/>get increasingly hard as p increases. The investigators believe this does<br/>not stem from a failure to create good numerical methods--the difficulty<br/>is intrinsic. The investigators believe solving the Schrodinger equation<br/>suffers the curse of dimensionality on a classical computer. That is, the<br/>time to solve this problem must grow exponentially with p. (A classical<br/>computer is any machine not based on the principles of quantum<br/>mechanics--all machines in use today are classical computers.) The<br/>investigators hope to show this problem is tractable on a quantum<br/>computer. Success in this research would mark the first instance of a<br/>PROVEN exponential quantum speedup for an important non-artificial<br/>problem.<br/>"
"0914021","Collaborative Research: Numerical Methods for Fully and Implicitly Nonlinear Equations","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/18/2009","Danny Sorensen","TX","William Marsh Rice University","Standard Grant","Junping Wang","08/31/2012","$120,000.00","","sorensen@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","0000, 9263, OTHR","$0.00","The main goal of this project is to further investigate the numerical solution of fully nonlinear elliptic equations such as Monge-Ampre?s, in order to extend previous work done with the support of NSF Grant DMS-0412267. The main findings of these previous investigations are that, after regularization of the data (which is not always necessary), well-chosen least-squares formulations in appropriate Hilbert spaces lead to robust solution methods able to compute classical solutions, or generalized ones if classical solutions do not exist. The objectives of the present project are: (i) To improve the performances of the iterative methods used to solve the least-squares problems. This will require the development of novel algorithms to solve the many (one per grid point) low dimensional nonlinear eigenvalue problems obtained from the decomposition of the least squares problems, since this results in a number of small but intricate constrained eigenvalue problems. (ii) To demonstrate the effectiveness of these new algorithms on a variety of test problems (Monge-Ampre, Pucci, Gaussian curvature, sigma-2, in dimension 2, 3, and even 4 for the Pucci problem, using parallelization). (iii) To determine whether the remarkable homogenization properties observed in two-dimensions for some of these fully nonlinear elliptic equations when a coefficient in the operator varies periodically or randomly in space persist in higher dimensions. (iv) To apply, ultimately, the above methodology (or close variant of it) to the solution of some implicitly nonlinear partial differential equations from non-smooth differential geometry that model folding phenomena. <br/><br/>What motivates these investigations is the fact that fully nonlinear elliptic equations play an important role in areas as diverse as material sciences, nonlinear elasticity, fluid mechanics, atmospheric sciences, nonlinear elasticity, shape design in electrical and structural engineering (antennas, car shape,?), finance, applied and theoretical physics, differential geometry and others. The related mathematical problems have generated a large literature. In contrast these problems have the reputation to be difficult from a computational standpoint explaining why the computational and applied mathematicians have not made significant progress on their numerical solution. One of the goals of this project is to close the gap between the various communities concerned with fully nonlinear elliptic equations so that each of them will learn from the others, setting an example of interdisciplinary science. Such an effort will also benefit science and engineering professionals and students, via publications, dedicated web sites and post-graduate courses, lectures at conferences, and of course direct involvement for some graduate students. It will also stimulate the contributions of other scientists to these important areas. Since computational methods developed previously by the Principal Investigators and their associates are currently used in many areas of Science and Engineering, Academia and Industry, one can expect a similar endeavor for the results and products originating from this collaborative project."
"0914674","Reality, exactness, and computation in numerical algebraic geometry","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/23/2009","Daniel Bates","CO","Colorado State University","Standard Grant","Junping Wang","07/31/2012","$159,601.00","","bates@math.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1271","0000, 6890, 9263, OTHR","$159,601.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The methods of numerical algebraic geometry extend the reach of algebraic geometry to problems for which existing symbolic methods are not well suited, e.g., due to the number of variables or the inexactness of the coefficients.  The value of these methods is continuing to gain recognition.  For example, the algebraic geometry software packages Macaulay 2 and CoCoA are both actively developing either new homotopy modules or interfaces to existing numerical software, such as Bertini and PHCpack.  Despite the benefits of these numerical methods (e.g., parallelizability), there are a few drawbacks.  For example, to find the real isolated solutions of a polynomial system using homotopy methods, one must first produce all complex isolated solutions and then sort out those with imaginary part below a pre-chosen tolerance.  Also, one major benefit coming from numerical algebraic geometry is that it is simple to produce approximations of many generic points on any given irreducible component of an algebraic set.  However, there is currently no way to recover exact defining equations for the component.  This project has two directions.  In one, a new set of techniques, based on Gale duality and the Khovanskii-Rolle theorem, for finding only the real solutions of polynomial systems will be developed.  In the other, the simplicity of finding generic points on algebraic sets via numerical methods will be exploited.  The latter direction will include work on recovering exact defining equations via lattice basis reduction techniques such as LLL or PSLQ.  Both directions are expected to result in new, freely available software.<br/><br/>Polynomial systems of equations are ubiquitous throughout mathematics, science, and engineering.  An entire mathematical field - algebraic geometry - grew out of the need to find solutions to these sorts of equations.  Until the 1960s, though, there was no known general technique for solving such systems of equations.  However, the methods developed at that point require too much memory to be effective except for relatively small problems.  More recently developed methods - the numerical methods of Sommese, Verschelde, and Wampler, now collectively known as numerical algebraic geometry - allow for the solution of much larger polynomial systems, opening the application of algebraic geometry methods to a wider class of problems.  However, there are still drawbacks to these numerical methods.  The goals of this project include addressing two of these drawbacks.  In particular, the PI will work on developing efficient methods to find only those solutions that are of interest in real-world applications (i.e., real solutions rather than complex solutions) and on recovering valuable exact data from the approximate data that is provided as the output of these powerful new numerical methods."
"0911434","Augmented methods for Navier Stokes equations involving free boundary and moving interface and applications","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","07/29/2014","Zhilin Li","NC","North Carolina State University","Standard Grant","Junping Wang","03/31/2015","$368,819.00","","zhilin@math.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1271","0000, 9263, OTHR","$0.00","This proposal concerns the development and analysis of new numerical <br/>methods for Navier-Stokes equations involving discontinuities,<br/>free boundaries, moving interfaces, and problems on irregular domains. <br/>The new methods will be applied to several important applications in<br/>multi-phase flows, fluid structure interactions with interfaces, fluids, <br/>or solids. In the proposal, augmented immersed interface methods are proposed<br/>to tackle those difficulty problems. The new methods are based on sharp <br/>interface models (no smearing),  second order and implicit discretization. <br/>The new methods are proposed to be implemented to simulate several <br/>important applications such as the moving contact line problem using one <br/>and two phase flow models; non-extensible interfaces in incompressible flows, <br/>open-ended interfaces, and moving interfaces with masses. <br/> <br/><br/>The projects described in the proposal have wide applications in mathematical <br/>biology, material science, aerodynamic and mechanical engineering. The <br/>non-extensible interfaces have been used to model the motion of red blood <br/>cells in the life science. Efficient numerical simulations will have positive <br/>impact on early detection of illness related to the red-blood cells. The moving<br/>contact line problem has direct applications in the material science. Efficient <br/>numerical simulations using computers may avoid expensive experiments. This<br/>is true for other projects in the proposal as well. This proposal will also <br/>have positive effect on education by attracting under and graduate <br/>students to conduct research in this area. Some components of the<br/>projects will be designed as undergraduate projects. Furthermore, the research <br/>results can lead to a computer software package that would be useful to the<br/>computational science involving free boundary and moving interface, and <br/>problems defined on complicated domain.<br/>"
"0915253","New techniques in characteristic finite element methods for flow problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/31/2009","Jiangguo Liu","CO","Colorado State University","Standard Grant","Junping Wang","08/31/2012","$128,366.00","","liu@math.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1271","0000, 6890, 9263, OTHR","$128,366.00","This proposal is awarded using funds made available by the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The focus of the proposed research is to design, analyze, and implement accurate and efficient numerical methods for flow problems.  As is well known, incompressibility and convection-dominance in flow problems pose significant challenges to numerical simulators.  In this research project, the PI and his collaborators will develop numerical schemes based on the locally divergence-free (LDF) finite elements and characteristic tracking to overcome the aforementioned two major difficulties.  The LDF finite elements have strong potentials for both h-adaptivity (due to the independence of the basis functions to element geometry) and p-adaptivity (due to the hierarchical structure of the basis functions).  Discrete Helmholtz decomposition and hybridization will be utilized to decouple resulted discrete linear systems and reduce computational costs.  Higher order characteristic methods are developed for long-term simulations with increased accuracy.  These new techniques will be incorporated in the discontinuous Galerkin framework to develop fast and robust solvers for Stokes, Navier-Stokes, convection-diffusion-reaction, and magnetohydrodynamics equations.<br/><br/>The success of the proposed research will have direct impact on the efficiency and robustness of a large class of numerical simulators for real world problems such as groundwater contamination remediation, intracellular protein trafficking, nuclear waste disposal, oil recovery, weather forecast, and wildfire control.  The new mathematical methods and software modules developed in this project will provide reliable tools for a wide range of scientific computing tasks.  This project will also provide hands-on training opportunities for graduate students to gain a wide knowledge base and sophisticated skills to apply mathematics and computer simulations to a variety of applications in sciences and engineering."
"0914539","Computational Methods in Risk Management and Financial Engineering","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","06/11/2011","Mark Broadie","NY","Columbia University","Continuing Grant","Leland Jameson","08/31/2013","$564,331.00","Paul Glasserman, Steven Kou","mnb2@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1271","0000, 9263, OTHR","$0.00","This project addresses modeling and computational problems in financial risk management. Meeting current challenges in risk management requires combining mathematical modeling and computational techniques with an understanding of industry practice and the economic and regulatory environment. This project focuses on three topics from this perspective:  (1) large-scale portfolio risk measurement; (2) model calibration and model risk; (3) default clustering and portfolio credit risk.  Across these topics, the investigators will develop mathematical models of risk and computational methods for the efficient calculation of risk. Specific objectives include the development of efficient Monte Carlo methods for portfolio risk measurement, model calibration, and evaluating portfolio credit risk in the presence of default clustering.<br/><br/>In the wake of the current crisis, financial firms and regulators are rethinking simple approaches to risk taken in the past with a new focus on longer horizons and more thorough exploration of scenarios and potential losses.  This project will address mathematical modeling and computational problems that arise in the development of new approaches to risk measurement. Computational complexity is often a major constraint in realistic assessment of risk; advances in computing methods need to be closely tied to the development of new models to be effective.  More broadly, the field of quantitative finance is in transition, with the highest near-term priorities for research and education shifting away from structured derivative securities to risk management.  The investigators are active in this transition through their research, teaching, interactions with practitioners and regulators, and participation in educational standard setting through professional risk management organizations.  This project aligns with their activities across these areas, including mentoring students and developing and updating courses at all levels. The U.S.'s leadership in the intensely competitive global financial services industry has relied on a large influx of highly trained talent from the mathematical sciences.  This project will help maintain this base of talent and help shift academic research and training to the highest near-term priorities and to emerging areas of quantitative finance with the greatest growth potential.<br/>"
"0914923","Collaborative Research: Computational and theoretical approaches for the morphological control of material microstructures","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/21/2009","Shuwang Li","IL","Illinois Institute of Technology","Standard Grant","Leland Jameson","07/31/2013","$260,300.00","Xiaofan Li","sli15@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","MPS","1271","0000, 6890, 9263, OTHR","$260,300.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/><br/>The investigator and colleagues study the prediction and morphological control of two-phase microstructures in solid/liquid and solid/solid diffusional phase transitions. Much of the research in this area is concerned with detailed and extensive studies of complex patterns such as dendritic growing shapes. In many applications (e.g. castings), it is desirable to control the formation of dendrites and grow compact shapes, which, however, has been much less studied.  This project helps to fill the gap and aims to develop guidelines by which microstructures with desired shapes may be grown.  The research team plans to (1) develop a suitable nonlinear theory of compact precipitate growth including existence, uniqueness, and stability of self-similar shapes; (2) develop and employ state-of-the-art adaptive 3D numerical methods to test the validity and limitations of theory;<br/>(3) compare theoretical and numerical results with existing experiments to test the validity of the mathematical assumptions and to verify the accuracy of predictions derived from the theory and simulations.<br/><br/>Diffusional phase transformations deal with transformations of melts into precipitates (and vice-versa) as well as the separation of solids (e.g. metal alloys) into distinct phases.  These phenomena have importance for a variety of processes including casting, welding and soldering, crystal growth, and related problems concerning protein and macromolecular crystallization.  For example, crystal growth processes for technological applications began in the late 19th century, and form the cornerstone of virtually all modern semiconductor electronics and photonics today. The research activities will provide new mathematical theory and numerical simulations that can be used to develop guidelines for controlling the morphology of certain solidified materials.  The new mathematical theory and adaptive numerical methods developed in the project have applications to a broader set of related problems including multiphase flows, biostructures and growth of solid tumors. In addition, this project will provide valuable interdisciplinary training opportunities for young researchers."
"0940371","International Symposium on Fluid Turbulence; Beijing, China","DMS","COMPUTATIONAL MATHEMATICS, FD-Fluid Dynamics, NANOMANUFACTURING","10/01/2009","09/28/2009","Shiyi Chen","MD","Johns Hopkins University","Standard Grant","Junping Wang","09/30/2010","$25,000.00","Charles Meneveau","syc@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","1271, 1443, 1788","1303, 7556, 9263, EGCH","$0.00","Turbulence, ""the last unsolved problem in classical physics"" (Feynman), has been with us for over 100 years since Reynolds identified the laminar-turbulence transition in 1883. The correct description of fluid turbulence will impact such diverse fields as atmospheric pollution dispersion, weather, commercial chemical processes and aircraft design. Recent advances on information technology, turbulence modeling and experimental techniques have shown a great promise to overcome the fundamental difficulty in studying fluid turbulence.  In addition, modern database technology has made exploration of Terabyte datasets possible, which becomes a key component in understanding fluid turbulence. <br/><br/>The proposed International Symposium on Fluid Turbulence will be held at Peking University, Beijing, People?s Republic of China on Sept. 21-25, 2009. This Symposium aims to provide a forum to bring together scientists from all over the world to present their latest research results on fluid turbulence and to enhance academic exchanges. The participants will discuss and share developments and innovations in theoretical, numerical and experimental studies of fluid turbulence, covering all of its aspects. The focus will be given to physical and mathematical aspects of fluid turbulence and their connections to engineering modeling and applications. <br/><br/>Improved understanding of fluid turbulence will benefit diverse areas of science and technology, from engineering energy-efficient machinery, to predicting transport of pollutants in the atmosphere, and to long-term forecasting of Earth's climate. The purpose of this Symposium is to bring together the world leading experimental, theoretical and numerical experts with a key interest in recent advances in fluid turbulence, in particular Lagrangian and Eulerian Turbulence, non-ideal flows, statistical tools, sub-grid modelisation and data analysis.  This would help stimulate an intense and fruitful interaction between these communities, with the idea of encouraging collaborations. China is among the world fastest economical growth regions. It is expected that this Symposium will provide a direct platform for US participants to exchange ideas on fluid turbulence with scientists from over the world and to foster collaborations with Chinese scholars. This Symposium will also serve as a window to allow Chinese graduate students to know US turbulence research better. The requested financial support is intended to support 28 US invited speakers attending this Symposium."
"0914648","Collaborative Research: Computational Problems in Heterogeneous Nanomaterials","DMS","COMPUTATIONAL MATHEMATICS","08/15/2009","08/19/2009","Vivek Shenoy","RI","Brown University","Standard Grant","Junping Wang","02/28/2013","$250,000.00","","vbshenoy1@gmail.com","BOX 1929","Providence","RI","029129002","4018632777","MPS","1271","0000, 7237, 9263, OTHR","$0.00","In many applications ranging from energy to biomedicine, nanocrystalline materials, such as quantum dots and nanowires, promise to yield revolutionary new technologies. The realization of this promise is hindered by the challenges inherent in reproducibly fabricating nanocrystalline materials with controlled morphologies and compositions. These nanomaterials are typically heterogeneous and consist of alloys with multiple constituents. While there has been much work on formulating conditions under which spatially ordered nanocrystals with nearly uniform shapes and sizes may be produced, a quantitative description of the mechanisms that determine the spatial distribution of the alloy components, which is crucial to device performance, is still poorly understood. The investigators and their collaborators address this issue in this proposal. They study the nonlinear dynamics of heterogeneous, strained strained nanocrystalline materials by (1) developing and applying state-of-the-art adaptive numerical methods to large-scale computation and (2) performing analytical, numerical and modelling studies of important constituent processes. The investigators focus on the dynamic, nonlinear coupling among shape, elastic stress and composition in the context of (i) the dynamics of thin film alloys and quantum dots under far-from-equilibrium processing conditions where there may be bulk and surface transport of the different constituents, as well as phase decomposition; and (ii) the coarsening dynamics and stability of capped nanocrystals. The cap material is needed in applications to provide the confinement potential for charge carriers as well as passivation against the external environment. These problems are characterized by the presence of multiple constitutive components, bulk-surface interactions, complex pattern formation and/or singularities (i.e. spatial complexity). The mathematical models involve high-order spatial derivatives (e.g. up to sixth-order), evolving free boundaries and highly nonlinear interactions that make analysis and simulation difficult, particularly in 3D. The highly nonlinear nature of these problems makes fast, accurate and robust numerical methods essential to their study. <br/><br/>Nanocrystalline alloy materials have physical properties that make them ideally suited for a wide range of potential applications including advanced electronic and magnetic devices as well as biological and chemical sensors. The properties of nanoscale devices are determined both by the spatial composition of the heterogeneous nanocrystal components and the nanocrystal geometry. Recent advances in experimental techniques have enabled the characterization of nanoscale composition variation in nanocrystals. <br/>However, a quantitative understanding of these variations remains elusive and yet is critical to device performance. The investigators and their collaborators address this issue by developing new mathematical models, theory and computational methods that make it possible to characterize and quantify the interactions among nanocrystal shape, elastic stress and composition. The investigators also consider capped nanostructures where the cap material provides protection from the environment that is needed in many applications including the use of nanocrystals in silicon-based electronic circuits. The interaction among the nanocrystalline and capping materials introduces additional complexity. These problems are multidisciplinary and progress requires the combined expertise of the investigators in materials science and applied and computational mathematics. Through this study, the investigators provide guidance in the quantitative interpretation of experimental measurements of composition variation in nanocrystals and suggest optimized processing conditions to achieve desired device shape, composition and performance. The project establishes a new collaboration between two institutions and provides interdisciplinary training of two Ph.D students and one postdoctoral researcher. In addition, the investigators build on their recent success and continue to develop and teach a course on crystal and epitaxial growth for gifted high school students as part of the Calif. State Summer School for Mathematics and Science (COSMOS) at UC Irvine. This course also helps to recruit new math and science majors and enhance the participation of high school students in research."
"0914987","Numerical optimization for large-scale experimental design of ill-posed inverse problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","05/21/2009","Luis Tenorio","CO","Colorado School of Mines","Standard Grant","Junping Wang","08/31/2012","$173,348.00","","ltenorio@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","MPS","1271","0000, 9263, OTHR","$0.00","Inverse problems play a key role in a variety of fields such as computer vision, geophysics and medical imaging. The proposed work focuses on experimental design of ill-posed inverse problems; a field that has not received sufficient attention in the inverse problems community where the focus is usually on the analysis of the inverse problem given the data. Obviously, this already imposes restrictions on the quality of the possible solutions.   On the other hand, the objective of the funded work is the study of an  important pre-data acquisition question: How should the experiment be conducted to obtain optimal data given the physical constraints and available resources? Solutions to this question require techniques from numerical optimization, statistics and inverse problem theory. In particular, the problem of experimental design can be cast as a bilevel optimization problem that consists of two nested optimization problems. The proposed work is a study of design criteria and new numerical algorithms for the solution of the bilevel optimization problems that arise from them.<br/><br/><br/>This work  will address the fundamental question of experimental design of   ill-posed inverse problems. Such problems arise in the design of any practical experiment  or instrumentation from geophysical  and medical imaging to the  production of better vision systems. This work will develop new criteria for the design and development of new algorithms that will enable its numerical implementation. The results of the research will be applied to electromagnetic imaging, a field that is  routinely used in geophysics and medical physics. <br/>It will lead to better experiments that yield better images and as such, will assist in  the decision making of geoscientists and and physicians."
"0913695","Numerical methods for elliptic systems with complex interfaces","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","07/05/2011","John Strain","CA","University of California-Berkeley","Continuing Grant","Junping Wang","08/31/2013","$409,863.00","","strain@math.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1271","0000, 9263, OTHR","$0.00","The proposed research builds general theory, fast new algorithms and efficient open-source implementations for three computational methods in elliptic interface problems: 1. Locally-corrected spectral methods solve general elliptic systems with complex interfaces by the following new techniques.  Distributional potential theory converts stable first-order formulations into bounded invertible operator equations. Ewald summation splits elliptic fundamental solutions into singular local corrections and smooth global terms.  Off-diagonal low-rank structure couples with singular quadrature and graded meshes to enable fast direct solvers.  2. Geometric nonuniform fast Fourier transforms evaluate sparse sets of Fourier coefficients for distributions in elliptic solvers, invert boundary integral operators, and address a variety of computational problems.  3. Efficient contouring and distancing algorithms convert between convenient explicit and robust implicit interface representations. They employ fast computational geometry, global topology resolution, and high-order local piecewise-polynomial parametrizations. Solvers for elliptic systems with complex 3D piecewise-smooth interfaces convert explicit CAD formats to implicit representations for robust topological analysis.<br/><br/>A vast array of technological processes, from global warming to cancer therapy, from shape optimization to crack propagation, are modeled by elliptic systems of partial differential equations with complex material interfaces.  Efficient general computational techniques for elliptic systems are essential in designing better models.  Many existing codes for specific systems and interfaces use computational resources efficiently, but require excessive reprogramming to investigate new models and thus hamper scientific inquiry.  The proposed research will develop, analyze, implement and disseminate a collection of robust, efficient and accurate new computational methods for the solution of general elliptic systems with complex interfaces.  This interdisciplinary enterprise involves researchers and students in mathematics, physics, engineering and computer science, and benefits a wide range of scientific endeavors."
"0914580","Mathematical Models & Computational Algorithms for Image Processing, computer Vision & Computer Graphics","DMS","COMPUTATIONAL MATHEMATICS","09/01/2009","08/08/2011","Christoph Thiele","CA","University of California-Los Angeles","Continuing Grant","Junping Wang","08/31/2012","$600,001.00","","thiele@math.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1271","0000, 9263, OTHR","$0.00","Abstract - Thiele<br/><br/>This proposal addresses three sets of problems in the intersection of image processing, computer graphics and computer vision: (1) image segmentation with applications to object tracking in videos, (2) inpainting problems, both in the pixel domain and the transform domain, and (3) applications of variational PDE-based models to image processing on manifolds. The approach makes use of powerful concepts from computational mathematics, such as duality, convexification, non-smooth optimization, fast combinatorial optimization, numerical PDE techniques, harmonic analysis, and computational differential geometry. The research is focusing on key issues, such as computational efficiency, feature extraction, object tracking, global analysis, and optimization, and preserving both geometric and texture information, which are keys to further advances. The research will impact many areas ranging from entertainment through homeland security and medical imaging.<br/><br/>For image segmentation, typical models are generally nonconvex and admit many local optimal solutions, making them sensitive to initial guesses. Moreover, numerical algorithms can be trapped in local non-optimal minima. To obtain better segmentation algorithms, we are extending a novel convexification technique (through the deep connection between TVL1 models and level sets) developed earlier for the Chan-Vese segmentation model to other segmentation models, such as non-local segmentation models inspired by texture synthesis techniques. For inpainting, recent advances include PDE and geometry techniques, texture synthesis, and a hybrid framework for inpainting missing transform (e.g. wavelets or Fourier). This research is examining the synergy between these methods and deriving models and algorithms that combine the best features of each. For image processing on manifolds, we are using conformal mapping techniques and PDE-based image processing models to derive efficient algorithms for general surfaces. New applications include the automatic tracking of landmarks on general surfaces and the incorporation of shape information in landmark matching.<br/><br/>"
"0847241","CAREER: Development and Applications of Discontinuous Galerkin Methods","DMS","COMPUTATIONAL MATHEMATICS","07/15/2009","03/07/2014","Fengyan Li","NY","Rensselaer Polytechnic Institute","Standard Grant","Junping Wang","06/30/2015","$582,112.00","","lif@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","MPS","1271","0000, 1045, 6890, 9263, OTHR","$582,112.00"," <br/>This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/> <br/>The use of the basic principle of conservation has led to accurate and reliable mathematical models of the physical world. Such models, known as conservation laws, together with the related models such as Hamilton-Jacobi equations, display rich features in solutions and therefore continue to challenge the computational scientists. The primary objective of this CAREER proposal is to make several important steps towards the development and applications of high order accurate methods for solving these nonlinear equations.  Discontinuous Galerkin methods will constitute the core methodology of this effort, this is due to their great flexibilities and capabilities in accurately and reliably simulating complicated problems. The project will comprehensively cover the algorithm design, analysis, implementation and applications.<br/><br/>The success of the proposed research will have direct impact on the efficient and robust modeling of problems in areas as diverse as optimal control, image processing, computer vision, astrophysics, space physics and energy physics. The resulting ideas and methodologies will bring new components to reliably simulate nonlinear problems and complex systems arising in science, physics and engineering. Besides training graduate and undergraduate students in conducting research in computational science, mentoring women students in mathematics, the proposed educational and academic activities will also enhance interaction and collaborations among regional research groups."
"0917661","Workshop on Stochastic Multiscale Methods: Mathematical Analysis and Algorithms; August 2009, Los Angeles, CA","DMS","COMPUTATIONAL MATHEMATICS","08/15/2009","08/04/2009","Roger Ghanem","CA","University of Southern California","Standard Grant","Leland Jameson","07/31/2011","$24,890.00","","ghanem@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1271","0000, 7556, 9263, OTHR","$0.00","Exchanging information across scales is one of the most significant challenges in multiscale modeling and simulation.  By necessity, and naturally within a multiscale context, information is truncated as it is presented to a coarser scale, and is enriched as it traverses the opposite path. Information is lost and corrupted as it is, respectively, upscaled and downscaled.  Mitigating these errors can be set on rigorous ground through a probabilistic description of information, whence finite-dimensional approximations of measures provides an analytical path for describing the coarsening and refining of information. Stochastic analysis, therefore, provides a rational context for the analysis of multiscale methods.  This workshop on ""Stochastic  Multiscale Methods: Mathematical Analysis and Algorithms""<br/>will serve to define challenges and opportunities in the development of stochastic multiscale methods for various problems in science and engineering.  Issues of uncertainty quantification, model validation, and optimization under uncertainty have taken center stage in many areas of science and engineering.  Likewise, multiscale modeling and computing capabilities are becoming the standard against which model-based predictions are gauged.  It thus behooves the scientific community, at this juncture, to elucidate the mathematical foundation of stochastic multiscale concepts so as to ensure a steady evolution of scientific capabilities as engines of economical growth societal well-being.  This workshop will initiate a dialog between mathematicians, mechanicians, and computational scientists that will lay the foundation for an accelerated growth in stochastic multiscale methods.<br/><br/>Rapid growth in computational resources has heightened the expectation that scientific knowledge can indeed be a driver for societal well-being and betterment.  At the same time, our ability to measure the natural and social world around has significantly increased, aided by technological development in sensors, the internet, and other modalities of communication.  Science is thus faced, simultaneously, with a complex description of reality at an unprecendented resolution, and the possibility to describe this reality with mathematical models of increasing complexity.<br/>Multiscale descriptions of physical problems can be viewed as attempts to take advantage of these new oppotunities, while tackling the conceptual challenges they inevitably present.<br/>The communities of stochastic analysis and computational science have evolved essentially along separate paths. The path forward, however, in the direction of disruptive scientific impact, requires significant exchange and<br/>collaboration.   It is the intent of this Workshop ``Stochastic <br/>Multiscale Methods:<br/>Mathematical Analysis and Algorithms'' to bring together leading researchers in these two fields with view to delineate new horizons and forge new synergies that will accelerate the evolution of multiscale capabilities to become an enabler of scientific and economic progress.<br/>"
"0914706","Hybrid Gaskinetic Computation Scheme for flows with Multi-scales and Multi-physics","DMS","COMPUTATIONAL MATHEMATICS","07/15/2009","09/30/2009","Chunpei Cai","NM","New Mexico State University","Standard Grant","Leland Jameson","06/30/2013","$184,220.00","","ccai@mtu.edu","Corner of Espina St. & Stewart","Las Cruces","NM","880038002","5756461590","MPS","1271","0000, 6890, 9150, 9263, OTHR","$184,220.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The accurate and efficient computation of complex gas flows  is a very challenging task that requires significant computing resources.  Hypersonic flows around an entry vehicle, for example, can experience a full range of Knudsen (Kn) numbers, and can include thermochemical nonequilibrium (NE) effects, with radiation heat transfer. Essentially, these flows require modeling from atomic, mesoscopic and macroscopic scales, employing quantum, statistical  and classical fluid mechanics. Because of associated large rarefication effects, traditional computational schemes based on continuum Navier-Stokes equations cannot always simulate those complex flows. New computation schemes are preferred.  The investigators propose a consistent time-accurate Hybrid gaskinetic Bhatnagar-Gross-Krook method (H-BGK) scheme, valid in the full Kn number range to simulate hypersonic aerothermodynamics  flows. H-BGK method provides automated sub-domain solutions by direct BGK method and by the gaskinetic BGK method of Xu (BGKX) in the high and low Kn regimes, respectively. Direct BGK employs the Shakov model using quadratures for discrete velocity integration. The BGKX solution can produce accurate heat rate prediction for near-continuum thermochemical NE flows. The proposed work focuses on developing general, accurate and efficient gaskinetic schemes which can compute complex gas flows with multiphysics, multiscales, multispecies, multidimensions and a wide range of Kn numbers. These new hybrid BGK schemes are original in design and implementation. The proposed method, with pertinent modifications, can be applied to a variety of flows with multiple Kn numbers. The investigators will compare their simulation results with those obtained by different numerical methods and experiments. <br/><br/>The investigators propose a powerful hybrid computational scheme which can simulate complex gas flows (with multiscales, multispecies, multidimensions, and multiphysics, especially with a large range of rarefication effects).   Once developed, this unique new scheme can both accurately and efficiently simulate many challenging complex gas flows.  Traditional gas simulation schemes are much inferior because they deal with dense gas flow situations rather than diluted ones. With its powerful capabilities, this new scheme can be applied in many fields and events. It can address gas flow problems in physics, astronomy, aerospace and mechanical engineering, and is also relevant to vacuum and semi-conductor industries.  There are numerous possible space related scenarios. For example, NASA must consider the drag and heat flux of hypersonic flows over a re-entry space shuttle. The new scheme addresses the drag and heat flux at high altitude regions with low atmospheric density as the spacecraft returns to earth. Another low-density application involves materials processing inside vacuum chambers."
"0915077","Overcoming the Bottlenecks in Polynomial Chaos: Algorithms and Applications to Systems Biology and Fluid Mechanics","DMS","COMPUTATIONAL MATHEMATICS","10/01/2009","09/21/2009","George Karniadakis","RI","Brown University","Standard Grant","Leland Jameson","09/30/2013","$200,000.00","","George_Karniadakis@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1271","0000, 9150, 9263, OTHR","$0.00","The PI proposes to develop new effectrive methods for solving stochastic partial differential equations (SPDEs). In particular, the PI will address two outstanding issues in polynomial chaos (PC) methods for modeling uncertainty in computer simulations of physical and biological systems. The first one is related to treating effectively many stochastic dimensions while the second one is related to modeling accurately white noise. Such problems arise in applications with small relative correlation length or large number of independent random parameters. The two approaches are complementary to each other as problems with very small correlation length can be effectively modeled by white noise processes.The new ideas are the use of ANOVA decomposition and the introduction of proper weighted Wiener chaos<br/>spaces and stochastic convolution products. ANOVA provides a hierarchical functional decomposition that exploits the effective dimensionality of the system. This type of dimension-wise decomposition can effectively break the curse of dimensionality in certain approximation problems in which the effective dimensionality is much lower than the nominal dimensionality. In preliminary work, the PI has demonstrated the effectiveness of the new approach in approximating efficiently problems with more than 500 dimensions.<br/><br/>The proposed work will have significant and broad impact as it will set rigorous foundations in uncertainty quantification, data assimilation and sensitivity analysis for many physical and biological systems. For example, in computational fluid dynamics, it will establish a robust and efficient framework to endow simulations with a composite error bar that goes beyond numerical accuracy and includes uncertainties in operating conditions, the physical parameters, and the domain.The proposed work is transformative as it will make stochastic simulations the standard rather than the exception. It will also affect fundamentally the way new experiments are designed and the type of questions that can be  addressed, while the interaction between simulation and experiment will become more meaningful and more dynamic.<br/>The PI plans to incorporate these new ideas in engineering and applied mathematics courses at Brown. Sponsored graduate and undergraduate students will be involved in this research and will interact with<br/>all senior personnel that includes several international visitors. The PI will work closely with undergraduate students who are involved with outreach activities through two very effective organizations at Brown that target women in science and engineering and also middle school students. He also plans outreach activities for inner-city high schools by developing along with the teachers computer-based interactive math learning strategies. Preliminary results working with the MET school have been very encouraging, and the PI plans to expand this activity nationwide.<br/>"
"0915013","Mesoscale Computational Modeling and Analysis of Materials Microstructure","DMS","COMPUTATIONAL MATHEMATICS","09/15/2009","09/15/2009","Maria Emelianenko","VA","George Mason University","Standard Grant","Leland Jameson","08/31/2013","$267,817.00","","memelian@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1271","0000, 7237, 9263, OTHR","$0.00","Most metallic and ceramic materials used in a variety of applications, including aircraft, automobiles, and devices such as computers are polycrystalline, i.e. are made up of many microscopic crystals (grains) held together by boundaries. Grain boundaries have a property called ?grain boundary energy?, which is responsible for how strong the grains are connected to each other. Depending on the energy, polycrystalline structures may have very different properties. Very little is known about this energy and its dependence on the crystallographic nature of the boundary. The main objective of this work is to create efficient mathematical models and numerical algorithms for predicting and controlling materials microstructure by quantifying the kinetics of the coarsening processes and understanding the influence of grain boundary energy on microstructural evolution. The novelty of the proposed approach lies in placing focus on understanding statistical effects of the topological reconfigurations during the grain boundary network evolution. This project will allow to develop a new stochastic framework for mesoscopic analysis of materials based in part on recently discovered evolution equations and an in-depth study of grain disappearance rates. This research will link coarsening processes common to nearly all materials with underlying stochastic processes that characterize specific microstructures by means of accurate and validated computational modeling based on random walk theory, modulated Poisson processes and distributed Boltzmann equations. Significant effort will be dedicated to optimizing performance of numerical algorithms, such as numerical schemes for solving fractional integro-differential equations with nonlocal kernels and methods for parameter estimation in stochastic processes. The models and algorithms developed and analyzed in this project will offer an accurate and low cost alternative to large-scale simulations and other numerical techniques traditionally used in studying interfacial properties of complex materials. <br/><br/>This work will lead to control over grain growth kinetics, a primary issue for materials science applications, and to a deeper understanding of the materials parameters and their interaction across different scales. This will have a direct impact on many practically important areas by advancing the synthesis of novel ?smart? materials - highly sophisticated materials, possessing very specific set of properties to target particular applications necessary to meet increasing demands of the society in the era of tremendous economic and environmental challenges. For instance, functional materials that can reliably withstand extreme thermal and pressure environments are indispensable for solving energy-related problems such as improving power plant efficiency, while lighter weight high strength components are required for designing new generation vehicles with reduced fuel consumption. The models developed by the investigator and collaborators will be used to build a suite of algorithms capable of simulating and analyzing microstructures that can be predisposed to certain type of behavior under external stimuli. These methods will allow for faster and more reliable analysis, control and optimization of materials properties and will equip engineers with powerful predictive tools that can be readily transferred to industry for modeling the processing of commodity materials. <br/>"
"0914852","A High Order Adaptive Semi-Lagrangian WENO Method for the Vlasov Equation","DMS","COMPUTATIONAL MATHEMATICS","07/15/2009","11/16/2011","Jing-Mei Qiu","CO","Colorado School of Mines","Standard Grant","Leland Jameson","06/30/2013","$253,981.00","","jingqiu@udel.edu","1500 Illinois","Golden","CO","804011887","3032733000","MPS","1271","0000, 6890, 9263, OTHR","$253,981.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The investigator and her colleagues consider a novel high order adaptive semi-Lagrangian approach for kinetic plasma simulations. The major challenge of kinetic simulations is the huge computational cost, from the high dimensionality (3-D in physical space and 3-D in phase<br/>space) and from the spatial and temporal multi-scale features of the problem. To address these challenges, the proposed methodology consists of three advanced numerical techniques: 1) a conservative semi-Lagrangian method with high order weighted essentially non-oscillatory (WENO) reconstructions; 2) a high order Strang split spectral deferred correction method to bridge time scales of different species and integrate in time with high order accuracy; 3) mesh adaptivity by incorporating the Strang split Semi-Lagrangian WENO method into the framework of adaptive mesh refinement. As a result of high order accuracy and adaptivity in both space and time, the investigator and her collaborators hope to apply the algorithm in realistic plasma applications with affordable computational cost by using relatively coarse and dynamically adaptive mesh. The eventual goal is to enable large-scale parallel simulations in order to predict/confirm/explain the physical phenomenon observed in a broad range of applications.<br/><br/>The investigator and her collaborators aim at developing robust and highly efficient numerical algorithms. The well-developed algorithm will have a direct impact in plasma applications, such as developing fusion energy, the modeling of magnetosphere, among many others.  Besides, there is large room for further extensions and applications.  The algorithm can be extended to solve the more general Boltzmann equation. It can served as a microscopic solver in a mix kinetic-hydrodynamic model via the heterogeneous multi-scale method.  While designed for the plasma simulations, the proposed algorithm can be further extended to astrophysics applications, semi-conductor device simulations among many others. The broader impact comes from the multi-disciplinary nature of the proposed research. The proposed research will initiate and serve as a solid foundation for collaborative research work with applied mathematicians, plasma physicists and astrophysicists. The collaborative work will not only expedite the development of the research in both sides of collaborations, but also help training graduate students with a diverse background and multidisciplinary skills."
"0917889","DMS-CM: Workshop of the Southeastern Clustering and Ranking Group; August 2009, Charleston, SC","DMS","COMPUTATIONAL MATHEMATICS","06/15/2009","06/12/2009","Amy Langville","SC","College of Charleston","Standard Grant","Tanya Kostova Vassilevska","05/31/2010","$9,940.00","","langvillea@cofc.edu","66 GEORGE ST","CHARLESTON","SC","294240001","8439534973","MPS","1271","7556, 9178, 9263, SMET","$0.00","The Southeastern Clustering and Ranking Group (PI Amy Langville, College of Charleston, Carl Meyer, North Carolina State University, Davidson College) focuses on the computational mathematics associated with ranking and clustering items. This work uses numerical linear algebra algorithms applied to very large datasets from the group's industrial collaborators. We propose to host a three-day student-centered workshop with industrial participation to be held in mid-August 2009. This workshop will be an information exchange and planning meeting, allowing members of the group to establish relationships that will facilitate a planned Wiki website and student exchange program."
"0849383","Special Meeting:  Foundations of Computational Mathematics","DMS","ALGEBRA,NUMBER THEORY,AND COM, TOPOLOGY, COMPUTATIONAL MATHEMATICS, COMPLEXITY & CRYPTOGRAPHY, NUM, SYMBOL, & ALGEBRA COMPUT","08/01/2009","08/04/2009","Adrian Lewis","NY","Cornell University","Standard Grant","Tanya Kostova Vassilevska","07/31/2010","$110,000.00","Michael Todd","adrian.lewis@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1264, 1267, 1271, 7927, 7933","0000, 7556, 9263, OTHR","$0.00","This thematic program will explore the evolving interface between mathematics and computation.  A wide variety of mathematical disciplines are involved, including computational algebraic geometry and symbolic computation, computational number theory, computational geometry, topology and dynamics, complexity and computability in real computation, computer science and optimization theory.  Algorithms will be central, not only in their traditional role in the invention and analysis of numerical methods, but also as fundamental theoretical tools for mathematical experimentation and discovery, as proof techniques, and as models of computability and tractability.  By assembling a large, broad group of participants, ranging from established mathematicians and computer scientists to graduate students, the program will transform our knowledge of computational mathematics well into the future.<br/><br/>The Fields Institute in Toronto will host this timely six-month program, capitalizing on rapidly evolving networks of international expertise linking mathematics and computation; NSF funding will support participation of junior U.S. researchers and graduate students.  The impact of constantly increasing computational power on applied mathematics, science and engineering, motivated by new applications such as encryption, data mining, and web search, has inevitably led to the study of the computational tools themselves, creating and reinvigorating a rich spectrum of mathematical disciplines broadly represented at this special semester.  The program involves three one-week focus workshops, special lecture series and graduate courses (delivered in particular by two distinguished U.S. female computer scientists, Eva Tardos and Lenore Blum), diverse senior and junior researchers and postdoctoral fellows making long- and short-term visits and engaging in day-to-day interaction, and extensive efforts to disseminate the emerging new science.  These diverse activities will integrate cutting-edge research, interdisciplinary synergy, and learning and training opportunities for an array of participants as broad and inclusive as possible.<br/>"
"0914892","A New Initiative in Computational Mathematics at Princeton","DMS","COMPUTATIONAL MATHEMATICS","08/01/2009","07/23/2009","Ingrid Daubechies","NJ","Princeton University","Standard Grant","Junping Wang","07/31/2012","$980,043.00","Arthur Calderbank, Amit Singer","ingrid@math.duke.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","MPS","1271","0000, 6890, 9263, OTHR","$980,043.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>Computational Mathematics has been central to the Program in Applied and Computational Mathematics(PACM) since its inception in the mid 1970s. This tradition is rooted in the traditional, in uential and powerful fields of computational fluid dynamics, control theory and operations research, fields in which PACM is committed to continue to invest energy and resources, and in which it is well represented by dynamic and top-level researchers branching out into quantum chemistry, materials science and nanotechnology. Parallel to these traditional computational mathematics elds, recent years have seen exciting new developments in mathematics and computer science, which have opened up new domains of application for computational mathematics. These come with their own challenges, for which new approaches and tools must be and are being developed. Machine learning and compressive sensing are two typical examples; they draw not only from traditional linear-algebra-based numerical analysis or approximation theory, but also from information theory, graph theory, the geometry of Banach spaces, probability theory, and more. This proposal seeks to fund the research of three PACM faculty drawn to these new computational challenges, who are also finding increasingly that their different fields of expertise all contribute to the development of dramatically more effective tools. This contiuence of interests, and the conviction that joining their efforts will produce a whole that exceeds the sum of its parts, constitute the engine that drives the approaches proposed here. The PIs will bring to bear harmonic analysis, combinatorial group theory and statistical data analysis approaches on the construction of algorithms that address large-scale computational problems not yet solved by traditional approaches.<br/><br/><br/>Whereas acquiring a sufficient amount of data used to be the main preoccupation of scientists and engineers, they now often find themselves deluged with massive amounts of often unstructured data, of which much more can be saved than was possible before. Faced with an enormous mass of unstructured, noisy data, the challenge is then to identify and study the hidden structures within the data (often much lower dimensional than the data set itself). This is like searching for needles in haystacks, when one doesn't even know how many (if any!) needles are hidden among the hay. In a very real sense, this is similar to the learning task accomplished by babies, who, in a few years' time, learn to make sense of the overwhelming amount of information provided to them by their senses, and succeed in learning many skills, including language, from identifying the structure in these data. The PIs will study how to discover such hidden structure in several applications. These include the determination of the geometric structure of biological molecules for which standard X-ray Crystallography doesn't work because the substance cannot be crystallized; the study of masters' paintings to learn how to better distinguish original from fakes or copies; the identification of the structure in very large data arrays for which only a small percentage of the entries are known, so that the others can be inferred; the detection of anomalies in internet or other network traffic."
"0946649","Supplemental Funding for a Conference on: Combinatorics, groups, algorithms, and complexity; March 2010; Columbus, OH","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS, TRUSTWORTHY COMPUTING, COMPLEXITY & CRYPTOGRAPHY","12/01/2009","09/18/2009","Akos Seress","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Tomek Bartoszynski","11/30/2010","$20,000.00","Mario Szegedy","akos@math.ohio-state.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1264, 1271, 7795, 7927","0000, 7556, 9263, OTHR","$0.00","  ABSTRACT<br/><br/>Principal Investigator: Seress, Akos    <br/>Proposal Number: DMS - 0946649<br/>Institution: Ohio State University Research Foundation <br/>Title: Supplemental Funding for a Conference on: Combinatorics, groups, algorithms, and complexity<br/><br/>A conference on ``Combinatorics, groups, algorithms, and complexity'' will be held on March 21-24, 2010,  at The Ohio State  University. The objective of the conference is to provide a forum to explore the manifold interactions between the branches of mathematics and computer science named in the title. Ever since the inception of the polynomial-time paradigm in the late 1960s, theoretical computer science has been a major consumer of concepts and techniques developed in combinatorics, and, conversely, the conceptual frameworks developed in algorithms and complexity theory have transformed much of combinatorics. As the relatively young areas of combinatorics and complexity theory have matured in the past decades, algebraic methods have become increasingly relevant to each. Group theory has played an important role both as a source of techniques and as a subject of rigorous algorithmic study, with the subarea of asymptotic group theory leading the way in the interaction.<br/><br/>The subareas to be covered include but are not limited to arithmetic combinatorics, asymptotic group theory, automorphism groups of combinatorial structures, vertex-transitive graphs, expanders, combinatorial models of computation (circuits, decision trees, communication complexity, etc.), probabilistically checkable proofs and approximation algorithms, derandomization, algebraic graph theory, abelian sandpiles, algorithmic problems in combinatorics and algebra, mathematical problems motivated by problems in algorithms and complexity theory. This interdisciplinary conference will focus on the crossfertilization between the areas of combinatorics, group theory, algorithms, and complexity theory, the first two being areas of mathematics and the last two - areas of theoretical computer science.   Each of these areas has significantly contributed to the development of the others over the past decades.  It is expected that the conference will increase our understanding of the deeper mathematical issues that underlie the connections between these areas, with implications to each of the areas concerned.   A particular occasion for this meeting will be the 60th birthday of Laszlo Babai whose work has been influential in developing connections between these fields.<br/><br/><br/><br/>"

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1720114","Multilevel Methods for Numerical Modeling with Applications in Hydrogeology","DMS","COMPUTATIONAL MATHEMATICS","07/15/2017","07/06/2017","Ludmil Zikatanov","PA","Pennsylvania State Univ University Park","Standard Grant","Leland Jameson","06/30/2021","$195,275.00","Tess Russo","ltz1@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1271","9263","$0.00","The project focuses on the development of new mathematical tools which improve our understanding and application of advanced computational methods. The research explores novel as well as established practical algorithms which are then validated and verified in a variety of hydrogeological scenarios. One major goal of computational mathematics research, in general, is to improve our understanding of numerical algorithms and to make them more efficient and accurate. However, the tasks of improving methods and then applying them are often completed by distinct groups of researchers and have long transfer times between development and application.  Often, the theoretical findings and the heuristic algorithms developed by practitioners follow different trajectories. Indeed, many valuable numerical techniques have been proposed and used by practitioners without much theoretical justification or for a narrow set of problems. Concurrent improvements based on new mathematical insights are often unrelated to practical problems. This research addresses the disparity between the two disciplinary trajectories by reconnecting advanced and abstract mathematical theories with practice.  The project has the potential to impact a wide range of applications, including for example the  simulation of variably saturated flow.<br/><br/>This project is concerned with the development and analysis of adaptive, conservative and monotone discretizations that are extendable to any order for the solution of nonlinear partial differential equations, such as Richards' equation used to simulate variably saturated flow and Biot's model in poroelasticity. Typically, such discretization methods result in large-scale, ill-conditioned linear systems. The efficient solution of such systems, generally non-symmetric and indefinite, is crucial for the performance of overall  numerical simulation as it consumes the larger part of the computing resources.  Part of this project includes the development of a class of efficient and adaptive multilevel solvers capable of generating flexible hierarchies of spaces. Such hierarchies are useful also in filtering and representing sparse data sets, robust with respect to the structure and the type of data: smooth, oscillatory or combinations of the two. In the targeted applications in hydrogeology, the research will be on techniques which efficiently approximate elevation, groundwater head, precipitation, and other relevant hydrogeological spatial data."
"1720001","Efficient Ensemble Methods for Predictive Fluid Flow Simulations Subject to Uncertainty","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/19/2017","Nan Jiang","MO","Missouri University of Science and Technology","Standard Grant","Leland Jameson","02/28/2021","$149,938.00","","jiangn@ufl.edu","300 W 12TH ST","ROLLA","MO","654096506","5733414134","MPS","1271","9150, 9263","$0.00","Uncertainty quantification is a central topic in predictive science, where model predictions with quantified uncertainties are critical for understanding and predicting scientific phenomena and making informed decisions based upon these predictions. The applications include energy (nuclear, wind, solar, etc.) generation, control and manufacturing, atmosphere-ocean modeling, weather prediction, surface water and ground water contamination, and so on. For all of these applications, the model problem is subject to numerous sources of uncertainty that include uncertain model parameters, forcing functions, initial conditions, and boundary conditions. For instance, in numerical weather prediction, to deal with uncertain initial conditions the weather model needs to be run multiple times with different initial conditions to generate an ensemble of possible model outputs, which will be analyzed and predictions made according to these data. This process is called ensemble forecasting, which is commonly done at all major operational weather prediction facilities worldwide, including the U.S. National Centers for Environmental Prediction and European Centre for Medium-Range Weather Forecasts (ECMWF). One common problem faced in these calculations is the excessive cost in terms of both storage and computing time. For many complex systems, especially those that deal with large spatial scales, running the model once is already very expensive. Running the model multiple times within a given limited computational time is very challenging even with modern supercomputers, and is not feasible in most large-scale applications. An efficient ensemble simulation algorithm that can reduce the computing cost significantly is thus highly desirable. This project seeks to develop novel, efficient ensemble algorithms and their analytical foundation for fast calculation of flow ensembles that is required to account for uncertainties in predictive simulations of fluid flows.<br/><br/>The inevitable conflict of high-resolution single realizations and computing ensembles is a central difficulty in many engineering and geophysical applications that are subject to uncertainties in both input data and model parameters. The development of efficient methods that allow for fast calculation of flow ensembles at a sufficiently fine spatial resolution is of great practical interest. This research is to develop novel, efficient ensemble algorithms for fast calculation of flow ensembles and conduct rigorous numerical analysis for the new algorithms and methods. The first research problem is to develop new efficient ensemble algorithms to compute multiple realizations for the Boussinesq equations. This includes the development of partitioned ensemble algorithms so that highly optimized Navier-Stokes-equation codes can be used to solve the problem. The second is to advance higher-order time discretizations for ensemble algorithms based on artificial compression. The third problem is the development of novel, efficient ensemble algorithms for the fast calculation of flow ensembles with varying model parameters. The methods studied will allow efficient determination of the multiple solutions corresponding to many parameter sets."
"1664679","FRG: Collaborative Research: Computational Methods for Complex Fluids: Adaptivity, Fluid-Structure Interaction, and Applications in Biology","DMS","COMPUTATIONAL MATHEMATICS, FD-Fluid Dynamics, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY","08/01/2017","07/18/2017","Robert Guy","CA","University of California-Davis","Standard Grant","Yuliya Gorb","07/31/2022","$600,000.00","Becca Thomases, Gregory Miller","guy@math.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271, 1443, 7334, 7454","1616, 8007, 9263","$0.00","Many biological systems involve flexible structures immersed in viscoelastic fluids (e.g., sperm in the reproductive tract, bacteria in the gut and lung,ciliary transport of mucus in the lung). In some cases, such as cilia-driven transport in the lung, these structures operate in a multi-fluid environment. Despite decades of work explicit challenges remain with developing suitable computational tools for the modeling of the complex fluid-structure interactions. The goal of this project is to develop and analyze accurate computational methods for these simulations, and to establish high-performance open-source implementations of these tools to be used by other researchers.  The new computational tools together with experimental measurements will be used to generate new insight into the mechanical behavior of mucus. Mucus provides a protective barrier for every human organ, and many diseases and disorders are associated with mucus pathology (e.g., in the lung (COPD, cystic fibrosis, asthma), stomach (ulcers),reproductive tract (infertility)). Moreover, tools developed as part of these projects have applications beyond mucus: to the food industry, for personal care products, as well as pharmaceutical applications, including drugs and drug delivery systems. The project will also provide broad interdisciplinary training for graduate students and postdoctoral researchers in the mathematical sciences.<br/><br/>The specific research objectives of this project are 1) to develop and analyze efficient higher-order accurate numerical methods for fluid-structure interaction and fluid-fluid interaction problems involving complex fluids, 2) to validate these methods by comparison to experimental data of project collaborators, and 3) to develop mathematical models of industrial and biological systems, including microbead rheology, ciliar synchronization and transport, and phase separation of suspensions. Previous research on numerical methods for viscoelastic fluids has been driven by engineering applications, while biological applications pose new challenges: large deformations of active soft structures and complex rheology. Despite past efforts to understand the dynamics of active structures in complex fluids, a significant bottleneck persists: the lack of accurate, efficient, adaptive numerical methods and software for viscoelasstic fluid-structure interaction and fluid-fluid interfaces. The research team aims to build, validate, and apply this technology, guided by applications and experimental data from biology and engineering."
"1720245","Multiscale Methods for Crystalline Nanomaterials","DMS","COMPUTATIONAL MATHEMATICS","07/15/2017","07/13/2017","Xingjie Li","NC","University of North Carolina at Charlotte","Standard Grant","Yuliya Gorb","06/30/2021","$132,278.00","","xli47@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","MPS","1271","9263","$0.00","Crystal defects such as grain boundaries, cracks, or dislocations play significant roles in determining material properties and behaviors. To efficiently and reliably predict failure phenomena, fundamental level of descriptions at nanoscales are widely used to study how defects affect macroscopic properties such as elasticity and plasticity. Challenged by the computational limitations as well as high-fidelity requirements, this research project aims to develop and employ multiscale methods which can retain accuracy around defect cores while improving efficiency through local continuum descriptions. Two popular multiscale modeling strategies are: (1) bottom-up: coarse-graining of microscopic descriptions (e.g., atomistic models) of material behavior; (2) top-down: informing macroscopic models (e.g., continuum equations) with physics gleaned from the microscopic scales. The former provides ""a closer"" comparison with macroscopic experiments and the latter predicts the materials microscopic properties. This project focuses on several aspects of the multiscale modeling and mathematical analysis for both bottom-up and top-down approaches, aims at developing efficient and reliable coupling methods which smoothly integrate the microscopic and macroscopic descriptions, and establishing rigorous error estimates that will set precise guidelines for practical implementation (e.g., the optimal parameters used in the coupling mechanism and the finite element mesh used in the macroscopic scale). This project will also provide an opportunity to train both graduate and undergraduate students in the context of challenging and interdisciplinary research among applied mathematics, materials science and mechanical engineering.<br/><br/>The overarching goal of this project is to develop multiscale methods for both bottom-up and top-down approaches with provable performance in terms of accuracy, efficiency, and reliability based on theoretical analysis and comprehensive error estimates. In particular, these are the four objectives: (1) The research will promote the understanding of bottom-up atomistic-to-continuum (AtC) methods for complex structured crystals. Successful outcomes of this project will greatly extend the applicability of existing AtC methods from simple lattice crystals to more complex ones. The comprehensive error analysis will lead to optimal coupling strategy and provide precise guidelines for practical implementations. (2) The research will also advance the developments of top-down nonlocal-to-local (NtL) continuum couplings. The new coupling framework is built based on geometric reconstruction, and will remove interfacial inconsistency while maintain all physical properties globally, such as conservation of energy and balance of linear momentum, in multi-dimensions, whereas none of existing coupling methods for NtL problems satisfies all of these properties. Furthermore, mathematical analysis will be built to ensure the well-posedness and reliability of modeling and computations. (3) This project will enhance the understandings and interconnections between the top-down and bottom-up approaches, and will also facilitate the exploitation of other scale-bridging algorithms in materials science. (4) High order and robust numerical schemes for large-scale problems will be developed to enhance the controls and designs of defects inside complex structured crystals, and improve the prediction of materials failures."
"1719637","Collaborative Research: Overcoming Order Reduction and Stability Restrictions in High-Order Time-Stepping","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/20/2017","Rodolfo Rosales","MA","Massachusetts Institute of Technology","Standard Grant","Leland Jameson","07/31/2021","$123,296.00","","rrr@math.mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1271","9263","$0.00","This project develops new computational approaches that remedy fundamental accuracy shortcomings of existing time-stepping methods, and increase their stability and robustness. A wide variety of practical applications, including fluid flows, quantum physics, heat and neutron transport, materials science, and many complex multi-physics problems, require the numerical simulation of models that involve a time evolution. This time evolution must be performed in a way that the high accuracy of modern computational methods is retained. This project addresses fundamental challenges that arise in this context, and delivers superior numerical methods that could replace existing time-stepping schemes currently used in computational science and engineering practice. This project provides a multi-institution collaboration, including two early-career researchers, and it involves the training of a PhD student.<br/><br/>The research in this project addresses two aspects in high-order time-stepping: order reduction in Runge-Kutta methods; and unconditionally stable ImEx linear multistep methods. A specific focus lies on time-stepping for partial differential equations. For those, order reduction can be associated with numerical boundary layers, caused by multi-stage time-stepping schemes. Based on this geometric understanding of the phenomenon, remedies for order reduction are developed. This includes the concept of weak stage order, as well as modified boundary conditions. An alternative avenue to avoid order reduction is provided by multistep methods. The key challenge here is their rather restrictive stability behavior. Based on a new stability theory for ImEx multistep methods, this project develops novel schemes that can, for certain problems, achieve unconditional stability. The new schemes can be included into many existing computational codes via a simple modification of the time-stepping coefficients, thus enabling practitioners to select the time step based solely on accuracy considerations."
"1664418","FRG: cQIS: Collaborative Research: Mathematical Foundations of Topological Quantum Computation and Its Applications","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS, CONDENSED MATTER & MAT THEORY","12/01/2017","09/21/2023","Siu-Hung Ng","LA","Louisiana State University","Standard Grant","Andrew Pollington","08/31/2024","$318,292.00","","rng@math.lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1264, 1271, 1765","057Z, 1616, 7203, 7928, 9263","$0.00","A second quantum revolution in and around the construction of a useful quantum computer has been advancing dramatically in the last few years. Topological phases of matter, the importance of which has been recognized by scientific awards that include the 2016 Nobel prize in physics, exhibit many-body quantum entanglement. This makes such materials prime candidates for use in a quantum computer.  Topological quantum computation is maturing at the forefront of the second quantum revolution as a primary application of topological phases of matter. The theoretical foundation for the second quantum revolution remains under development, but it appears clear that algebras and their representations will play a role analogous to that played by group theory in the first quantum revolution. This focused research group aims to formulate the theoretical foundations of topological quantum computation, leading to an eventual theoretical foundation for the second quantum revolution. It is anticipated that the results of the research will guide and accelerate the construction of a topological quantum computer. A working topological quantum computer will fundamentally transform the landscape of information science and technology. The project includes participation by graduate students and postdoctoral associates in the interdisciplinary research.<br/><br/>The goal of topological quantum computation is the construction of a useful quantum computer based on braiding anyons. The hardware of an anyonic quantum computer will be a topological phase of matter that harbors non-abelian anyons. A physical system is in a topological phase if at low energies some physical quantities are topologically invariant. Topological properties are non-local, yet can manifest themselves through local geometric properties. The success of topological quantum computation hinges on controlling topological phases and understanding their computational power. This research addresses the mathematical, physical, and computational aspects of topological quantum computation. The projects include classification of super-modular categories, vector-valued modular forms for modular categories, extension of modular categories to three dimensions, simulation of conformal field theories, topological quantum computation with gapped boundaries and symmetry defects, and universality of topological computing models. The research has potential impacts ranging from new understanding of vertex operator algebras to the development of useful quantum computers. One specific goal is a structure theory of modular categories analogous to that of finite groups. Such a theory would lead to a structure theory of two-dimensional topological phases of matter."
"1720349","Simulation of Multiphase Flow and Transport in the Partially Molten Mantle","DMS","COMPUTATIONAL MATHEMATICS","08/15/2017","08/29/2017","Todd Arbogast","TX","University of Texas at Austin","Standard Grant","Leland Jameson","07/31/2021","$250,000.00","Marc Hesse","arbogast@ices.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","9263","$0.00","Volcanic eruptions affect, for example, local water supplies and the global climate.  Volcanism at the Earth's surface is due to partial melting of the convecting mantle, and subsequent segregation of molten rock components.  The Earth's mantle is generally below its melting point and thus solid.  Partial melting occurs generally in three locations, at mid-ocean ridges where oceanic crust is formed, hot-spots below ocean islands and some continental sites like Yellowstone, and subduction zones where oceanic crust returns to the mantle and continental crust is formed. To better understand volcanic processes, geochemical surface observations must be linked to the composition and distribution of mantle rocks, which are heterogeneous due to incomplete chemical mixing of material during mantle convection.  The project will develop numerical models that can resolve the small-scale dynamics of these processes.  It is a joint effort of a mathematician and a geoscientist, as well as an undergraduate and a graduate student, who will be educated in an interdisciplinary setting.  People so trained are in high demand by industrial and government labs.<br/><br/>Project objectives include the development of (1) a mathematical framework for computational simulation of evolving mantle flow which covers the degenerate case of no melt, (2) a numerical method to accurately approximate the transport of temperature and chemical components within the mantle flow, (3) a computer code to implement the flow and transport algorithms, as well as a method for handling simple phase behavior.  Furthermore, (4) the code will be applied to study important problems in the geosciences and (5) students will be educated and trained in an interdisciplinary setting.  Mathematically speaking, because there are regions with no melt, two-phase flow in the mantle is governed by highly degenerate equations.  Recent work has established the mathematical foundations of two-phase flow when the porosity does not evolve.  The key is to scale the solution variables and equations appropriately by the porosity, which is the volume fraction of melt.  The evolving case will be treated in this project, with the goal of providing an appropriate computational method for simulating the flow in practical simulations.  To model the transport of temperature and the segregation of melt components, appropriate WENO and discontinuous Galerkin methods will be developed for this project, which will respect the possible degeneracies in the porosity."
"1720002","Multiscale Modeling and Computation of Nano-Optics","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","08/03/2017","Di Liu","MI","Michigan State University","Standard Grant","Leland Jameson","07/31/2020","$147,994.00","","richardl@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","7237, 8037, 8990, 9263","$0.00","The main objective of the proposed research is to study robust, efficient and accurate numerical methods for multiscale and multiphysical models of nanoscale optical devices. Nanoscale experimental and manufacturing technologies have proved huge successes and potentials in biomedical engineering and new materials. Quantitative modeling and simulation will not only help to understand the mechanism of these nano devices but also greatly reduce the cost by minimizing trial errors and optimizing the performance. The PI will collaborate with chemists and material scientists on photon driven nano devices and attosciences. Methods developed  for nano-optics will shed light on a wide variety of nanoscale problems. The mathematical analysis of the self-consistent multiscale methods will bring new insights into the field of multiscale modeling and computation. Junior collaborators and graduate students will receive cross disciplinary training on new frontiers of applied mathematics, and will be better prepared to solve realistic problems with a combination of modeling, analytical and computational skills.<br/><br/>To fully understand photon driven nano and micro devices, the PI will derive multiscale models for interactions between the electromagnetic field, electronic excitations and molecular motions by combining classical electrodynamics, time dependent current density functional theory and Ehrenfest molecular dynamics in the linear response regime. The self-consistent multiscale scheme will be adopted to solve the system efficiently. Metal enhancement will also be investigated. To accurately simulate long time propagations of strong field processes involving nonlinear higher order interactions, the PI will adopt adaptive numerical strategies. Multiscale schemes will be designed for the concurrent simulation of electrodynamics and quantum mechanical processes on the time domain. The PI will also conduct a thorough analysis of the algorithms in terms of convergence and stability."
"1720023","OP: Collaborative Research: Compatible Discretizations for Maxwell Models in Nonlinear Optics","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/02/2019","Yingda Cheng","MI","Michigan State University","Continuing Grant","Leland Jameson","07/31/2020","$100,000.00","","ycheng@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","8990, 9263","$0.00","Nonlinear optics is the study of the behavior of light in nonlinear media. This field has developed into a significant branch of physics since the introduction of intense lasers with high peak powers. Compared with the huge amount of literature on simulations of Maxwell's equations in linear optical media, developing mathematically well-understood computational tools for space-time models in nonlinear optical media is relatively less tackled by the computational math community. Major advancement in this aspect can provide the scientific community reliable and accurate tools to simulate and to understand nonlinear optical phenomena, which hence can be better harnessed for practical applications. <br/><br/>The objective of the collaborative research program is to make significant advances in the understanding and simulations of Maxwell models in nonlinear optics with the aim of: (1) providing robust simulation tools for the nonlinear optics community, (2) developing novel mathematical and numerical techniques that are specifically tailored for different types of nonlinear models. The specific technical aspect includes the development of energy-stable time discretizations as well as two classes of spatial discretizations, discontinuous Galerkin methods and mimetic finite difference methods, for the propagation of electromagnetic waves in nonlinear (dispersive) optical media. Both macroscopic phenomenological and microscopic quantum descriptions will be considered for modeling the nonlinear material responses. Applications involving femtosecond soliton propagation, harmonic generation, self focusing, among others will be simulated and compared to existing time domain methods. This collaborative program is strengthened by a cohesive research plan that relies on the complementary expertise of each principal investigator. The educational components are integrated through the training of graduate students."
"1723175","Computational Methods for Hierarchical Manifold Learning","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, CDS&E-MSS","08/01/2017","06/13/2017","Timothy Sauer","VA","George Mason University","Standard Grant","Christopher Stark","07/31/2021","$329,954.00","","tsauer@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1266, 1271, 8069","8083, 9263","$0.00","The enormous practical success of deep learning warrants a clear and concise mathematical explanation. Although the components of a neural network, and the rules for propagation of information through the layers, are extremely simple, there is to date a lack of a corresponding deep understanding of the roles of the various mechanisms involved. Second, there is a lack of transparency: While a given set of network weights may fit scientific or engineering data in-sample and even generalize well out-of-sample, using a neural net to explain the data or the system producing the data is usually very difficult or impossible.  In this project, the PI will leverage recent progress in mathematical methods from applied and computational harmonic analysis to develop a hierarchical algorithm, based on manifold learning, to replicate the strikingly successful properties of deep learning while adding improved accuracy, adaptability to data, smoothness priors, and transparency.<br/><br/>A sequence of computational projects is planned to develop a hierarchical algorithm for deep learning, based on representing manifolds by the Laplace-Beltrami operator. Proposed work supports the construction of a complete algorithm that uses layers of manifold learning kernel methods to represent data in a deep manifold learning infrastructure.  The development of the learning algorithm consists of three parts: (1) the construction of a hierarchical manifold learning architecture, using eigenfunctions of the Laplace-Beltrami operator to represent data, and replicating the sharing and pooling features of neural networks, (2) building innovative algorithms for the purpose of optimizing the solution to identification problems, and (3) development of resampling methods to handle large data sets."
"1720425","Topics of Immersed Finite Element Methods","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","07/17/2017","Xu Zhang","MS","Mississippi State University","Standard Grant","Leland Jameson","12/31/2019","$125,042.00","","xzhang@okstate.edu","245 BARR AVE","MISSISSIPPI STATE","MS","39762","6623257404","MPS","1271","9150, 9263","$0.00","Interface problems are ubiquitous. When simulations involve multiple materials or multi-physics, interface problems arise. Many real-world problems in fluid mechanics, material science, mechanical engineering, and biomedical engineering are modeled by three-dimensional interface problems. The immersed finite element methods (IFEM) are a class of numerical methods for solving interface problems on interface-unfitted meshes. Two interrelated problems will be investigated in this research project. The first problem aims to design a self-adaptive IFEM based on the a posteriori error estimation. The second problem focuses on development, implementation, and analysis of three-dimensional IFEM.<br/><br/>The first problem concerns the study of both residual-based and recovery-based error estimation for various immersed finite element discretizations. These include the immersed finite element approximation in conforming, nonconforming and discontinuous Galerkin frameworks. Rigorous mathematical analysis will be carried out for the reliability and efficiency error estimates of IFEM. The second problem focuses on interface problems of three spatial dimensions. It aims to develop an innovative approach to efficiently construct the three-dimensional immersed finite element functions. These immersed finite element functions will be implemented in various numerical schemes for three-dimensional interface problems. Theoretically, both a priori and a posteriori error estimates will be conducted for new IFEM schemes. Computationally, a three-dimensional IFEM software package will be developed with the feature of adaptive mesh refinement."
"1719849","Regularized Adaptive Methods for Classes of Nonlinear Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/20/2018","Sara Pollock","OH","Wright State University","Continuing Grant","Leland Jameson","09/30/2018","$75,798.00","","s.pollock@ufl.edu","3640 COLONEL GLENN HWY","DAYTON","OH","454350001","9377752425","MPS","1271","8012, 9263","$0.00","Nonlinear diffusion equations appear often in simulations of physical processes such as heat conduction, groundwater flow, and flow in porous media. Such equations appear in environmentally relevant modeling problems describing the distribution of subsurface contaminants. In these nonlinear problems, the diffusion coefficient is dependent on the solution and possibly its coordinates; as such, a numerical solution to the model cannot be determined by direct means. Generally, solutions can only be found by solving a sequence of simpler approximate problems, and making successive improvements to the solution. Convergence of the scheme may fail however if this is carried out in a standard way, and current numerical simulations can be limited by the failure of these standard iterative techniques. The focus of this work is on the mathematically rigorous development of stable and convergent iterative numerical algorithms to efficiently solve nonlinear diffusion equations, addressing a substantial problem in scientific computing for realistic physical modeling.<br/><br/>The technical goal of this project is to develop efficient and robust simulation technology for classes of nonlinear diffusion equations. Finite element solutions for nonlinear diffusion problems are known to have good approximation properties in the asymptotic regime, but a sound methodology to compute those discrete solutions has yet to be developed.  Regularized adaptive methods will be developed within the framework of adaptive finite element methods, for which (1) the iterates converge to discrete solutions; and (2) the discrete solutions converge to the solution of the partial differential equation, as the mesh is selectively refined.  One of the aims of this work is to develop guiding principles for the regularization of the induced discrete problems in concert with error indicators to determine the mesh refinement.  The combination of the regularization and error indicators should both allow the computation of a discrete solution, and guarantee the convergence to a correct solution from a theoretical standpoint.  Computational methods backed up by sound mathematical theory will be developed for representative classes of model problems, and the developed methods will be extended to larger, computationally-demanding simulations, for example to model the distribution of C02 injected into the earth's subsurface as a potential means for long-term storage. It is expected that the technical advances made in the course of this project will advance the realization of  efficient and accurate numerical simulation tools that allow the practical modeling of problems with realistic physical attributes."
"1719698","Rigorous Development of an Efficient Reduced Collocation Approach for High-Dimensional Parametric Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/15/2021","Yanlai Chen","MA","University of Massachusetts, Dartmouth","Standard Grant","Yuliya Gorb","07/31/2022","$179,414.00","Sigal Gottlieb","yanlai.chen@umassd.edu","285 OLD WESTPORT RD","NORTH DARTMOUTH","MA","027472356","5089998953","MPS","1271","102Z, 9263","$0.00","Many physical phenomena  depend on a range of parameters, and to understand the phenomena many repeated simulations for different parameter values are required. Such parameters may describe properties of a material, wave frequencies, uncertainties in measured data, physical states at the boundaries, or domain geometry, among others. These massively repeated simulations require significant computer time, and are frequently computationally prohibitive.  Reduced basis methods were developed to resolve this issue by providing efficient and accurate surrogate solutions for the large space of parameter values, based on a relatively small number of carefully selected parameters and their related pre-computed highly accurate ""snapshot"" solutions. Once these ""snapshot"" solutions are pre-computed, computing the surrogate solutions for any parameter values is quick and efficient. Moreover, their accuracy is certified by a mathematically rigorous error bound. A variant of the reduced basis method, called the reduced collocation method, was recently introduced by the PIs that is more efficient for nonlinear problems. The goals of this project are to develop this new method to be more efficient, both in precise identification of the ""snapshot"" solutions and in computing the surrogate solutions once the ""snapshot"" solutions are found, to integrate reduced basis and reduced collocation methods with uncertainly quantification techniques to efficiently handle complex problems, and to ensure that certain strong stability properties satisfied by the underlying snapshot solutions are preserved by the reduced collocation methods. These goals will enable an efficient and powerful reduced collocation method that can be applied to a wide range of parameter-dependent phenomena. <br/><br/>Reduced basis methods were originally developed for use with a Galerkin formulation of a partial differential equations, and recently extended by the PIs for collocation formulations, which are frequently preferred for nonlinear problems. In fact, this new reduced collocation method (RCM) is more efficient than the typical reduced basis method (RBM) for a large class of partial differential equations (PDEs). The goal of this project is to rigorously develop the new RCM in order to: (1) improve the offline stage by introducing novel approaches to building a more optimal reduced space faster; (2) make the online stage of the RCM more robust and efficient for nonlinear problems through a variety of mathematical approaches to selecting the collocation points, developing pre-conditioners, and creating a co-prime multi-grid approach; (3) integrate the RBM/RCM with uncertainty quantification approaches to efficiently handle problems with high dimensional random spaces; and (4) enhance the RBM/RCM approaches to guarantee that the surrogate solutions preserve the strong stability properties satisfied by the underlying snapshot solutions.  Rigorous mathematical analysis and innovative and efficient algorithm design will be combined to improve the reduced basis approaches thereby making them more efficient and robust for large classes of problems. This project will transform the RCM to make it efficient and robust, for linear as well as nonlinear problems. Furthermore, this project will combine knowledge from the field of reduced order modeling for linear and nonlinear PDEs and the field of uncertainty quantification to create powerful new methods, hybrids of the generalized polynomial chaos method and reduced basis methods. The resulting algorithms will impact the field of uncertainty quantification by improving the efficiency and robust handling of high parameter dimensions. Finally, the RBM/RCM will be improved to guarantee the preservation of nonlinear properties such as positivity. This novel development will resolve a major concern about the quality of the surrogate solution."
"1719942","OP: Collaborative Research: Compatible Discretizations for Maxwell Models in Nonlinear Optics","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","06/11/2019","Fengyan Li","NY","Rensselaer Polytechnic Institute","Continuing Grant","Yuliya Gorb","07/31/2022","$150,000.00","","lif@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","MPS","1271","8990, 9263","$0.00","Nonlinear optics is the study of the behavior of light in nonlinear media. This field has developed into a significant branch of physics since the introduction of intense lasers with high peak powers. Compared with the huge amount of literature on simulations of Maxwell's equations in linear optical media, developing mathematically well-understood computational tools for space-time models in nonlinear optical media is relatively less tackled by the computational math community. Major advancement in this aspect can provide the scientific community reliable and accurate tools to simulate and to understand nonlinear optical phenomena, which hence can be better harnessed for practical applications.  <br/><br/>The objective of the collaborative research program is to make significant advances in the understanding and simulations of Maxwell models in nonlinear optics with the aim of: (1) providing robust simulation tools for the nonlinear optics community, (2) developing novel mathematical and numerical techniques that are specifically tailored for different types of nonlinear models. The specific technical aspect includes the development of energy-stable time discretizations as well as two classes of spatial discretizations,  discontinuous Galerkin methods and  mimetic finite difference methods, for the  propagation of electromagnetic waves in nonlinear (dispersive) optical media. Both macroscopic phenomenological and microscopic quantum descriptions will be considered for modeling the nonlinear material responses. Applications involving femtosecond soliton propagation, harmonic generation, self focusing, among others will be simulated and compared to existing time domain methods. This collaborative program is strengthened by a cohesive research plan that relies on the complementary expertise of each principal investigator. The educational components are integrated through the training of graduate students."
"1723005","Collaborative Research: Stochastic Approximations for the Solution and Uncertainty Analysis of Data-Intensive Inverse Problems","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2017","06/13/2017","Matthias Chung","VA","Virginia Polytechnic Institute and State University","Standard Grant","Christopher Stark","08/31/2022","$210,000.00","Julianne Chung","matthias.chung@emory.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271, 8069","8083, 9263","$0.00","In scientific fields ranging from geophysics and atmospheric science to medical imaging and network communication, data are being generated at remarkable rates. Such data are typically indirectly related to quantities of interest and the data sets are in many cases dynamically growing. Extracting desired information from these data then requires the solution of very large data-intensive inverse problems, perhaps repeatedly and in real time. The computational challenges of obtaining such a solution are compounded by the demands of validation and uncertainty analysis, which can easily become computationally prohibitive. This project will develop mathematical/statistical methods and computational tools for the solution of data-intensive inverse problems. The core of this approach is a stochastic reformulation of such problems that aims to significantly reduce the computational costs while adapting to modern hardware architectures.<br/><br/>A framework will be developed to address the challenges arising at the interface between big data, inverse problems, data analysis, and uncertainty quantification.  First, randomized methods for the solution of linear and nonlinear inverse problems will be introduced, so that efficient stochastic optimization methods can be used to overcome the hardware limitations of current algorithms and to generate solutions and uncertainty assessments in near-real time. New theory and scalable methods will be developed within the stochastic framework, thereby ensuring solution accuracy, reliability, and robustness. Second, advanced tools will be developed for model validation, error analysis, and uncertainty quantification. By partnering with application scientists (e.g., in atmospheric remote sensing), methods developed in this project will be of immediate practical utility for scientists and engineers."
"1719658","Automated, Secure Homotopy Continuation and Parameter Space Exploration","DMS","COMPUTATIONAL MATHEMATICS","06/15/2017","03/22/2019","Daniel Bates","CO","Colorado State University","Standard Grant","Leland Jameson","05/31/2021","$249,835.00","Daniel Bates","bates@math.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1271","9263","$0.00","Systems of polynomial equations arise naturally in most scientific and engineering fields.  These systems describe the motion of robotic arms, the steady states of biochemical reactions, the location of radio transmitters based on sensor readings, and many other science and engineering scenarios that are rooted in physical laws and geometry.  The solution of polynomial systems was nearly impossible until roughly fifty years ago, and efficient methods are still being developed and analyzed.  The first broad direction of this project seeks to automate the tuning of some of these methods (so that non-experts can use them readily) and to incorporate more certainty in the computations.  The second broad direction seeks to further develop methods that are particularly useful in applications and to make progress on a few particular applications, namely, biochemical reaction networks, geolocation, and population genetics.  Three graduate students will be involved in the project.<br/><br/>Homotopy continuation is a fundamental computational tool of numerical algebraic geometry, on which virtually all advanced algorithms within the field depend.  While there has been much theoretical and algorithmic development on this topic over the years, there are still at least three basic problems to be addressed.  First, ill-conditioning forces the use of expensive high-precision numerical computations and sometimes path failure.  Second, homotopy continuation depends on a number of numerical tolerances that are highly problem-dependent and difficult to adjust, especially for non-experts.  Third, numerical methods are inherently inexact, making them undesirable in some contexts.  The three projects of the first research direction seek to make progress on solving these problems.  Fundamental scientific and engineering questions frequently involve parameters, and their solution depends on the ability to understand and move around parameter spaces effectively.  The three projects of the second direction aim to further develop numerical homotopy methods for parameter space exploration.  The first project will advance our ability to collect useful data from Euclidean parameter spaces, particularly when the goal is to find points in parameter space at which a parameterized polynomial system has a special kind of solution set.  The second project aims to extend homotopy continuation to general algebraic parameter spaces.  The third project is an assortment of applications, as mentioned."
"1720259","Matrix Functions and Network Analysis","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","08/21/2017","Lothar Reichel","OH","Kent State University","Standard Grant","Leland Jameson","08/31/2021","$150,000.00","Omar De La Cruz Cabrera","reichel@math.kent.edu","1500 HORNING RD","KENT","OH","442420001","3306722070","MPS","1271","9263","$0.00","Networks are important subjects of study in several fields, including biology, social sciences, and security. Two important properties of networks that are often overlooked are that they are dynamic (change over time), and have features that are relevant at different scales. Thus, it is important to study networks from a dynamic and multiscale point of view. A natural model is to assume that we have not just one network, but a family of networks, indexed by one parameter (or may be more). That parameter can track changes as time passes, or as one change one's focus to different scales. In this project the PIs will develop analysis tools and techniques that provide useful insights, but are also efficiently computable for large networks, even when dealing with many values for the parameter. They concentrate on several measures of interest in the network analysis literature, like measures of centrality, hubs and authorities, good broadcasters and good receivers. The study of these measures becomes more difficult for larger networks.<br/><br/>Methods for discovering important features of a network are often based on matrix functions. Two of them are the matrix exponential and the resolvent; these can be heuristically justified as measuring connectivity between nodes by weighted sums of paths connecting them, and modifications of these functions can be easily devised to stress some features of the network over others. The PIs have developed methods for approximate computation and error estimation that can be applied to a wide class matrix functions. They will adapt and develop methods of approximate computation, and determine error estimates for these methods, as relevant to the project setting. Some quantifications of network features involve matrix decompositions, like the eigenvalue or singular value decompositions, while others involve only simple matrix summaries, like the diagonal entries, or row and column sums. The PIs will devise approaches for carrying out decompositions that take advantage of the closeness between the matrices when the parameters are close, in order to reduce the computational burden and to better keep track of the parts of the decomposition that rise or fall in importance as the parameter changes."
"1720225","Collaborative Research:   Zero Forcing on Graphs: Computation and Applications","DMS","COMPUTATIONAL MATHEMATICS","07/15/2017","07/19/2017","Illya Hicks","TX","William Marsh Rice University","Standard Grant","Leland Jameson","06/30/2021","$204,429.00","","ivhicks@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","9263","$0.00","The concept of networks is a mathematical term used to analyze relationships between objects (e.g. people, places, and things).  Of particular importance is the study of how influence propagates throughout networks, especially how one can deduce the influence of an entire network by monitoring a few members.  The objective of this project is to establish a comprehensive knowledge base for developing, implementing, and applying computational methods related to this phenomenon.  As a result, this research has strong connections to applications related to social networks, electrical power networks, and quantum systems.  This project also supports a concerted effort to engage underrepresented groups within the research.  Hence, the impact of this educational component includes the development of underrepresented groups within the next generation of STEM researchers.<br/><br/>The main technical contribution of this project is the integration of combinatorial optimization, spectral graph theory, and numerical analysis techniques to examine zero-forcing in networks.  In particular, the research team will incorporate branch decomposition techniques and linear and integer programming techniques to significantly increase the computational efficacy of algorithms to solve zero-forcing problems on graphs. The models and algorithms that result from this study will be validated using experimental data and also by data attainable publicly like electrical grid data.  This research will significantly advance the knowledge base of combinatorial optimization, integer programming, and spectral graph theory while also contributing to the increased scalability and efficiency for solving computationally hard problems related to the aforementioned applications."
"1720398","OP: Collaborative Research: Novel Feature-Based, Randomized Methods for Large-Scale Inversion","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","05/15/2017","Arvind Saibaba","NC","North Carolina State University","Standard Grant","Leland Jameson","08/31/2021","$104,942.00","","asaibab@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1271","8990, 9263","$0.00","The desire to form an image of a region of space from externally collected data arises in applications ranging from detecting and characterizing cancers in the body, to quantifying the distribution of water, oil, or subsurface pollutants, and to the timely accurate identification of explosives in crowded venues. The physics associated with signal propagation and sensing in these problems creates substantial computational challenges for transforming raw data into useful information. The research team in this project aims to develop computational methods that greatly reduce the cost of real time imaging by providing improvements in statistical inverse theory, numerical inversion methods, simulation models, and hybrid imaging models.  The main thrusts of the project will be tested on imaging applications in medical tomography, environmental remediation, and airport security imaging. The techniques form the basis for addressing analogous problems associated with inversion of optical signals across a wide range of spatial and temporal scales. As part of the project, a modular course will be developed to teach these new methods at the graduate level. The course materials will be made available over the internet.<br/><br/>The large-scale imaging, or inverse, problems addressed by this collaborative team require the minimization of a parameter-dependent function that expresses the misfit of predicted measurements for a candidate image and actual measurement data. The potentially large number of parameters must be minimized over an ever-increasing huge number of measurements, while concurrently some unknown set of the data may be redundant.  Detailed images, however, are not always needed for addressing relevant, practical questions and decision making. A combination of computational techniques will be developed to make large-scale parameter-dependent minimization computationally feasible.  Furthermore, novel efficient approaches for inferring critical image features will be developed, obviating need for complete reconstruction of an image. The research builds on recent methods that exploit randomization to compute accurate estimates of solutions at greatly reduced computational cost, and on the efficient construction of smaller, approximate, reduced order numerical models that are accurate for relevant sets of parameters, and thus reduce the cost of full simulation of the sensing physics. Probabilistic approaches for inference of critical image features that guide image interpretation and decision making will be developed. The mathematics associated with this approach requires these methods to capitalize on other new tools also under development in this project."
"1654175","CAREER:  Integrated Approaches for Fast and Accurate Large-Scale Inversion","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","08/12/2021","Julianne Chung","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Yuliya Gorb","01/31/2023","$402,796.00","","jmchung@emory.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271","1045, 9263","$0.00","The ability to compute solutions to inverse problems is essential in various scientific applications (e.g., for cancer diagnosis or for crack detection in underground mines), but computing real-time solutions to large nonlinear problems that incorporate physics- or data-informed constraints is not feasible with current inversion algorithms. Moreover, as numerical solutions to inverse problems are increasingly being used for data analysis and to aid in decision-making, these computational limitations pose significant bottlenecks in algorithms for uncertainty quantification (e.g., for estimating solution variances). The overarching goal of this project is to significantly reduce the costs of numerical inversion and to enable statistical tools to aid scientists in making informed decisions.  These developments will lead to scientific advancement in many important fields. For example, existing collaborations with biomedical and mining engineers will ensure that the proposed research can result in improved medical diagnosis via advanced point-of-care imaging technologies, fewer injuries due to improved ground control monitoring of underground mines, and advanced signal estimation for real-time analysis of physiological systems. Moreover, the PI will continue to actively engage in activities that encourage students from historically under-represented groups.  The PI's focus on upper elementary to high school girls and on outreach that will feed back into the greater research and teaching communities (e.g., K-12 teachers) will contribute to the recruitment, training, and retention of a diverse next generation of computational scientists.<br/><br/>This research will advance knowledge in the field of computational inverse problems by developing faster methodologies and more robust frameworks for the design, computation, and analysis of solutions to inverse problems. An integrated framework will be adopted, where the main research thrusts are (i) to develop novel regularization methods and implementations to handle application-specific constraints, while simultaneously incorporating robust parameter selection methods; (ii) to advance technologies for real-time computation of solutions to large, nonlinear inverse problems (e.g., by integrating stochastic methods and update approaches); and (iii) to enable critical, yet previously unobtainable, quantitative diagnostics for complex, nonlinear systems by developing efficient error estimation methods."
"1717516","Unfitted Finite Element Methods for Partial Differential Equations on Evolving Surfaces and Coupled Surface-Bulk Problems","DMS","COMPUTATIONAL MATHEMATICS","07/15/2017","06/09/2017","Maxim Olshanskiy","TX","University of Houston","Standard Grant","Leland Jameson","06/30/2020","$154,506.00","","molshan@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","9263","$0.00","Many important processes in nature and in engineering applications take place along surfaces. Examples include interaction of lipid and protein molecules in cell membranes or the action of a chemical dispersant on the oil-water interface, while helping to clean-up an oil spill. These and other processes are described mathematically with the help of differential equations posed on surfaces. Numerical methods for solving these equations provide tools for scientists to understand better surface phenomena by doing in silico experiments. The present project aims to develop such accurate and reliable numerical methods for the benefit of academic and engineering community.<br/><br/>This research project develops new higher order unfitted finite element methods for partial differential equations (PDEs) posed on evolving   surfaces. The developed numerical approach uses time-independent background meshes. In a weak formulation of a PDE, the method employs traces of standard finite element spaces on reconstructed evolving physical domains.   The resulting methods will be optimally accurate, handle implicitly defined surfaces, allow a surface to undergo topological changes, and extend to surface-bulk coupled problems. The project goals will be met  by constructing a higher order space-time unfitted finite element method and performing a full convergence analysis, extending the method and analysis to coupled surface-bulk transport-diffusion problems, developing a new hybrid method for PDEs on evolving surfaces that uses finite difference approximations of time derivatives, extending the new approach to fluid equations posed on manifolds."
"1750319","Collaborative Research: New Algorithms for Group Isomorphism","DMS","COMPUTATIONAL MATHEMATICS","06/01/2017","07/13/2018","Joshua Grochow","CO","University of Colorado at Boulder","Standard Grant","Leland Jameson","05/31/2020","$112,927.00","","jgrochow@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1271","9150, 9251, 9263","$0.00","Symmetry reduces large complex systems to manageable quantities of information. Identifying those symmetries and understanding their structure helps to solve a wide range of problems, from improving engineering tasks to disrupting the mechanisms of disease. The century-old problem of deciding whether two sets of symmetries have the same structure is known today as the Group Isomorphism Problem. This problem is fundamental to both computational algebra and computational complexity, and has implications for fields as diverse as material science, particle physics, and chemistry. The primary goal of this project is to develop significantly better approaches to testing isomorphism of finite groups of symmetries. It supports a new multidisciplinary collaboration between researchers at four universities, including students and early-career mathematicians and computer scientists. <br/> <br/>The Group Isomorphism Problem asks for an algorithm to decide whether two finite groups are equivalent. Both the problem itself, and the techniques designed to improve upon it, have implications for other computational problems, including the better-known problems of Graph Isomorphism and P versus NP.  Our team's approach goes beyond existing static recursions such as working sequentially down a derived or lower central series. Using a new dynamic strategy we prioritize the optimal stages of the problem, thereby improving the performance of later stages. To achieve this we are investigating the use of nonassociative rings, spectral sequences, modular representation theory, and p-local cohomology. We are also inspecting recently developed data structures in computational algebra that seem well-suited to our approach, as well as investigating applications to geometric complexity theory."
"1654311","CAREER:  Large-Scale Bayesian Inverse Problems Governed by Differential and Differential-Algebraic Equations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","08/26/2021","Noemi Petra","CA","University of California - Merced","Continuing Grant","Yuliya Gorb","08/31/2024","$400,000.00","","npetra@ucmerced.edu","5200 N LAKE RD","MERCED","CA","953435001","2092012039","MPS","1271","1045, 9263","$0.00","Nontechnical explanation of the project's broader significance and importance: Model-based projections of real life applications will play a central role in prediction and decision-making, in environment and climate change applications, for instance, to anticipate ice sheet contribution to sea level rise, or in the context of energy applications, to predict faults and assess dynamic stability in a power grid.  However, models are typically subject to considerable uncertainties stemming from uncertain inputs to the model (e.g., coefficient fields, constitutive laws, source terms, geometries, and initial and/or boundary conditions) as well as from noisy and limited observations. While many of these input quantities cannot be directly observed or measured, they can be inferred from observations, such as those of ice surface velocities in ice sheets. This typically leads to an extremely challenging mathematical problem. This project aims to enable the propagation of uncertainties from data/observations through inference to prediction and increase predictability of complex physical systems.  The selected driving application (i.e., the ice sheet model) for research and education activities, capture important general and complex algorithmic challenges such as large-scale, nonlinearity, time-dependence, and ill-posedness. The research will be, therefore, applicable to a broader spectrum of problems. The algorithms, mathematical findings and open source codes will be shared through peer reviewed journal papers, and presentations at conferences and workshops.<br/> <br/>Technical description of the project: Bayesian inversion facilitates the integration of data with complex physics-based models to quantify and reduce uncertainties in model predictions. This opens the door to more advanced capabilities for prediction and decision-making under uncertainty.  However, the algorithmic developments for Bayesian inversion are subject to several challenges. For instance, characterizing the posterior distributions of parameters or predictions inevitably requires repeated evaluations of (possibly) large-scale and complex forward models governed by differential equations.  In addition, the posterior distribution has a complex structure stemming from the presence of possibly nonlinear forward models and heterogeneous sources of data. To overcome these computational challenges, it is essential to exploit problem structure (e.g., derivatives and local sensitivity of the data with respect to parameters). The objectives of this proposal is to conduct exploratory work in addressing the mathematical and computational barriers in solving large-scale Bayesian inverse problems governed by differential equations. Developing mathematically rigorous and computationally efficient and robust methods in the context of statistical inference has the potential of transformative research in the field of modern computational inverse problems. In particular, the PI and her student will work on the following vertically-integrated research areas: (i) scalable algorithms for large-scale inverse problems (here the focus will be on second derivative (i.e., Hessian) approximations for inverse problems and on developing efficient preconditioners for inexact Newton-Krylov systems to increase the computational efficiency of inverse solvers), and (ii) uncertainty quantification in high dimensions (here the focus will be on building Hessian- and reduced order model-based methods for efficient posterior exploration in high dimensions). The proposed research requires an interdisciplinary perspective, namely it brings together applied mathematics, scientific computing and statistics."
"1720288","Multi-Scale Computational Methods for Coastal Flooding","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/31/2017","Kyle Mandli","NY","Columbia University","Standard Grant","Leland Jameson","07/31/2021","$150,000.00","","kyle.mandli@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1271","8012, 9263","$0.00","Flooding along the nation's coastlines can lead to devastating consequences both to life and property as seen in Hurricanes Katrina and Sandy.  Efforts to protect our coastlines from the most dangerous of these events is critical to our sustainable future along our nation's coastlines.  The goal of this project is to provide a means for improving our ability to forecast and predict these events while taking into account key coastal infrastructure that are designed to protect at-risk populations.  This work not only enables direct reduction of loss of life and economic impact via better forecasts however, it also enables the application of mathematics and computation that can start to ask and answer critical questions such as ""I want to build a sea-wall, where should I put it and how high should it be?"".  The context of these questions is also critical, budgetary constraints and optimal protection are of course central to this effort and this projects aims to enable answering these questions as well.<br/><br/>The project develops numerical methods that can answer questions related to climate change adaptation strategies addressing coastal flooding due to tsunamis and storm surge.  Both types of events have in the past proven themselves to be immensely destructive and promise to be so in the future. Unfortunately with climate change these events could be made worse, whether due to rising sea-levels or changes to the likelihood of dangerous storms, it is critical to a sustainable existence along the coastlines of the world that we address the threat of coastal flooding.  The work proposed focuses on developing numerical methods that include sub-grid-scale structures that help protect coastal communities with barriers such as sea-walls.  The basis of the work will be the GeoClaw framework, a finite-volume, wave- propagation, and structured- grid code that has previously been validated for both tsunami and storm surge events. GeoClaw addresses some of the multi-scale nature of the coastal flooding problem by using adaptive mesh refinement (AMR) and local time stepping but struggles to resolve fine-scale structures without significant penalty due to the resolutions required.  The work proposed focuses on representing barriers at the sub-grid-scale level without the need for fully resolving these structures. This will be accomplished by the development of a new Riemann solver that will be able to represent zero-width barriers at grid cell edges.  This capability will then be used in conjunction with h-box and semi-Lagrangian methods allowing the barriers to have flexible parameterizations in terms of placement with respect to the grid. The method will then be tested on realistic cases and be incorporated into a larger optimization framework that is geared towards finding optimal adaptation solutions emphasizing the capabilities of the flexible specification of the barrier."
"1720366","Parallel Nonlinear Preconditioning Algorithms and Applications in Biomechanics","DMS","COMPUTATIONAL MATHEMATICS, FD-Fluid Dynamics, MSPA-INTERDISCIPLINARY","06/01/2017","05/17/2017","Xiao-Chuan Cai","CO","University of Colorado at Boulder","Standard Grant","Leland Jameson","12/31/2020","$240,000.00","","cai@cs.colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1271, 1443, 7454","7433, 8007, 9263","$0.00","Computer simulation of fluid-structure interaction problems has many applications in science and engineering, such as the vibration analysis of aircraft, automobiles, and suspension bridges. More recently, the technique has been extended to diagnosing and treatment planning of certain medical problems such as congenital and acquired cardiovascular diseases, and to the design and optimization of medical devices. Solving the fluid-structure interaction problems on supercomputers with a large number of processor cores is challenging because the mathematical model consists of a coupled complicated nonlinear system, and most existing algorithms and software for the solution of problems of this kind do not scale well beyond a few hundred processor cores. The principal investigator will develop highly scalable algorithms and software that are suitable for large scale supercomputers and  applicable for different models of blood flows and material parameters for the arterial wall. <br/> <br/>Mature technologies are available for solving many types of linear problems, but for coupled, highly nonlinear multi-physics problems, robust and scalable techniques are badly needed, especially for implementation on large scale parallel computers. The technical focus of this project is a class of non-linearly preconditioned Newton methods that combines a nonlinear elimination technique with multilevel domain decomposition for parallelization. Through this research the principal investigator will solve non-linear difficult problems modeling a wide range of physical models with different levels of non-linearities. Algorithms that provide a high degree of parallelism will be designed so that large scale parallel computers can be used efficiently.  The target application is a family of fluid-structure interaction problems in biomechanics. For the fluid model, both Newtonian and non-Newtonian models will be studied. For solid models, linear elasticity will be considered for small deformations,  geometric nonlinear elasticity will be considered for large deformations, and hyper-elasticity will be considered for materially nonlinear problems. Two- and many-level versions of the algorithms will be investigated to obtain high scalability on parallel computers with a large number of processor cores.  This research will have a great impact in areas of computational science and engineering where non-linear difficult problems need to be solved. The research is rich in opportunities for both graduate and undergraduate students interested in applications in biomechanics, parallel computing, and general computational science and engineering."
"1664645","FRG: Collaborative Research: Computational Methods for Complex Fluids: Adaptivity, Fluid-Structure Interaction, and Applications in Biology","DMS","COMPUTATIONAL MATHEMATICS, FD-Fluid Dynamics, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY","08/01/2017","07/18/2017","Boyce Griffith","NC","University of North Carolina at Chapel Hill","Standard Grant","Yuliya Gorb","07/31/2022","$600,000.00","M Forest","boyceg@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1271, 1443, 7334, 7454","1616, 8007, 9263","$0.00","Many biological systems involve flexible structures immersed in viscoelastic fluids (e.g., sperm in the reproductive tract, bacteria in the gut and lung, ciliary transport of mucus in the lung). In some cases, such as cilia-driven transport in the lung, these structures operate in a multi-fluid environment. Despite decades of work explicit challenges remain with developing suitable computational tools for the modeling of the complex fluid-structure interactions. The goal of this project is to develop and analyze accurate computational methods for these simulations, and to establish high-performance open-source implementations of these tools to be used by other researchers.  The new computational tools together with experimental measurements will be used to generate new insight into the mechanical behavior of mucus. Mucus provides a protective barrier for every human organ, and many diseases and disorders are associated with mucus pathology (e.g., in the lung (COPD, cystic fibrosis, asthma), stomach (ulcers), reproductive tract (infertility)). Moreover, tools developed as part of these projects have applications beyond mucus: to the food industry, for personal care products, as well as pharmaceutical applications, including drugs and drug delivery systems. The project will also provide broad interdisciplinary training for graduate students and postdoctoral researchers in the mathematical sciences.<br/><br/>The specific research objectives of this project are 1) to develop and analyze efficient higher-order accurate numerical methods for fluid-structure interaction and fluid-fluid interaction problems involving complex fluids, 2) to validate these methods by comparison to experimental data of project collaborators, and 3) to develop mathematical models of industrial and biological systems, including microbead rheology, ciliar synchronization and transport, and phase separation of suspensions. Previous research on numerical methods for viscoelastic fluids has been driven by engineering applications, while biological applications pose new challenges: large deformations of active soft structures and complex rheology. Despite past efforts to understand the dynamics of active structures in complex fluids, a significant bottleneck persists: the lack of accurate, efficient, adaptive numerical methods and software for viscoelasstic fluid-structure interaction and fluid-fluid interfaces. The research team aims to build, validate, and apply this technology, guided by applications and experimental data from biology and engineering."
"1719818","Bernstein-Bezier Techniques for High Order Time-Domain Discontinuous Galerkin Methods","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/24/2017","Jesse Chan","TX","William Marsh Rice University","Standard Grant","Yuliya Gorb","07/31/2022","$200,000.00","","Jesse.Chan@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","9263","$0.00","Simulations of wave propagation are the backbone of numerous applications in a diverse set of areas, such as medical imaging, predictive seismology, and engineering design.  Many applications require repeated high resolution simulations of waves in domains with complex boundaries or features.  For example, brain imaging methods require dealing with wave propagation through changing media within the human body, such as transitions between different types of tissue or transitions from tissue to bone.  The target application areas require numerical simulations which are reliable, accurate, and efficient.  However, incorporating accurate approximations of complex media and geometries typically sacrifices one or more of these properties, resulting in simulations which are accurate but slow, fast but inaccurate, or unstable outside of specific settings.  The proposed project aims to develop reliable, provably stable methods for wave propagation in complex media and geometries which retain accuracy and efficiency.  <br/><br/>The project will study numerical wave propagation using high order discontinuous Galerkin (DG) methods, and consists of two main tasks.  The first task will leverage high performance architectures such as Graphics Processing Units (GPUs), as well as the recently developed weight-adjusted and Bernstein-Bezier DG methods.  Weight-adjusted DG methods achieve provable stability and high order accuracy in the presence of variable media, but have a high computational complexity with respect to the order of approximation.  Bernstein-Bezier DG methods achieve an optimal computational complexity with respect to the order, but require low-resolution models of media and geometry in order to do so.  The first task will combine these two approaches to construct methods for wave propagation in varying media which are stable, high order accurate, and have low computational complexity.  The second task will be to improve the reliability of modeling complex boundaries using curvilinear unstructured meshes.  Typical methods for producing curvilinear meshes can result in elements unsuitable for numerical simulation. The second task will seek robust methods for constructing high order approximations of geometry by leveraging Bernstein-Bezier representations of polynomials, which are closely related to shape properties of the underlying function."
"1712970","Gene Golub SIAM Summer School: Data Sparse Approximations and Algorithms","DMS","COMPUTATIONAL MATHEMATICS","01/01/2017","11/22/2016","James Nagy","GA","Emory University","Standard Grant","Leland Jameson","12/31/2017","$10,000.00","","jnagy@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271","7556, 9263","$0.00","This project supports the participation of U.S. based PhD students to participate in the 2017 Gene Golub Society for Industrial and Applied Mathematics (SIAM) Summer School on Data Sparse Approximations and Algorithms, which will be held at Akademie Berlin-Schmockwitz in Germany, May 29 through June 9, 2017.  Detailed information can be found at http://www3.math.tu-berlin.de/numerik/G2S3/index.html. The topic of this summer school is motivated by the observation that in numerous modern applications throughout business, science and engineering, it is extremely challenging to efficiently and stably acquire, analyze, and process massive amounts of data. Recent mathematical advances have shown that massive data sets, and functions associated with them, can often be represented or accurately approximated by only a small number of relevant features; that is, massive data can be represented by a sparse set of features. The summer school will expose PhD students to recent mathematical and computational techniques used in the area of data sparse approximations, and this project ensures participation of well qualified students from U.S. based institutions.<br/><br/>Techniques from several different mathematical fields have been used and continue to be developed in the context of data sparse representations and approximations. Among them are applied harmonic analysis, approximation theory, convex analysis, frame theory, graph theory, imaging science, inverse problems, probability theory, random matrix theory, reduced order modeling, and tensor analysis. In all applications, the outcome of the modeling, simulation, optimization, or approximation is a linear algebraic problem that encodes the underlying functions, the data, and thus also the resulting sparsity. Together with appropriately chosen regularizations and metrics or norms, a key role in the process is played by the fields of numerical linear algebra and optimization. A solid knowledge of these fields is required for working with, and making further advances in the field of data sparse approximations and algorithms.  The school will consist of four courses over the two-week period of the summer school: Two courses in the first week of the school will focus on the theory of sparse representation and approximation as well as tensor methods, and two courses in the second week will deal with sparse numerical linear algebra as well as optimization methods in the sparse context.  The courses will include lectures as well as computational exercises and small group projects for the participants."
"1652330","CAREER: Turbo-Charging Hybrid Functional Electronic Structure Calculations via Adaptive Compression Methods","DMS","CAREER: FACULTY EARLY CAR DEV, COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","05/01/2017","06/10/2021","Lin Lin","CA","University of California-Berkeley","Continuing Grant","Yuliya Gorb","04/30/2023","$400,000.00","","linlin@math.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1045, 1271, 8048","1045, 9263","$0.00","This project aims to improve numerical methods for the simulation of electronic structure in atoms and molecules. Electronic structure theories, particularly represented by Kohn-Sham density functional theory (KS-DFT), have been developed into workhorse tools with a wide range of applications in physics, chemistry, materials science, and biology.  High fidelity electronic structure simulation requires the solution of increasingly larger and more complex problems.  Efficient, accurate, and scalable numerical methods can transform the range of applicability of electronic structure theories and enable scientific applications that are beyond reach today.  The education component of this project focuses on introducing electronic structure theories to graduate students, early career researchers, and advanced undergraduate students with mathematical backgrounds. Efficient techniques developed in the applied mathematics community, after adapted to specific computational problems, can be valuable tools to reduce the cost of electronic structure calculations. Electronic structure theories can also offer new ideas and application areas for the development of general mathematical and numerical techniques. Currently there is a high language barrier between the applied mathematics community and the electronic structure community. This project will develop new education activities, such as new topic courses and summer school activities, with the goal of reducing the language barrier and bridging the gap between the two communities.  <br/><br/>The research component of this project aims at significantly reducing the computational cost for KS-DFT calculations with high fidelity hybrid exchange-correlation functionals. Mathematically, hybrid functionals involve the Fock exchange operator, which is generally nonlocal and full-rank, and its treatment is computationally very challenging.  This project will develop an adaptive compression strategy for treating the Fock exchange operator, and accelerate two important types of calculations with hybrid functionals: ab initio molecular dynamics, and real-time time dependent density functional theory.  The new methods will be transferred to community electronic structure software packages used by chemists and materials scientists, and be validated by simulation with real materials. The goal is to transform how hybrid functional electronic structure calculations are performed for large, complex systems."
"1719578","Generalized Matrix Functions: Theory, Algorithms, and Applications","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","05/10/2017","Michele Benzi","GA","Emory University","Standard Grant","Leland Jameson","07/31/2020","$250,016.00","","mbenzi@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271","9263","$0.00","In recent years the scientific enterprise, like daily existence, has been greatly affected by the availability of new technologies that have enabled the collection of an unprecedented amount of data. This continuous creation of enormous amounts of raw digital information demands new, efficient ways to extract useful content and filter out the noise inherent in any data-gathering process. Mathematical and computational techniques of data analysis offer powerful tools that can be brought to bear, but new challenges arise on a daily basis. This requires the constant refinement and improvement of existing techniques, as well as the development of new ones. This research project aims to produce a new body of mathematical knowledge and new computational techniques that will enhance the ability to tackle challenges arising from data-intensive fields of science and engineering including computer vision, compressed sensing, control, and others. Quantitative finance, network analysis and synthesis, and medical imaging are other areas where the research can be expected to have an impact. The principal investigator will study a class of mathematical objects known as generalized matrix functions and aims to exploit the resulting knowledge to develop new, efficient computer methods for data analysis. Training of a PhD student in computational mathematics is also an integral part of the project.<br/><br/>The principal investigator aims to develop the theory of generalized matrix functions, a type of matrix function based on the singular value decomposition of a (possibly rectangular) matrix. The resulting theory is intended to be the basis for the development of numerical methods for the efficient approximate evaluation of such matrix functions. The focus will be primarily on large-scale problems for which the (full) singular value decomposition cannot be computed. Techniques based on sparsity and low-rank approximations will be combined with Krylov-type methods (especially the Lanczos and Golub--Kahan algorithms) to design fast algorithms for solving a variety of problems involving generalized matrix functions. The algorithms will be applied to problems such as low-rank matrix optimization, the regularization of discrete inverse problems, and the analysis of directed networks. As a by-product of this research, new algorithms for the computation of standard matrix functions where the matrix argument is only available in factored form will be derived and analyzed. This research represents a new direction in numerical linear algebra and is expected to produce useful numerical tools for the solution of a variety of problems in data science and optimization."
"1719907","Simulation of the Wave-Matter Interactions at Extreme Scales","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/28/2017","Songting Luo","IA","Iowa State University","Standard Grant","Leland Jameson","07/31/2020","$124,934.00","","luos@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1271","7237, 8037, 8990, 9263","$0.00","This project is devoted to providing reliable models and the state-of-the-art numerical methods for studying the wave-matter interactions at extreme scales as in geometrical optics, kinetic models, and nano optics, with practical applications arising from astrophysics, geosciences, plasma physics, fusion energy sciences, biology, semiconductor technology, material sciences, nano-technology and nano-sciences. For example, the geophysical exploration processes, the wave-plasma interactions in fusion energy sciences, and the fabrication of nano-motors for solar energy conversion. The proposed models and numerical methods are based on knowledge of geometrical optics, physical optics, kinetic theories, nano optics, computational chemistry and scientific computing. Interdisciplinary collaborations will be pursued to extend the practical applications of the proposed methods. The project also involves the integration of research and education in computational mathematics. Graduate and undergraduate students, and members from underrepresented groups will be encouraged to participate in the project to enhance their knowledge and research. The proposed project will further the training and education of students and encourage them to pursue future career in science, technology, engineering and mathematics (STEM).<br/><br/>In the project, reliable models and efficient numerical methods will be proposed to simulate the wave-matter interactions at extreme scales: (I) If the size of the matter is much greater than the wavelength, the interactions are equivalent to high frequency wave propagation in inhomogeneous media. The PI will develop asymptotic methods that combine integral representations of the waves and asymptotic high frequency theories (notably geometrical optics). (II) If the size of the matter decreases to meso or micro scale, kinetic models are popularly applied for modeling the interactions.  The PI will develop asymptotic methods that utilize the Hopf-Cole transformation of the 6-D probabilistic distribution function. (III) If the size of the matter keeps decreasing to nano or atomic scale, the motion of the matter must be determined quantum mechanically while the wave propagation is determined. The PI will use semi-classical theories as the building block to develop numerically trackable semi-classical models and efficient multi-scale methods. The core ideas consist of the following: For (I), integral representations with Green's functions will serve as the mechanism for wave propagation, geometrical optics approximations will provide the information of Green's functions, and fast multi-level algorithms will be designed to evaluate the oscillatory integrals efficiently with optimal complexities. For (II), the phase function of the Hopf-Cole transformation of the probabilistic distribution function will be approximated as a power series expansion with the expansion terms determined through solutions of equations formulated in 3-D spatial space, and high-order schemes can be designed to solve such equations efficiently. For (III), the wave propagation will be determined classically through Maxwell's equations, Ehrenfest molecular dynamics and time-dependent current density functional theory will be applied to resolve the difficulty of dealing with many-body Schrodinger equations for the matter, and the models will be solvable by well-designed multi-scale solvers."
"1719538","Sums of Squares Polynomials in Optimization, Combinatorics, and Computer Vision","DMS","COMPUTATIONAL MATHEMATICS, Combinatorics","07/01/2017","06/02/2017","Rekha Thomas","WA","University of Washington","Standard Grant","Leland Jameson","06/30/2021","$250,000.00","","thomas@math.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1271, 7970","9263","$0.00","Many questions in science and engineering can be modeled as questions in polynomial optimization, in which the goal is to find an optimal solution given a set of practical constraints on the model parameters. This field has undergone a revolution in the last two decades by incorporating novel ideas originating from several fields of mathematics and computer science. These advances have made polynomial optimization a viable tool in many applications. An important example is computer vision, for which a key goal is to estimate and characterize the three-dimensional shapes within a scene, given a set of two-dimensional images. This research project tackles such challenges by combining techniques from optimization, computer vision, and combinatorics. Special emphasis is placed on designing efficient and practical computational algorithms. Graduate students will be involved in this cross-disciplinary research project, providing the students with broad training in mathematics that intertwines theory and computation.<br/><br/>This research investigates three topics in the field of polynomial optimization. The first project extends prior work on positive semi-definite representations of polytopes for the creation of a canonical model of realization spaces of polytopes. This research has the potential to initiate a new algebraic viewpoint of polytopes via their slack ideals, simplifying geometric results and settling longstanding questions such as understanding projective uniqueness. For the second project the focus is on algebraic vision, for which the aim is to apply techniques from algebraic geometry and polynomial optimization to questions in computer vision, with potentially important practical application. Concurrently, the study of applied formulations has the potential to inspire new mathematical theories. Research in algebraic vision continues to create strong ties to the computer vision community and will help create an intellectual exchange between mathematics and vision, benefiting both fields and providing a stimulating training environment for mathematics students looking toward careers in industry. The long-term aim of the last project is to develop the computational aspects of polynomial optimization in the presence of symmetry.  Numerous problems in applications come with symmetries, and the ability to exploit this feature often determines whether or not the problem can be solved."
"1719841","Collaborative Research:  Zero Forcing on Graphs: Computation and Applications","DMS","COMPUTATIONAL MATHEMATICS, GOALI-Grnt Opp Acad Lia wIndus, WORKFORCE IN THE MATHEMAT SCI","07/15/2017","07/30/2019","Michael Young","IA","Iowa State University","Standard Grant","Leland Jameson","12/31/2021","$107,742.00","","michaely@andrew.cmu.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1271, 1504, 7335","019Z, 9263","$0.00","The concept of networks is a mathematical term used to analyze relationships between objects (e.g. people, places, and things).  Of particular importance is the study of how influence propagates throughout networks, especially how one can deduce the influence of an entire network by monitoring a few members.  The objective of this project is to establish a comprehensive knowledge base for developing, implementing, and applying computational methods related to this phenomenon.  As a result, this research has strong connections to applications related to social networks, electrical power networks, and quantum systems.  This project also supports a concerted effort to engage underrepresented groups within the research.  Hence, the impact of this educational component includes the development of underrepresented groups within the next generation of STEM researchers.<br/><br/><br/>The main technical contribution of this project is the integration of combinatorial optimization, spectral graph theory, and numerical analysis techniques to examine zero-forcing in networks.  In particular, the research team will incorporate branch decomposition techniques and linear and integer programming techniques to significantly increase the computational efficacy of algorithms to solve zero-forcing problems on graphs. The models and algorithms that result from this study will be validated using experimental data and also by data attainable publicly like electrical grid data.  This research will significantly advance the knowledge base of combinatorial optimization, integer programming, and spectral graph theory while also contributing to the increased scalability and efficiency for solving computationally hard problems related to the aforementioned applications."
"1720408","Collaborative Research:  Efficient High-Order Algorithms for Nonequilibrium Microflows Over the Entire Range of Knudsen Number","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/20/2017","Li-Shi Luo","VA","Old Dominion University Research Foundation","Standard Grant","Yuliya Gorb","06/30/2022","$162,500.00","","lluo@odu.edu","4111 MONARCH WAY","NORFOLK","VA","235082561","7576834293","MPS","1271","8037, 9263","$0.00","Nonequilibrium microflows are ubiquitous in sensors, microfluidics, and microelectromechanical systems and have important applications in bio-medical and environmental sciences, aerodynamic, chemical, and energy industries, and space science. For example, vacuum pumps manipulate rarefied gases at very low density and pressure, where approaches based on continuum theory, which is the engineering model for gaseous flows at standard temperature and pressure, is no longer valid.  The extent of nonequilibrium of a gaseous flow is qualitatively measured by the Knudsen number -- the ratio of mean free path to a macroscopic length.  Nonequilibrium flows may be modeled by the Boltzmann equation for the single-particle velocity distribution function in phase space. Standard methods for numerical solution may not be accurate enough, and their computational cost may be prohibitively expensive due to the high-dimensionality of phase space, especially for time-dependent problems. This project aims to create effective and efficient simulation tools for nonequilibrium microflows. Graduate students are involved in the research.<br/><br/>For low-speed microflows, reduced kinetic models, such as the linearized Bhatnagar-Gross-Krook-Welander (BGKW) equation, coupled with the diffuse reflection boundary condition, are reliable and capable of producing very accurate results for microflows in the whole range of the Knudsen number.  The equation can be transformed into a system of linear integral equations for macroscopic variables including the density of the gas, the flow velocity, and the temperature, which leads to great dimension reduction, consequently drastic enhancement of computational efficiency.  The overarching goal of this research project is to develop efficient high-order algorithms to solve a system of integral equations pertaining to nonequilibrium gaseous flows in various geometries over the entire range of Knudsen number, for applications to microflows.  The work consists of the following technical ingredients to overcome the challenges encountered in simulation of microflows: (1) An accurate and efficient algorithm to evaluate the Abramowitz function on the complex plane, as required for time-dependent problems; (2) Theoretical analysis, especially on the nullspace, of the integral equations; (3) Efficient and high-order algorithms for the integral equations on smooth and nonsmooth convex domains in two dimensions.  The results of the project are anticipated to provide enabling technologies for a broad range of engineering applications involving multiscale multi-physics microflows."
"1719960","Collaborative Research:  Modeling and Computation of Three-Dimensional Multicomponent Vesicles in Complex Flow Domains","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","07/18/2017","John Lowengrub","CA","University of California-Irvine","Standard Grant","Leland Jameson","08/31/2021","$34,475.00","","lowengrb@math.uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271","9263","$0.00","Vesicles have long been considered as model systems for studying fundamental physics underlying complicated biological systems such as cells and microcapsules.  Additionally, vesicles are increasingly being used as carriers for drug delivery or as biochemical micro-reactors operating in physiological environments.  This project aims to develop efficient computational models and numerical tools for modeling of vesicle dynamics, which involves phase separation, formation of compartments, and interactions among vesicles and their aqueous environment.  The results are expected to aid in the design of phase domains on the surface of a vesicle such that specific proteins can anchor on the membrane to initiate subsequent biological reactions.  The project aims to contribute to the forefront of research on constructing artificial cells with multiple compartments. The research also advances numerical method development, analytical theory, and integral equation formulations in the context of bio-membrane mechanics and particulate flows. The project will create opportunities for students to receive interdisciplinary training crossing the mathematical, biological, and physical sciences.<br/><br/>This project addresses the challenges of mathematically modeling and numerically simulating three-dimensional multi-component and multi-compartment vesicles in complex flow domains using sharp interface methods. At the continuum level, the mathematical description of vesicle dynamics is a highly nonlinear, nonlocal moving boundary problem where the bilayer membrane serves as the moving boundary. The fundamental mathematical feature is that this system effectively couples surface phase dynamics, morphological evolution and compartment formation, and fluid motion so that the model describes a more realistic physical system than has been developed previously in the literature. The investigators develop and apply state-of-the-art adaptive numerical methods, perform analytical, numerical and modeling studies of important constituent processes, and work with experimentalists to test the model predictions and to help elucidate the underlying physical processes. The project will investigate how the presence of surface phases and multiple compartments modifies the classical motions and hydrodynamic interactions and may lead to novel dynamical regimes. The project will also investigate the morphological stability of multi-compartment vesicles in applied flows, and possible control strategies using multiple surface phases for optimizing stability in complex flow domains."
"1714973","Collaborative Research: A New Multiscale Methodology and Application to Tumor Growth modeling","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","06/15/2017","06/28/2019","John Lowengrub","CA","University of California-Irvine","Continuing Grant","Junping Wang","05/31/2020","$242,863.00","","lowengrb@math.uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271, 7334","9263","$0.00","The complexity of tumor growth, which involves interactions within cells, among cells, and between cells and their environment, calls for development of mathematical and computational models that can connect processes from the cell, and sub-cell scales, to tissue level scales. These methods are needed to help tumor biologists gain further insight into the underlying mechanisms of the processes (e.g., proliferation, differentiation, and migration) involved in tumor development, at the scales which influence their behavior. Because of this complexity, it has been challenging to functionally link cell and tissue scale processes, the knowledge of which is key to development of predictive multiscale tumor models. However, current models typically use ad-hoc rules to bridge between scales, which limits their predictive capability. This project will address this challenge by developing a new multiscale method where directly measurable quantities at the cell-scale inform the model parameters at the continuum tissue scale through rigorous, mathematical upscaling techniques. The multiscale model will be tested and validated by comparing simulation results against experimentally obtained information about the overall growth rates and spatiotemporal behaviors of the different cells and tumors. The new multiscale method will be used to study pancreatic tumors to elucidate the transition of pancreatic lesions into invasive pancreatic ductal adenocarcinoma (PDAC). By integrating patient data analysis with quantitative tumor modeling, the project will develop reliable methods that can predict the likelihood of pancreatic cyst progression to PDAC using relatively non-invasive approaches. <br/><br/>The project team will develop a new class of multiscale models that bridge these scales non-phenomenologically through application of rigorous upscaling techniques in order to close the continuum equations at the tissue scale and provide an accurate description of the processes across both cell and tissue scales. Specifically, stochastic agent-based models at the cell-scale and continuum partial differential equation models at the tissue-scale will be developed. Consistent functional relationships between the variables at the tissue-scale and measurements at the cell-scale will be found by upscaling the discrete models by using and extending the framework of dynamic density functional theory (DDFT) to obtain multi-cell scale continuum equations that account for correlations among cells as well as biological processes such cell birth and death. Further upscaling to the tissue scale will be done by identifying and deriving equations for slowly varying variables. The consistency of the different models in domains where the scales overlap will be tested and validated. The new multiscale method will be applied to model the progression of pancreatic neoplasms into invasive carcinomas in order to estimate the probability of this progression. Large-scale human patient datasets of pancreatic lesions, provided by our consultants through a separately funded project, will be used to validate and refine the models. The project will enhance the cross disciplinary training of students."
"1720297","Optimal Convergence Rates for Adaptive Finite Element Techniques","DMS","COMPUTATIONAL MATHEMATICS","06/15/2017","06/23/2017","Peter Binev","SC","University of South Carolina at Columbia","Standard Grant","Yuliya Gorb","05/31/2022","$275,000.00","Wolfgang Dahmen","binev@math.sc.edu","1600 HAMPTON ST # 414","COLUMBIA","SC","292083403","8037777093","MPS","1271","9150, 9263","$0.00","Numerical simulation is an indispensable tool for acquiring deeper and more quantitative insight into increasingly complex scientific and technological processes. Despite the ever-increasing power of digital computing facilities, numerical simulation technology is somewhat of a weak link. This research project aims to develop improved adaptive numerical algorithms, which have the ability to optimally allocate computational resources -- viz. degrees of freedom -- in the course of the solution process based on information gathered so far. Economizing as much as possible the number of degrees of freedom with the aid of adaptive solution techniques while still accurately capturing the structures of interest remains central to large-scale simulation and a fundamental prerequisite for ultimately further advancing the frontiers of computability. While large-scale scientific computation usually takes place in a highly interdisciplinary arena, the design of adaptive algorithms with rigorously-founded certifiable performance guarantees is an inherently mathematical task that is pursued in this project.  The many conceptual facets of this research project additionally offer unique opportunities for talented young researchers to develop their potential.<br/><br/>This project aims at developing and analyzing hp-adaptive approximations through a process of locally distributing the degrees of freedom through a coarse-to-fine procedure based on local error estimators. The challenges in accomplishing the goals of the project have two major sources. On the one hand, the type of the partial differential equation to which such methods are to be applied, of course, matters very much. On the other hand, there are several fundamental problem aspects that are independent of the particular application and are primarily of approximation-theoretic nature. Even when restricting the problem to a fixed mesh refinement depth and a largest allowable polynomial degree, finding the optimal degree distribution in conjunction with an adequately locally-refined partition is an NP-hard problem. In particular, when progressing from coarse to successively refined meshes seemingly good degree assignments could turn out at a much later stage to prevent near-optimal results. It is therefore of crucial importance to address such core approximation theoretic issues and understand to what extent they are affected by the particular type of partial differential equation. For instance, when using conforming methods for the important class of elliptic boundary value problems, trial functions need to be globally continuous, which severely impedes the analysis of local refinements due to ""smoothness pollution,"" particularly in the multivariate case. To address these aspects and build a solid footing for future specifications to different application areas is the primary goal of this project. Some of the envisaged theoretical results are expected to be of asymptotic nature. Therefore, the theoretical investigations will be accompanied by implementing the strategies for model problems that shed light on the quantitative behavior of the methods. A high level of adaptivity interferes with parallelization, opening yet another direction of research, especially regarding modern processor technologies."
"1719834","Collaborative Research:  Modeling and Computation of Three-Dimensional Multicomponent Vesicles in Complex Flow Domains","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","07/18/2017","Shravan Veerapaneni","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Leland Jameson","08/31/2020","$34,473.00","","shravan@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","9263","$0.00","Vesicles have long been considered as model systems for studying fundamental physics underlying complicated biological systems such as cells and microcapsules.  Additionally, vesicles are increasingly being used as carriers for drug delivery or as biochemical micro-reactors operating in physiological environments.  This project aims to develop efficient computational models and numerical tools for modeling of vesicle dynamics, which involves phase separation, formation of compartments, and interactions among vesicles and their aqueous environment.  The results are expected to aid in the design of phase domains on the surface of a vesicle such that specific proteins can anchor on the membrane to initiate subsequent biological reactions.  The project aims to contribute to the forefront of research on constructing artificial cells with multiple compartments. The research also advances numerical method development, analytical theory, and integral equation formulations in the context of bio-membrane mechanics and particulate flows. The project will create opportunities for students to receive interdisciplinary training crossing the mathematical, biological, and physical sciences.<br/><br/>This project addresses the challenges of mathematically modeling and numerically simulating three-dimensional multi-component and multi-compartment vesicles in complex flow domains using sharp interface methods. At the continuum level, the mathematical description of vesicle dynamics is a highly nonlinear, nonlocal moving boundary problem where the bilayer membrane serves as the moving boundary. The fundamental mathematical feature is that this system effectively couples surface phase dynamics, morphological evolution and compartment formation, and fluid motion so that the model describes a more realistic physical system than has been developed previously in the literature. The investigators develop and apply state-of-the-art adaptive numerical methods, perform analytical, numerical and modeling studies of important constituent processes, and work with experimentalists to test the model predictions and to help elucidate the underlying physical processes. The project will investigate how the presence of surface phases and multiple compartments modifies the classical motions and hydrodynamic interactions and may lead to novel dynamical regimes. The project will also investigate the morphological stability of multi-compartment vesicles in applied flows, and possible control strategies using multiple surface phases for optimizing stability in complex flow domains."
"1719727","Computational Tools for Polycrystalline Materials","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","04/19/2021","Selim Esedoglu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yuliya Gorb","06/30/2022","$201,890.00","","esedoglu@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","8037, 9251, 9263","$0.00","This project will develop new algorithms for computer simulation of how the internal structure (called microstructure) of many technologically essential materials, such as most metals and ceramics, change during common manufacturing processes such as heat treatment. The microstructure of such materials is known to have implications for the physical properties, such as conductivity or yield strength, of the material. Although models describing how the microstructure evolves, for example during heat treatment, have been available, their efficient numerical simulation at scales large enough to be of practical interest to materials scientists have remained a challenge. The project will address this challenge. Understanding the evolution of microstructure holds the promise of materials with more desirable characteristics. The project will also develop machine learning and computer vision algorithms for automatically extracting microstructure information from experimental measurements (microscopy images) of such materials, so that predictions of the models to be simulated can be more readily checked against experiments. One Ph.D. student's training and thesis work will be an integral part of the research.<br/><br/>Continuum models of interfacial motion in polycrystalline materials often take the form of a system of nonlinear partial differential equations describing the geometric flow of a network of surfaces. For certain important interfacial phenomena, such as grain boundary motion, the evolution is given by second order differential equations describing motion by mean curvature of the network. For that setting, a surprisingly simple, elegant, and efficient class of algorithms known as threshold dynamics have been developed that makes large scale simulation particularly feasible. Other phenomena, such as the motion of the free surface of a thin polycrystalline film, or that of pores in a sintered metal, are described by higher order geometric evolutions, such as motion by surface diffusion, and are more challenging to simulate. This project will develop efficient algorithms for such high order multi-phase geometric evolutions. In particular, it will explore whether threshold dynamics or a combination of it with phase field methods can be devised to simulate the motion of networks of surfaces in which some of the interfaces evolve via motion by mean curvature, and others via motion by surface diffusion, coupled along free boundaries known as junctions along which appropriate boundary conditions are satisfied. Some of these models describing the multi-phase geometric motion of networks of interfaces also arise almost verbatim in the context of computer vision and machine learning. By leveraging this mathematical connection, the project will also develop new algorithms for automatically extracting grain boundaries in microscopy images of real polycrystalline materials."
"1753581","CAREER: High Order Structure-Preserving Numerical Methods for Hyperbolic Conservation Laws","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","07/01/2017","06/29/2021","Yulong Xing","OH","Ohio State University","Continuing Grant","Yuliya Gorb","06/30/2024","$400,000.00","","xing.205@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1271, 8048","1045, 7433, 8396, 8611, 9263","$0.00","Partial differential equations of the type known as hyperbolic conservation laws have attracted great attention in mathematical, scientific, and engineering communities due to their wide practical applications in modeling physical systems of interest in fluid mechanics, aerodynamics, meteorology, combustion, and other areas. Development of efficient and accurate numerical algorithms for simulation of solutions to conservation laws continues to be a challenging task. Structure-preserving methods, which provide numerical solutions that preserve a certain continuum property of the underlying models exactly, are recently demonstrated to be more efficient with limited computational resources. This project aims to develop a comprehensive framework to understand structure-preserving methods for hyperbolic conservation laws. The work will have a direct impact in many multi-disciplinary application areas, including fluid and gas dynamics, astrophysics, and atmospheric modeling. This project also has significant broader impact through various educational and outreach activities aimed at students at all levels. These activities include a summer camp program that will expose students including underrepresented minorities to the areas of mathematical modeling, computational science, and computational mathematics. Graduate students will also be mentored and trained through planned working group activities. <br/><br/>The notion of conservation (of number, mass, energy, momentum) is a fundamental principle that is used to derive hyperbolic conservation laws. Recent study reveals that structure-preserving numerical methods, which either conserve important physical quantities in addition to mass or preserve other properties of the underlying physical problems, are demonstrated to be more accurate and often have a much improved long time behavior. The objective of this project is to establish a detailed study of novel high-order structure-preserving methods for the linear and nonlinear hyperbolic conservation laws arising in various applications, and to educate students at various levels about the potential and challenges of utilizing numerical simulation to solve important practical problems. The PI aims to study structure-preserving numerical methods in the following directions: (i) Energy conserving methods for wave equations; (ii) Asymptotic preserving methods for kinetic equations; (iii) Well-balanced methods for hyperbolic problems with source terms. The activity is planned to include new algorithm development, theoretical numerical analysis, numerical implementation, and practical applications. This project will also provide excellent training opportunities for graduate and undergraduate students interested in computational sciences, and includes an outreach program for high school students."
"1719854","Efficient, Adaptive, and Convergent Numerical Methods for Phase Field Equations with Applications","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","06/28/2017","Steven Wise","TN","University of Tennessee Knoxville","Standard Grant","Leland Jameson","07/31/2021","$200,000.00","","swise1@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","MPS","1271","8037, 9263","$0.00","This project focuses on the design and application of efficient computational algorithms for approximating the solutions to models describing various physical phenomena, including multi-phase fluid flow, tissue growth, phase transformations, and polymer processing. The investigator will study models that apply to applications that include optimizing organic photovoltaic devices for energy conversion; predicting cancerous tumor growth; simulating complex biological flows involving sub-cellular structures like endoplasmic reticulum; and designing durable semiconductor and energy storage materials. As a central pillar of this project, he will continue to develop his software package, BSAM, based on the efficient algorithms developed in the research. This software package, which is, and will always be, freely available and open source, is designed to solve a broad spectrum of nonlinear, multi-physics partial differential equations, in two and three dimensions. This tool is ultimately useful for researchers from many disciplines and provides the capability to efficiently simulate complex phenomena, without the need to reinvent algorithms or redesign code.<br/><br/>The principal investigator will examine high-order, highly nonlinear partial differential equations through focus on three specific project goals. These include the design and rigorous numerical analysis of high-order energy stable numerical schemes, the design and analysis of algorithms and software for efficient two and three-dimensional time-space adaptive modeling and simulation, and the design and rigorous analysis of novel, nearly-optimally complex preconditioned nonlinear solvers. The models under examination in the project describe a number of physical processes, including solidification; grain boundary dynamics; crack propagation; tumor growth; two-phase polymer flows; organic photovoltaic processing; and complex biological flows involving lipid bilayers. Because the equations under study are coupled systems of highly nonlinear, high-order partial differential equations, the analysis of their solutions and the design of efficient and reliable numerical methods that give rise to convergent approximations is a non-trivial task. The principal investigator will design unconditionally energy stable, second and third-order-in-time approximations and aims to rigorously prove that the schemes are optimally convergent. He will implement optimally or nearly-optimally efficient solvers that take advantage of the variational/convexity structure of the proposed schemes, producing sophisticated numerical software."
"1720495","Construction, Analysis, Implementation and Application of New Time Integrators for Large Scale Complex Systems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","08/02/2017","Mayya Tokman","CA","University of California - Merced","Standard Grant","Leland Jameson","08/31/2021","$150,291.00","","mtokman@ucmerced.edu","5200 N LAKE RD","MERCED","CA","953435001","2092012039","MPS","1271","8012, 9263","$0.00","Computer simulations are an essential tool of science and engineering.  Relying on computational models to design a car or predict movement of a hurricane is commonplace practice that allows scientists and engineers to significantly reduce time and resources required to accomplish these tasks.  The complexity of phenomena we are interested in simulating is constantly increasing.  Demand for predictive computer models of complex systems dynamics, such as folding of a protein molecule or movement of a tsunami, is ever-growing and cannot be met with advances in computational hardware alone.  Advanced mathematical methods and computational techniques are an integral component of improving fidelity and efficiency of predictive computational models. In particular, numerical methods, which enable simulation of the time evolution of a complex system, are needed to create computer models that can accurately and efficiently predict the behavior of the system over long periods of time.  The proposed research focuses on construction of advanced numerical techniques that enable simulating dynamics of complex systems that evolve over a wide spectrum of temporal scales.  The project will include both development of theory of such methods as well as their implementation and application to specific problems such as numerical weather prediction and climate modeling.  This work will make available software package that can be utilized in a wide variety of scientific and engineering fields ranging from biochemistry and geo-engineering to fluid dynamics and plasma physics.  <br/><br/>The proposed project will advance the state-of-the-art in time integration from theoretical and practical perspectives. It has become increasingly clear that fast high-order discretization methods to resolve spatial and temporal scales of complex phenomena are essential to ensure that computational models are sufficiently efficient and trustworthy. While extensive research efforts have been dedicated to the development of high-order spatial discretization approaches such as finite-element or spectral methods, research on construction and application of advanced time integrators to large-scale complex problems lags behind. The proposed research will focus on development of new time integration algorithms that significantly improve the efficiency of currently available methods. The project aims to construct, analyze and implement time integration schemes that can simultaneously take advantage of explicit, implicit and exponential integration approaches. Important theoretical questions such as accuracy and convergence of these techniques will be addressed. General integrators as well as classes of methods that can exploit particular problem structure will be derived. In addition, custom-designed time integrators will be created for weather and climate prediction problems to maximize the efficiency of computational models routinely used in these fields.  The most efficient time integration schemes developed as a result of this project will be implemented as part of a publicly available software package designed for serial and parallel computational platforms.   Outcomes of this research will also be integrated into the core graduate curriculum of the applied mathematics program at UC Merced."
"1646339","RTG: Research Training Group in Mathematical Modeling and Simulation","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY, WORKFORCE IN THE MATHEMAT SCI","09/01/2017","06/07/2021","Aleksandar Donev","NY","New York University","Continuing Grant","Pedro Embid","08/31/2023","$1,866,045.00","Charles Peskin, Esteban Tabak, Leif Ristroph, Miranda Holmes-Cerfon","ad139@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1266, 1271, 7334, 7335","7301","$0.00","This Research Training Group (RTG) project is devoted to training through research of undergraduates, graduate students, and postdoctoral fellows in several salient aspects of modern applied mathematics. The activities that this project is based upon recognize the fundamental importance of the interplay between modeling and simulation for most real-life applications. Modeling involves identifying the fundamental components of a problem and posing them in mathematical terms. Simulation solves the mathematical problems thus posed using computers to make quantitative predictions. <br/>Both modeling and simulation will be used to investigate a wide variety of phenomena in physics, chemistry, engineering, and biology, such as how microorganisms swim, how blood flows in the heart, the unusual properties of suspensions of bacteria or active particles, and how to efficiently design new materials. A unique element of the project is an experimental laboratory (Applied Mathematics Laboratory at the Courant Institute) that will provide raw data and motivation for mathematical models and simulations as well as measurements for quantitative validation. The Courant Institute is particularly well-positioned for this enterprise. Since early on, the Institute had a strong emphasis in applied mathematics, with modeling and simulation at its core. This research and training project will increase the number of U.S. citizens, nationals, and permanent residents who are well prepared to undertake careers that require a thorough understanding of applied and computational mathematics, not only in academics, as is the case with many educational mathematics programs, but also in business, industry, and government.<br/> <br/>This RTG program will emphasize the connections among modeling, simulation and experimental observation. The project, coordinated by five Co-PIs, will provide academic-year and summer funding for a growing number of Ph.D. students, starting from three and increasing to six by the end of the project, two postdoctoral scholars per year, as well as a number of undergraduate summer internships, for a duration of five years. The project will support the formation of a vertically-integrated activity which integrates a new research course, a seminar on oral and written presentation, a collaborative research seminar, visitor seminars and undergraduate summer research activities. A unifying theme of the study of passive and active particle suspensions will be used to build collaborations among computational scientists at Courant, the Applied Mathematics Laboratory, and the Soft Condensed Matter physics group at NYU's Physics Department. This research theme and the associated collaborations will serve as a framework for investigating other themes. For all themes, the research activities in this project will train students and postdocs to work in a multidisciplinary environment in which they have access to world leading experts in several disciplines. Furthermore, this research is expected to have substantial scientific impacts and to lead to new discoveries and potential applications. The five-year project will create new activities that will become a permanent part of mathematics teaching, research and training efforts at the Courant Institute, and will provide valuable experience that can be exported to other institutions."
"1719694","Numerical Solution of Partial Differential Equations: Algorithms, Analysis, and Applications","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/01/2017","Douglas Arnold","MN","University of Minnesota-Twin Cities","Standard Grant","Leland Jameson","06/30/2020","$300,000.00","","arnold@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","9263","$0.00","Much of modern technology is dependent on fast and accurate computer simulation of physical phenomena.  For example, computer simulations enable the development of high performance vehicles, medical devices, geophysical monitoring and prediction, and countless other applications on which the health and welfare of our society depend.  Once such a physical system is modeled by a system of mathematical equations, successful simulation depends not only on powerful computer hardware but also on mathematical algorithms that can harness the computer's high speed number-crunching to obtain accurate solutions of model's equations.  While effective simulation algorithms exist for many important physical systems, there remain many important applications for which fast and reliable algorithms do not yet exist. Moreover, it is crucial to develop not only the algorithms, but a robust and rigorous understanding of their performance, in order to be able to assess and certify their accuracy and delineate their limitations.  This project will develop, improve, and validate simulation algorithms for two important and challenging classes of physical phenomena.  The first concerns the propagation of waves through complex and disordered media, such as arises on tiny scales when electrical signals travel through semiconductors, on massive scales when seismic waves travel through the earth, and in many other relevant regimes.  The second application area is in the burgeoning field of gravitational wave astronomy, made possible by the recent detection of gravitational waves from a black hole collision, and highly dependent on numerical simulation of Einstein's equations of general relativity.<br/> <br/>This project will advance the algorithms used to simulate complex physical phenomena modeled by partial differential equations. A key focus of effort will be on the propagation of waves through complex media, as, for example, the conduction of electrons in a semiconductor with impurities. This work will study the remarkable effect known as localization of eigenfunctions, first discovered over 50 years ago in Nobel prize winning research, and with major ramifications for many systems involving wave propagation. Until now, the understanding needed to accurately predict and control localization was lacking, but recent theoretical advances bring it within reach. A major goal of the research will be fast accurate prediction of localization from inexpensive processing of the media. This will then open the way for control of localization: the design of media with desired properties for applications.  The second main direction concerns the simulation of Einstein's equations of general relativity. The goal will be to develop structure-preserving finite element methods for the equations, with the potential to greatly improve accuracy, efficiency, and robustness of simulations of general relativity. The work will focus on two types of approaches. First will be methods related to the Regge calculus introduced over 50 yeas ago as a discrete analogue of general relativity, overcoming the low accuracy of Regge calculus by developing a family of high order Regge elements and attacking the instabilities encountered, by using a 3+1 decomposition to separate time-like from space-like behavior. The second approach will be based on a system known as the Einstein-Bianchi equations, which hold the potential for leveraging advances in the numerical simulation of electromagnetic systems for the benefit of numerical relativity."
"1723153","Conference on the Foundations of Computational Mathematics 2017","DMS","PROBABILITY, ALGEBRA,NUMBER THEORY,AND COM, TOPOLOGY, COMPUTATIONAL MATHEMATICS","07/01/2017","01/04/2017","Ricardo Nochetto","MD","University of Maryland, College Park","Standard Grant","Leland Jameson","06/30/2018","$25,000.00","","rhn@math.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1263, 1264, 1267, 1271","7556, 9263","$0.00","The International Conference on the Foundations of Computational Mathematics 2017 is the ninth of a successful series and will be held in Barcelona, Spain from July 10 to July 19, 2017. This award funds the attendance at the meeting of at least twenty junior U.S. participants, including graduate students,  postdoctoral fellows, and early career tenure track faculty.  The conference features seventeen plenary speakers, eight of whom are from the US, and 21 workshops where the junior participants participants will be able to present a talk or a poster.<br/><br/>The impact of constantly increasing computational power has changed the relationship between mathematics and computation. New  applications such as encryption, large scale data management, and signal/image processing, inevitably led to the study of the  computational tools themselves, creating and reinvigorating a rich spectrum of mathematical disciplines. Mathematicians now increasingly recognize that, besides its value as a numerical and experimental tool, computation is a significant theoretical tool in its own right. This conference aims to further the understanding of the connections between mathematics and computation, including the interfaces between pure and applied mathematics, numerical analysis and computer science. This is reflected in the diverse and vibrant research areas covered by the 21 workshops which are organized by world experts,  including many from the US. This award is  instrumental in enabling  the participation of many US young researchers at this renowned and exciting conference.<br/><br/>Conference website: http://www.ub.edu/focm2017/"
"1720226","Computational Methods for Heterogeneous Soft Living Materials","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","07/01/2017","06/25/2019","Andreas Aristotelous","PA","West Chester University of Pennsylvania","Continuing Grant","Leland Jameson","06/30/2020","$100,115.00","","aaristotelous@uakron.edu","700 S HIGH ST","WEST CHESTER","PA","193830003","6104363060","MPS","1271, 7334","9263","$0.00","Biofilms are present everywhere in the natural world and can have beneficial or adverse health effects. Through this award the principal investigator will provide a solid modeling platform and develop and analyze the computational tools that are needed for better understanding the behavior of biofilm formation and interaction with the microenvironment. For example,  biofilms can be present in water systems affecting pipes and filters, and they can also be found in chronic wounds and infected prosthetic joints and heart valves, causing resistance to antibiotic use and then requiring further surgical interventions and prosthesis replacement. The  computational methods developed in this research will provide an additional inexpensive and powerful method that compliments the work done by experimentalists. The computational algorithms and software tools are sufficiently general that they may be applied to other problems in science and engineering that present similar challenges. An interesting example is to better understand how bacteriophage adhere to mucus as a way to provide a nonhost-derived immunity. Improved insights provided through computational modeling have the potential to lead to the creation of new kinds of antibiotics. <br/><br/>The principal investigator (PI) aims to develop numerical techniques for the solutions of equations that model heterogeneous soft living materials arising in biology. The focus is on biofilms and mucosal surfaces, and their interactions with surrounding fluid and biological agents, e.g. bacteria and viruses.  Multiphase living materials are characterized by adhesive forces, they have viscoelastic properties and can diffuse, grow, deform or shear. The PI plans to focus on computational tools for solving the energy driven Cahn-Hilliard type equations describing phase separation and sharp diffusive interfaces, coupled with fluid flow equations. Interacting biological agents introduce heterogeneities and discontinuities into multiphase systems, highlighting the need for the design of a robust computational framework that couples continuous and discrete models. The PI aims to develop integrated methods for addressing challenges particular to heterogeneous biomaterials. He aims to combine high order discontinuous Galerkin finite element discretizations, in conjunction with time and spatial adaptivity, for analyzing the solution of the underlying systems while accounting for discontinuities, nonlinearities and moving sharp interior interfaces.  Unconditionally energy stable and locally mass conservative schemes will be used to model long time dynamics. The resulting ill-conditioned nonlinear algebraic systems will be solved using efficient solvers based on multigrid techniques that exploit the specific local structure stemming from the choice of discretization. The development of computational tools that resolve these issues can lead to a better understanding of the factors that impact the evolution of soft biological systems. The efficient nonlinear and spatiotemporally adaptive solvers proposed will apply to even more general equations and will therefore advance the field of numerical nonlinear partial differential equations."
"1764187","Path Integral Monte Carlo Methods for Computing Polarizability Tensors of Nano-materials and Electrical Impedance Tomography","DMS","COMPUTATIONAL MATHEMATICS","09/08/2017","10/18/2017","Wei Cai","TX","Southern Methodist University","Standard Grant","Leland Jameson","07/31/2020","$182,516.00","","cai@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","9263","$0.00","This research project aims to develop improved efficient numerical methods for two application areas: highly accurate simulation of the electric and magnetic properties of nanometer-scale materials, and electrical impedance tomography.  In both areas, numerical computations with traditional methods are challenging, if not impossible.  This project aims to develop novel computational methods based on probabilistic representations of solutions to the partial differential equations under study.  Results of the project are expected to have wide applicability, from the development of solar cells to the detection of cancer.<br/><br/>This project concerns the development of highly accurate and efficient numerical methods to simulate the electric and magnetic polarizability tensors of nanoparticles of complex shapes as in nanowires, quantum dots, and DNA, and fast algorithms for electrical impedance tomography (EIT). Due to the geometric complexities of nanoparticles, numerical computations with traditional mesh-based discretization methods such as finite element and boundary element methods face great challenges, if not impossibility. To meet these challenges, in this project, path integral Monte Carlo (PIMC) methods, based on Feynman-Kac probabilistic representations of solutions to partial differential equations, will be studied for material science applications as well as EIT problems. Compared with traditional grid-based numerical methods, the PIMC methods offer the capability of handling objects with highly irregular geometries arising from materials science applications on the one hand, and provide local solutions of partial differential equations over electrodes in forward problems in EIT on the other hand."
"1720473","Collaborative Research: A Posteriori Error Analysis for Complex Models with Applications to Efficient Numerical Solution and Uncertainty Quantification","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","08/01/2017","07/23/2021","Simon Tavener","CO","Colorado State University","Standard Grant","Yuliya Gorb","07/31/2022","$211,902.00","Donald Estep","tavener@math.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1271, 8069","8083, 9263","$0.00","Many scientific and engineering problems of importance to the nation's infrastructure and defense are concerned with multi-physics systems in which multiple physical processes interact in complex ways. An important example is the flow of a liquid transporting reacting chemicals in which the reaction affects the fluid properties of the liquid. Such reacting flows arise in applications ranging from biological systems to combustion processes associated with energy use. In general, the complexity of multi-physics systems prevents direct experimental observation of crucial features. Thus their study depends critically on computing approximate solutions of mathematical models describing the processes and their interactions. However, such simulations strain the computational capabilities of the most powerful computers and consequently, computational errors in the approximations are always significant and may be overwhelming. Over two decades, the project investigators have developed a systematic approach for producing accurate computational estimates of the error of approximate solutions of models of multi-physics systems. In this project, the investigators explore the use of error estimates from this approach to guide the efficient use of computational resources in order to maximize the fidelity of approximate solutions of multi-physics systems. They also apply the error estimates to accurately quantify the uncertainty in predictions of behavior of multi-physics systems based on the approximate solutions of models. The results of this project will enhance the ability of the nation's engineers and scientists to investigate and predict the behavior of complex physical systems important to the nation's security and infrastructure. <br/><br/><br/>This project tackles critical problems associated with using sophisticated cutting-edge multi-discretization numerical methods for multiscale, multiphysics models to pursue scientific inference and engineering design.  The research is based on a posteriori error analysis for multi-physics, multi-discretization problems that quantifies the effects of a wide variety of discretization steps through the use of adjoint problems and computable residuals. The primary focus of the project is twofold: (1) Developing and analyzing methods for using accurate error estimates to guide discretization choices in order to achieve a desired accuracy at roughly minimal computational cost; and (2) Investigating how to extend accurate error estimation methods to address uncertainty quantification for multiphysics systems, where 'discretization' includes the sampling of a random process and the overall error is a combination of discretization and sampling errors. The investigators pursue the development of novel multi-stage approaches to the construction of efficient numerical solutions and the extension of a posteriori error analysis to statistical computations. The project also involves the extension of the theory of a posteriori error analysis to hyperbolic problems and nonstandard quantities of interest."
"1720335","Collaborative Research:  Selection Methods for Algebraic Design of Experiments","DMS","COMPUTATIONAL MATHEMATICS","08/15/2017","08/14/2017","Brandilyn Stigler","TX","Southern Methodist University","Standard Grant","Yuliya Gorb","10/31/2021","$100,000.00","","bstigler@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","9263","$0.00","Data science has emerged as an important field for making decisions based on data collected from sectors as varied as healthcare and housing.  Though data are plentiful, thanks to phone apps, merchant loyalty cards, and social media accounts, there is still a question of whether more data translates to more knowledge. Furthermore collection and storage can be problematic especially when data are sensitive, as it is often the case with clinical trials and genetic experiments.  The problem of selecting information-rich data becomes crucial for creating models that can reliably predict the outcome of future experiments. Few results have been published on the amount of necessary data, and currently there are no guidelines for generating specific data sets which would unambiguously identify a predictive model. As a first step towards developing a complete theory, the PIs will focus on models described by finite-valued nonlinear polynomial functions. (For example, the internal 'function' in WedMD's Symptom Checker returns medical conditions according to symptoms input by the user.)  They will construct the smallest data sets that have a single associated polynomial model and study properties of such data sets.  From these computational experiments, they will build the appropriate theory, design algorithms, and generate code that can be later developed into software complete with a graphical user interface. Graduate students will participate at the appropriate level of each component of the project. Such an experience will provide them possible topics for an MS or PhD dissertation and will very likely inspire a career-long involvement in the STEM disciplines. The theoretical results will advance the fields of design of experiments, network inference, and finite dynamical systems through the determination of criteria for selecting data sets to uniquely identify models. The algorithms will serve as a guide for experimentalists in determining the data that are needed to identify the structure of a network of interest. Such knowledge has the potential to drastically reduce wasted resources that arise from too much data with too little information.<br/><br/>While this is the age of big data, there is still a question of whether more data translates to more knowledge. Particularly when collecting data is expensive or time consuming, as it is often the case with clinical trials and biomolecular experiments, the problem of selecting information-rich data becomes crucial for creating relevant models. Finite-state multivariate polynomial functions have successfully been used to model complex networks from discretized data; however, few results have been published on the amount of data necessary for such models, with the majority applying to Boolean models only. It is still unknown which data points explicitly identify such discrete models, and as a consequence, there are no methods for generating the specific data sets which would unambiguously identify the model.  The PIs will address the issue of the minimality and specificity of data to uniquely identify discrete polynomial models by developing the appropriate theory, designing algorithms, and generating code that can be later built into software. Graduate students will participate at the appropriate level of each component of the project. This project will resolve some important computational issues in network inference and will improve experimental design and model selection by eliminating the effect of computational artifacts that arise when working with nonlinear multivariate polynomials. The theoretical results will advance the fields of design of experiments and network inference through the establishment of criteria to select data sets to uniquely identify models. The proposed work will also increase the utility of polynomial dynamical systems as models of complex networks by establishing the minimal amount of the data for unique model identification. The algorithms will serve as a guide for experimentalists in determining the data that are needed to identify the structure of a network of interest. Such knowledge has the potential to drastically reduce the number of experiments performed and to eliminate the generation of data with little intrinsic value."
"1802143","High Order and Efficient Numerical Methods for Simulating Electromagnetic Phenomena","DMS","COMPUTATIONAL MATHEMATICS","10/02/2017","12/07/2017","Wei Cai","TX","Southern Methodist University","Standard Grant","Leland Jameson","08/31/2019","$119,209.00","","cai@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","8037, 9263","$0.00","New materials with special properties are necessary in the search for new clean energy sources and advanced medical devices. Electromagnetic phenomena play a key role in the design of new materials such as meta-materials and conducting materials. Meta-materials, assembled with blocks of meta-atoms of naturally available components, have provided a wide range of new possibilities to design man-made materials with special properties. Novel devices using meta-materials have been proposed including perfect lens and sub-diffraction-limited imaging for medical applications, light harvest in clear energy solar cells. In addition, understanding the conducting flow of a charged system is essential for studying confined nuclear thermal reactions for the exploration of new clean energy sources.<br/><br/>The computational simulation of electromagnetic phenomena is challenging, owing to the demand of highly accurate and efficient numerical methods that not only represent the correct physics in the magnetic induction equation but also resolve the multiple scattering and local field enhancements from random objects in meta-materials. To meet these requirements, the PIs will accomplish the following two tasks in this project: (1) to develop a highly efficient volume integral equation method for Maxwell equations for very accurate computation of multiple scatterings of large number of regular or random objects employed in the construction of meta-materials; (2) to devise a high order constrained transport finite element method for the magnetic induction equations in the magneto-hydrodynamics  problem so the global divergence free condition on the magnetic field is preserved. The research findings will be disseminated through journal publications and software tool development."
"1720431","Fast Galerkin Methods for Boundary Integral Reformulations of Time Dependent Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/24/2017","Johannes Tausch","TX","Southern Methodist University","Standard Grant","Leland Jameson","05/31/2021","$204,063.00","","tausch@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","9263","$0.00","The project will develop numerical methods for heat transfer and micro flow problems. The focus will be on handling moving geometries that may change the topology over time.  Application areas that immediately benefit from the proposed work are in manufacturing, for instance hot forming processes, solidification and melting processes. Moreover, the numerical solution of micro flows is important in the design of Mems devices, the simulation of microbial swimmers and sedimentation processes. Engineering design is often interactive process, and therefore speed and robustness are essential for the underlying numerical methods.  By engaging a graduate student in research, the proposed activities will also contribute to excellence and growth in the education of the future workforce.<br/><br/>Galerkin boundary integral methods are an effective numerical tool to solve parabolic partial differential equations.  Boundary integral operators are non-local in space and time, which implies that their discrete counterparts are dense matrices. However, a space-time version of the fast multipole method will be used to overcome the high numerical cost associated with dense matrix algebra.  The main goal of this project is to expand this methodology to problems with moving geometries. Here several new techniques will be developed, this includes quadrature methods to handle the unique singularities of parabolic integral operators and new discretization methods of the space-time boundary manifold which will address the difficulties associated with topology changes.  The numerical methods developed will be applied to transient Stokes flow problems and Stefan problems. This project will also investigate shape optimization techniques to solve for the unknown evolution of the solid-liquid interface. Solving free surface problems by minimizing an energy functional fits well within the variational formulation of the Galerkin discretization and promises to be more robust and stable than the more conventional time stepping methods."
"1720452","Multiscale Algorithms for the Geometric Analysis of Hyperspectral Data","DMS","COMPUTATIONAL MATHEMATICS, MSPA-INTERDISCIPLINARY, EnvS-Environmtl Sustainability","10/01/2017","06/01/2017","Demetrio Labate","TX","University of Houston","Standard Grant","Stacey Levine","09/30/2022","$270,284.00","Bernhard Bodmann, Saurabh Prasad","dlabate@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271, 7454, 7643","8007, 9263","$0.00","Hyperspectral imaging is a sensing technique that collects hundreds of narrowband images from across the electromagnetic spectrum. By both going beyond the visible spectrum and accurately discriminating wavelengths within the visible range, this technology can be remarkably powerful for distinguishing different materials when standard imagery is ineffective. As a result, hyperspectral remote sensing offers unique capabilities for tasks that include monitoring the development and health of crops, mapping oil spills and invasive species, and detecting objects that may be camouflaged. With modern remote sensing applications not being constrained to satellite images, however, the image acquisition in many scenarios is no longer under controlled conditions, because illumination, physical parameters, and viewing angles may change over time and objects of interest may be partially occluded. This investigation introduces a new generation of mathematical and algorithmic tools that are designed to provide robust classification of hyperspectral data under such realistic conditions. The project aims to develop a new class of analysis and classification algorithms for hyperspectral data that are robust with respect to changes of illumination, viewpoint, and physical conditions.  The results are intended to have direct application to the monitoring of environmental conditions in coastal wetlands and to other observations of societal, economic, and national security interest.<br/><br/>While hyperspectral imaging and image processing have been well developed within the remote-sensing community, image acquisition in remote sensing may occur in conditions where illumination, physical parameters, and viewing angle change over time. This research program combines ideas from sparse representations, multilayer convolutional networks, and machine learning to address the challenges to imaging posed by such changing conditions. A novelty of the approach is the adaptation of methods from sparse representations and shearlets, an anisotropic multiscale system that is particularly effective at capturing the directional content of multidimensional data. This approach provides the basis for constructing a deep learning neural convolutional network tailored to hyperspectral data and designed to generate stable and robust feature vectors. This investigation aims to develop an efficient multiscale representation that is customized to the specifics of hyperspectral data. The scattering transform will be adapted in combination with shearlets by exploiting the covariance properties of shearlets under affine transformations to build stable and viewpoint-invariant features for hyperspectral data. A novel hierarchical scheme for classification optimized for the specific structure of hyperspectral data and sparsity-based inpainting methods to restore hyperspectral data corrupted by occlusions will be developed. These new algorithms will be used for the analysis of hyperspectral data to monitor environmental conditions of coastal wetlands, a challenging case study of great social and economic importance."
"1719828","Practical Large-Scale Sum-of-Squares Optimization","DMS","OE Operations Engineering, COMPUTATIONAL MATHEMATICS","07/01/2017","06/03/2019","David Papp","NC","North Carolina State University","Continuing Grant","Leland Jameson","06/30/2021","$220,000.00","","dpapp@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","006y, 1271","073E, 9263","$0.00","Polynomial optimization is a fundamental computational technique, with applications in a wide variety of fields that include power systems engineering, signal processing, statistics, geometry, and medicine. There are several existing computational approaches for polynomial optimization; however, they all share a few core ideas that limit both their efficiency and stability. As the size and complexity of the models arising from modern applications continues to increase, these existing approaches are increasingly limiting. This research project is aimed toward the development of novel computational methods that are simultaneously more reliable and more efficient than the existing techniques. To assure the relevance of the research, the approaches will be implemented as easily usable computational tools, which will be disseminated widely to the scientific and engineering community.<br/><br/>One of the most common approaches to the solution of global polynomial optimization problems utilizes semi-definite programming (SDP) hierarchies. These arise from combining the algebraic theory of sum-of-squares polynomials and the observation that sum-of-squares polynomials are semi-definite representable. While theoretically satisfactory, the translation of sum-of-squares optimization problems to SDPs is not always practical. First, the SDP representation of sum-of-squares polynomials roughly squares the number of optimization variables, increasing the time and memory complexity of the solution algorithms by several orders of magnitude. The second problem is numerical. In the common SDP formulation, the dual variables are semi-definite matrices whose condition numbers grow exponentially with the degree of the polynomials involved. This is detrimental for a floating-point implementation. This project builds on recent results in non-symmetric conic optimization and multivariate interpolation to derive the algorithmic theory and practical computational tools needed to circumvent the need to use the standard SDP-based approach to sum-of-squares optimization. The aim is to provide algorithms for these problems that are both efficient and computationally effective. The principal investigator will investigate the impact of the novel algorithm developments for a diverse set of applications, including the design of optimal radiotherapy treatments."
"1720116","OP: Collaborative Research: Compatible Discretizations for Maxwell Models in Nonlinear Optics","DMS","COMPUTATIONAL MATHEMATICS, GOALI-Grnt Opp Acad Lia wIndus, WORKFORCE IN THE MATHEMAT SCI","08/01/2017","08/08/2019","Vrushali Bokil","OR","Oregon State University","Continuing Grant","Yuliya Gorb","10/31/2022","$127,761.00","","bokilv@math.oregonstate.edu","1500 SW JEFFERSON ST","CORVALLIS","OR","973318655","5417374933","MPS","1271, 1504, 7335","019Z, 8990, 9251, 9263","$0.00","Nonlinear optics is the study of the behavior of light in nonlinear media. This field has developed into a significant branch of physics since the introduction of intense lasers with high peak powers. Compared with the huge amount of literature on simulations of Maxwell's equations in linear optical media, developing mathematically well-understood computational tools for space-time models in nonlinear optical media is relatively less tackled by the computational math community. Major advancement in this aspect can provide the scientific community reliable and accurate tools to simulate and to understand nonlinear optical phenomena, which hence can be better harnessed for practical applications. <br/><br/>The objective of the collaborative research program is to make significant advances in the understanding and simulations of Maxwell models in nonlinear optics with the aim of: (1) providing robust simulation tools for the nonlinear optics community, (2) developing novel mathematical and numerical techniques that are specifically tailored for different types of nonlinear models. The specific technical aspect includes the development of energy-stable time discretizations as well as two classes of spatial discretizations, discontinuous Galerkin methods and mimetic finite difference methods, for the propagation of electromagnetic waves in nonlinear (dispersive) optical media. Both macroscopic phenomenological and microscopic quantum descriptions will be considered for modeling the nonlinear material responses. Applications involving femtosecond soliton propagation, harmonic generation, self focusing, among others will be simulated and compared to existing time domain methods. This collaborative program is strengthened by a cohesive research plan that relies on the complementary expertise of each principal investigator. The educational components are integrated through the training of graduate students."
"1720487","Fine-Scale Singularity Detection in Multi-Dimensional Imaging with Regular, Orientable, Symmetric, Frame Atoms with Small Support","DMS","COMPUTATIONAL MATHEMATICS, MSPA-INTERDISCIPLINARY, Modulation","07/15/2017","07/20/2017","Emanuel Papadakis","TX","University of Houston","Standard Grant","Stacey Levine","06/30/2022","$250,000.00","Demetrio Labate","mpapadak@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271, 7454, 7714","8007, 8091, 9263","$0.00","One of life's essential characteristic is movement. Whether it is the spectacular, delicate dance of the unblemished, white swan in Tchaikovsky's Swan Lake, or the early attempts of a toddler to use his or her hands, the neurology of movement is uniquely common for all animals and humans: learning a motor skill, and the necessary muscle coordination. With practice the skill is perfected. Finally, the retainment of this experience-based learning process is the conclusion of this learning process. The ultimate goal of this project is to provide new tools to neuroscientists who study the biological basis of learning at the cell level using live animals. This function is facilitated by a number of anatomical changes in the structure of the cytoplasm of neural cells, such as the formation of lengthy branches known as axons and dendrites, and at a fine scale of dendritic spines and axonal buttons. The latter are less anatomically permanent structures arising on the surface of these cytoplasmic extensions. Dendritic spines and axonal buttons form synapses, which are the communication gateways between neurons. The research team will develop mathematical and computational tools for automatizing the study of spine populations in live neurons, and of their time-evolution during learning. The anticipated outcomes will provide neuroscientists with a number of software tools which will automatize the analysis of synaptic strength and its evolution with learning. These findings will contributed to the better understanding the biological mechanisms of autism and drug addictions.<br/><br/>The investigators on this project will develop algorithms for the 3D digital segmentations of dendritic surfaces including spines from 3D images acquired with a certain type of microscope, which uses laser light and works as a scanner by exploiting the natural ability of neurons to fluoresce. They aim to generate accurate binary reconstructions of a dendritic arbor including its spines. The primary challenge in this project is that image acquisition of live neurons has a resolution which provides limited detail of the spines. Often, images contain noise which further complicates the extraction of accurate, binary 3D reconstructions of dendritic surfaces showing spine details. Overcoming this problem is a core goal of the project because spine volume estimation quantifies synaptic strength. These unique challenges lead the investigators to the development of novel mathematical tools for fine scale analysis. They will build ensembles of short in size 3D imaging, 3D-orientation selective, frame-based filters, suitable for sensing curves and surfaces in noisy images. These filters will be designed to respond to local changes of image smoothness. Information obtained from these filters at various scales, will be utilized as input for multilayer, deep-learning inspired neural networks which will determine in an image which voxels belong to spine surfaces. Further algorithmic tools will be developed to track every spine of a dendrite individually over time. The same filtering tools will be used in a different application domain, the generation of illumination neutral images, in real-time. This will help the fast, high throughput removal of the effects of uneven illumination in images inhibiting the detection, by software or the naked eye of contours associated with shapes or textures in a scene. The investigators will develop the mathematical theory of illumination neutralization using concepts from fractal and microlocal analysis. The illumination neutralization algorithm is envisioned to work for real-time video analysis and in conjunction with face verification algorithms with the potential to be used in face recognition, laser microscopy and remote sensing applications."
"1720402","Collaborative Research: A Posteriori Error Analysis for Complex Models with Applications to Efficient Numerical Solution and Uncertainty Quantification","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/17/2017","Jehanzeb Chaudhary","NM","University of New Mexico","Standard Grant","Leland Jameson","07/31/2021","$100,160.00","","jehanzeb@unm.edu","1700 LOMAS BLVD NE STE 2200","ALBUQUERQUE","NM","871063837","5052774186","MPS","1271","8083, 9150, 9263","$0.00","Many scientific and engineering problems of importance to the nation's infrastructure and defense are concerned with multi-physics systems in which multiple physical processes interact in complex ways. An important example is the flow of a liquid transporting reacting chemicals in which the reaction affects the fluid properties of the liquid. Such reacting flows arise in applications ranging from biological systems to combustion processes associated with energy use. In general, the complexity of multi-physics systems prevents direct experimental observation of crucial features. Thus their study depends critically on computing approximate solutions of mathematical models describing the processes and their interactions. However, such simulations strain the computational capabilities of the most powerful computers and consequently, computational errors in the approximations are always significant and may be overwhelming. Over two decades, the project investigators have developed a systematic approach for producing accurate computational estimates of the error of approximate solutions of models of multi-physics systems. In this project, the investigators explore the use of error estimates from this approach to guide the efficient use of computational resources in order to maximize the fidelity of approximate solutions of multi-physics systems. They also apply the error estimates to accurately quantify the uncertainty in predictions of behavior of multi-physics systems based on the approximate solutions of models. The results of this project will enhance the ability of the nation's engineers and scientists to investigate and predict the behavior of complex physical systems important to the nation's security and infrastructure. <br/><br/>This project tackles critical problems associated with using sophisticated cutting-edge multi-discretization numerical methods for multiscale, multiphysics models to pursue scientific inference and engineering design.  The research is based on a posteriori error analysis for multi-physics, multi-discretization problems that quantifies the effects of a wide variety of discretization steps through the use of adjoint problems and computable residuals. The primary focus of the project is twofold: (1) Developing and analyzing methods for using accurate error estimates to guide discretization choices in order to achieve a desired accuracy at roughly minimal computational cost; and (2) Investigating how to extend accurate error estimation methods to address uncertainty quantification for multiphysics systems, where 'discretization' includes the sampling of a random process and the overall error is a combination of discretization and sampling errors. The investigators pursue the development of novel multi-stage approaches to the construction of efficient numerical solutions and the extension of a posteriori error analysis to statistical computations. The project also involves the extension of the theory of a posteriori error analysis to hyperbolic problems and nonstandard quantities of interest."
"1719620","Ubiquitous Doubling Algorithms for Nonlinear Matrix Equations and Applications","DMS","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS","09/01/2017","08/06/2021","Ren-Cang Li","TX","University of Texas at Arlington","Standard Grant","Yuliya Gorb","08/31/2022","$255,624.00","","rcli@uta.edu","701 S. NEDDERMAN DR","ARLINGTON","TX","760199800","8172722105","MPS","1253, 1271","1253, 1515, 9263","$0.00","The principal investigator's (PI's) research is aimed at the solution of practical problems of optimal control theory from areas as diverse as vibration analysis for high speed trains and quantum transport in nano research. Underlying these applications are formulations that require the efficient and accurate solution of important nonlinear matrix equations which critically influence the overall performance and fidelity of the entire simulations. Past experience has shown that practically relevant simulations requiring solutions of nonlinear matrix equations are notoriously challenging to the point that computed solutions may be completely erroneous. This project aims at changing the status quo by developing a complete theory, devising innovative algorithms to better reflect problem structures, and designing more robust implementations. In addition to advancing research in these nonlinear matrix equations, the PI will recruit and train graduate students in computational mathematics and interdisciplinary studies.<br/><br/>This project will advance the understanding and solution techniques for high impact nonlinear matrix equations in the context of mathematical theory, computational methods, and software. For decades, computational scientists and engineers have been struggling to compute trustworthy numerical solutions to some of these nonlinear matrix equations with limited success. For example, the entries of the solution matrices to M-matrix algebraic Riccati equations from Markov-modulated fluid flow theory represent probabilities of events, and tiny entries indicate events that are rare but still important,  and thus need to be computed accurately. The eigenvalues from vibration analysis of high speed trains vary widely in magnitude beyond the ranges that the IEEE double precision can handle if not dealt with correctly. The doubling algorithm may not converge at all on the unperturbed nonlinear matrix equations for quantum transport in nano research. These difficulties are often not caused by the large sizes of the problems but rather are more due to their inherent mathematical properties. With the successful completion of this project, a complete and coherent unifying framework of doubling algorithms, along with the relevant theory, will be developed. A much deeper understanding of doubling algorithms will be gained, and it is possible that important applications will be identified where doubling algorithms can be utilized and are faster than the current state-of-the-art methods. More significantly, the theory developed will assure that the new algorithms will produce more trustworthy accurate numerical results than is currently possible."
"1720489","Modeling, Analysis, and Computation for Water-Drive Oil Recovery","DMS","COMPUTATIONAL MATHEMATICS","06/15/2017","06/13/2017","Ying Wang","OK","University of Oklahoma Norman Campus","Standard Grant","Leland Jameson","05/31/2021","$139,999.00","","wang@ou.edu","660 PARRINGTON OVAL RM 301","NORMAN","OK","730193003","4053254757","MPS","1271","9150, 9263","$0.00","Accurate mathematical analysis and efficient numerical methods play an increasingly important role in studying partial differential equation models in the petroleum industry. The goal of this project is to carry out research in mathematical analysis and design of high-order-accuracy numerical methods for the modified Buckley-Leverett (MBL) model, which is a partial differential equation model for water-drive secondary underground oil recovery. When an underground source of oil is tapped, a certain amount of oil flows out on its own due to pressure difference. After the flow stops, there is typically a significant amount of oil still left in the ground. One standard method of ""secondary recovery"" is to pump water into the oil field through an injection well, forcing oil out through a production well. In this process there will be a water and oil mixture created. The MBL equation models the evolution of the oil saturation in the entire oil reservoir, in particular, at the production well.<br/><br/>This research project combines mathematical analysis, design of numerical schemes, and computational techniques. The mathematical analysis is based on partial differential equation theory, and the numerical methods under development are based on state-of-the-art discontinuous Galerkin (DG) schemes. The results will be cross-validated using data from laboratory experiments. The investigator plans to carry out the following specific research tasks: (1) extend the well-developed 1D MBL model to 2D and 3D fully nonlinear and linearized MBL models; (2) determine the approximation error induced by employing the MBL model on a spatial domain smaller than an entire reservoir; (3) design high-order-accuracy discontinuous Galerkin (DG) methods to numerically solve the MBL model; (4) study the nonlinear asymptotic stability of the traveling-wave solutions of the MBL model; (5) employ experimental data to cross-validate both the analytical and computational conclusions. The project involves a postdoctoral associated and a graduate student in the research. The investigator also plans to develop a new graduate-level course on numerical solutions to water-drive oil recovery models."
"1720222","RUI:  Efficient Adaptive Backward Stochastic Differential Equation Methods for Nonlinear Filtering Problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/23/2019","Abdollah Arabshahi","TN","University of Tennessee Chattanooga","Continuing Grant","Leland Jameson","07/31/2022","$124,995.00","Feng Bao, Anthony Skjellum","abi-arabshahi@utc.edu","615 MCCALLIE AVE","CHATTANOOGA","TN","374032504","4234254431","MPS","1271","9229, 9263","$0.00","Nonlinear filtering problem is a mathematical model for system estimation in signal processing problems arising from various scientific and engineering fields. Examples of the nonlinear filter's applications include tracking an aircraft using radar measurements, estimating a digital communications signal using noisy measurements, and estimating the volatility of financial instruments using stock market data. The key mission of the nonlinear filtering problem is to establish a ""best estimate"" for the true value of a dynamic system from an incomplete, potentially noisy set of observations on that system. The goal of this project is to develop novel numerical algorithms, which are accurate and efficient for the nonlinear filtering problem, by solving a backward stochastic differential equation (SDE) system. The proposed project will engage undergraduate students at an RUI institution in computational and applied mathematics research.<br/><br/>The cornerstone of this proposed approach, named the backward SDE filter, is the fact that the solution of the backward SDE system is the probability density function of the signal state as required in the nonlinear filtering problem. This project will start with the construction of backward SDE filter algorithms that are high order in time and adaptive in space, which blends the strengths of well known methods from this area of research. Then, the applicability of the backward SDE filter will be enlarged to tackle the grand challenge problems. Specifically, massively parallel algorithms will be designed for the backward SDE filter so that it could be implemented to solve large scale scientific computing problems on high performance computing facilities. The backward SDE filter is a new approach to solve the nonlinear filtering problem, and it addresses the main issues in the numerical solutions for nonlinear filtering problems, such like the low regularity problem and the high dimensionality problem. As a result, the backward SDE filter will provide scientists and engineers in various disciplines an accurate, efficient, and easy to use algorithm for data assimilation."
"1720276","OP: Variational Principles, Minimization Diagrams, and Mixed Finite Elements in Computational Geometric Optics","DMS","COMPUTATIONAL MATHEMATICS","06/01/2017","06/03/2019","Gerard Awanou","IL","University of Illinois at Chicago","Continuing Grant","Leland Jameson","05/31/2021","$200,000.00","","awanou@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1271","8037, 8990, 9263","$0.00","In many devices, including projection displays, laser weapons, and medical illuminators, it is required to accurately control light.  The fundamental question to be addressed by this research project in computational geometric optics is the efficient design of lenses and mirrors through provably convergent numerical methods. The goal of the project is to develop improved efficient and theoretically sound algorithms for a variety of illumination problems. The project involves training of graduate students through involvement in the research.<br/><br/>Available tools for the design of refractors and reflectors are limited, and some are not backed up by a sound theory. Promising approaches consist in solving numerically the associated nonlinear partial differential equations of Monge-Ampere type and variational methods that solve the illumination problem as an equation in measures. Existing methods based on partial differential equations make ad hoc assumptions and do not address appropriately the unusual boundary conditions for these problems. On the other hand, most existing variational methods scale poorly with the size of the problem. This has created a need for improved efficient and robust numerical methods based on rigorous analysis to solve computational geometric optics problems. This project aims to:  (1) implement and analyze an efficient variational method, based on minimization diagrams, for computing solutions of a variety of illumination problems; (2) address the numerical resolution of the relevant partial differential equations with a provably convergent and efficient finite-difference method; and (3) solve the relevant nonlinear equations with mixed finite elements based on approximations by smooth functions. The project will also investigate the convergence properties of the different approaches. It is anticipated that the results of the project will identify which approach is the most efficient."
"1720433","Collaborative Research: Tolerance-Enforced Simulation of Stochastic Processes","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2017","08/17/2017","Jing Dong","IL","Northwestern University","Standard Grant","Leland Jameson","08/31/2020","$89,428.00","","jd2736@columbia.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1271, 8069","8083, 9263","$0.00","High performance computing of continuous random structures arises in a large body of scientific and engineering investigations. For example, these structures are used in environmental models for floods in different geographical areas, which are subject to random measurement errors. They are also used in the prediction and mitigation planning of potential disasters. However, these random structures are impossible to capture in a computer without incurring bias, due to their continuous nature. This research project investigates a new framework for the numerical analysis of continuous random structures. It achieves stronger error control, compared to current state-of-the-art methods, at basically the same computational cost. If successful, the framework and algorithms to be investigated will facilitate analysis and performance evaluation of fundamental random structures of interests to a broad community of scientists and engineers. To enhance the broader impact, the Principal Investigators will train graduate students through research and integrate the results from this research into new graduate courses in scientific computing. <br/><br/>This project investigates a new Monte Carlo framework for continuous stochastic structures (such as differential equations and random fields). The main innovative feature of the framework is the ability to approximate a continuous random object by a fully simulatable (typically piece-wise constant) object with a uniform error bound in the path space with 100% certainty. The error bound is user-specified and can be sequentially refined. Research projects involve developing simulation algorithms for fundamental random structures of interests. These include: Gaussian random fields, Levy processes, fractional Brownian motion, max-stable fields, etc. The algorithms are scalable in the sense of being easily extendable to more complex models by applying the continuous mapping principle with quantifiable error analysis. An important aspect of the methodology is the connection established between Monte Carlo simulation and the theory of rough paths in the setting of stochastic analysis."
"1720218","New Simulation Methods for Levy Processes and Related Distributions","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","08/01/2017","Zhiyi Chi","CT","University of Connecticut","Standard Grant","Leland Jameson","07/31/2021","$201,273.00","","zhiyi.chi@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1271","9263","$0.00","In the age of high power computing, random simulation, or sampling, of Levy processes and infinitely divisible distributions is key to finding numerical solutions to many complicated problems in those fields. In addition to their ubiquitous application in finance and insurance, Levy processes and infinitely divisible distributions are extremely useful in a wide range of fields of physical or social science, technology, engineering, and industry, such as turbulence, laser cooling, internet traffic, communication or server queues, equipment maintenance, health hazard monitoring, and clinical trial.  The project aims to make advances in Levy processes through development of novel approaches, with emphasis on high dimensional situations. The results of the project will be integrated into course material to attract students with diverse backgrounds to research on random sampling and its applications in other fields.<br/><br/>The lack of closed-form distribution functions is one of the most serious hurdles to the random sampling.  This project proposes to build on the so-called ""embed-and-extract"" approach developed by the PI to exact sampling of the first exit event of a large class of univariate Levy processes and a so-called ""Poisson-gamma-normal"" approximation for approximate sampling of univariate infinitely divisible distributions.  Based on these preliminary findings, the goal of the project is twofold. The first goal is to develop new random sampling methods for the multivariate processes.  Second, the project will develop refined or new ideas and methods to sample for the univariate processes.  To achieve the goal, the project will investigate the issue of random sampling from two aspects.  First, exact sampling methods will be developed for the first passage event or first exit event of a Levy process.  These random events play an important role in applications as well as in theory, but at present, the knowledge on their exact sampling is very limited, especially for multivariate processes.  Second, high-precision approximate sampling methods will be developed for infinitely divisible random variables as well as Levy processes, with the emphasis on easy implementation."
"1720416","Computational Methods for Multivariate Orthogonal Polynomials","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","08/03/2017","Akil Narayan","UT","University of Utah","Standard Grant","Leland Jameson","07/31/2020","$99,998.00","","akil@sci.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1271","9263","$0.00","Much of computational mathematics relies on the task of constructing computer-based models that predict a physical property, such as expected capacitance of an eletrochemical battery or power output of a wind turbine. A computationally stable way to construct such a computer model is to build new mathematical functions. This project aims to provide rigorous mathematical foundations and robust computational tools for generating polynomials and subsequently devise efficient algorithms for using these polynomials to build functions. The objectives of this project provide insight into the related mathematical fields of analysis and applied approximation theory, and aid computational scientists in building robust simulation models.<br/><br/>Approximations built from an expansion in orthogonal polynomials are classical tools in applied mathematics. These approximations are frequently the bedrock of algorithms for computationally solving differential and integral equations. The generation and manipulation of such expansions in one variable has been the subject of a great deal of theoretical and computational research in past decades, and there are constructive algorithms for most problems of interest. Far less is known in the multivariate non-tensorial case, for which there are only a few restrictive computational tools for generation of multivariate orthogonal polynomials. The research of this project focuses on theoretical development and computational implementation of methods for generating multivariate orthogonal polynomials on non-tensorial domains with non-tensorial weights. Mathematical investigations of this project involve fundamental contributions to the theory of multivariate orthogonal polynomials, and design of robust algorithms for generation of multivariate orthogonal polynomials. From a practical standpoint the algorithms and methodologies in this project produce expansions in an orthogonal series and will be useful in various engineering design, optimization, and reliability contexts."
"1719699","Numerical Analysis of Smoothed Particle Hydrodynamics Type Methods via  Nonlocal Models","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/20/2017","Qiang Du","NY","Columbia University","Standard Grant","Leland Jameson","07/31/2020","$200,000.00","","qd2125@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1271","9263","$0.00","Computational fluid dynamics is an important research field that plays a crucial role in the understanding of fluid flows appearing in many mechanical, hydrodynamic and biophysical processes. It occupies a central place in the development of computational science.  The proposed research intends to help designing effective numerical algorithms, particularly those related to the so-called smoothed particle hydrodynamics (SPH), for modeling complex fluids and interfacial phenomena.  The overall research objective is consistent with the long term vision of predictive and reliable computational science, and in the near term, it serves to complement ongoing research on SPH related methods and their applications currently being carried out by various academic institutions and national laboratories.  The PI will not only work to facilitate the research effort but also to strengthen the training and education of young students and junior researchers. He will team up with collaborators to ensure the timely translation and integration of new theoretical findings into enhanced simulation capability for a variety of applications such as those involving heterogeneous transport in underground, atmospheric and biophysical systems, energy and high-strength materials, which are highly relevant to important national and societal interests. <br/> <br/>Particle based computational methods such as the Smoothed Particle Hydrodynamics (SPH) and related methods offer great flexibility in numerical simulations and are becoming widely used in various scientific and engineering applications. As these techniques get populated into major simulation codes to be used by a large computational science and engineering community,  it is imperative to carry out a more quantitative assessment and mathematical analysis as part of the rigorous validation and verification process. Assessing SPH based simulations is challenging since these methods have been historically applied to solve complex problems where either traditional methods do not work well or the formal accuracy is of secondary concern. Studies based on conventional numerical analysis techniques may not always produce mathematical findings that are strongly relevant in practice. Our proposed research is to improve the theoretical understanding of SPH and related methods. A novelty of our approach draws on the recently developed nonlocal models and their numerical approximations by the PI's group. It leads to new avenues to analyze SPH type methods by both distinguishing and relating the different roles of integral kernel representations/approximations of the locally defined spatial derivatives and numerical discretization of the resulting nonlocal operators. Indeed, inappropriate nonlocal relaxations of differential operators on the continuum level may be the root cause of some problematic issues inherent to particle based simulations. Incompatible discretization can also contribute to the loss of fidelity and stability. By taking nonlocal integral operators and nonlocal continuum formulations as bridges connecting continuum PDE models and particle like discrete approximations, our approach represents a significant departure from conventional numerical analysis that compares the discrete schemes with the underlying continuum PDEs directly. The focus on algorithm robustness is particularly relevant to SPH like methods given their intended application to complex systems involving multiple scales and extreme operating conditions. Specific objectives for the next few years include: developing continuum reformulations of SPH like methods with nonlocal/integral operators; and studying SPH type methods using discretization of nonlocal models as a bridge.  In carrying out the proposed work, we use an integrated analytical and computational approach to provide both mathematical infrastructure needed for theoretical analyses and practical insight  for code development. We pay close attention to techniques that work for solutions lacking regularity or exhibiting strong variations and for particle distributions and boundary conditions that are frequently encountered in practical implementations."
"1720171","Extensions of Boundary Integro-Differential Operators and the Associated Computational Methods","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/18/2017","Yen-Hsi Tsai","TX","University of Texas at Austin","Standard Grant","Leland Jameson","07/31/2020","$209,807.00","","ytsai@math.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","9263","$0.00","This research project will advance computer-based simulations for better process control and prediction in a wide range of applications; these include seismic imaging, petroleum engineering, electromagnetism in domains containing thin-wires, and molecular biology applications where molecular electrostatics energies are needed to understand the structure-function relations of proteins. The training of students in the proposed research areas, both through direct supervision of the PI or through the new graduate course to be developed, will allow them to conduct research in highly inter-disciplinary projects and bring state-of-the-art numerical analysis and computational algorithms to the related areas.<br/><br/>This research project will develop a general framework for formulating extension of a class of boundary integro-differential operators for numerical computation. The main advantage of the framework is that simple and accurate numerical algorithms can easily be designed on a variety of grid geometries and computational methodologies. The research will concentrate on (i) boundary integro-differential equations that arise from wave scattering problems in unbounded domains containing irregularly shaped scattering surfaces, and (ii) calculus of variation problems posed on manifolds of different codimensions. Regarding to (ii), minimization of convex energies and least-action principles defined on the manifolds will be considered. The minimization problems lead to elliptic equations on surfaces while the least-action principles lead to hyperbolic equations. A major focus will be on deriving extensions of these boundary operators ""to the bulk"", while preserving as much analytical properties (of the operators and of the solutions) as possible, using the additional degrees of freedom that come from the codimensions of the boundary. Concrete applications involving electromagnetic wave propagation coupled with thin-wires in space will be studied and simulated under the framework."
"1719635","Conference on Nonconvex Statistical Learning, University of Southern California, May 26-27, 2017","DMS","OE Operations Engineering, COMPUTATIONAL MATHEMATICS, CDS&E-MSS","04/15/2017","12/30/2016","Jong-Shi Pang","CA","University of Southern California","Standard Grant","Leland Jameson","03/31/2018","$15,000.00","Phebe Vayanos, Meisam Razaviyayn","jongship@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","006y, 1271, 8069","7556, 9263","$0.00","The two-day interdisciplinary Conference on Nonconvex Statistical Learning takes place at the campus of the University of Southern California on Friday, May 26, and Saturday, May 27, 2017. The website of the conference: https://sites.google.com/a/usc.edu/cnsl2017/home will be continuously updated prior to the conference and will provide a repository for the lectures of the meeting to be made available generally.  In today's digital world, huge amounts of data, i.e., big data, can be found in almost every aspect of scientific research and every walk of human activities.  These data need to be managed effectively for reliable prediction, inference, and improved decision making.  Statistical learning is an emergent scientific discipline wherein mathematical modeling, computational algorithms, and statistical analysis are jointly employed to address such a data management problem.  The aim of the conference is to bring together researchers at all levels from multiple disciplines, including computational and applied mathematics, optimization, statistics, and engineering to report on the state of the art of the conference subject and exchange ideas for its further development. Collaborations among the participants will be fostered with the goal of advancing the science of the field of statistical learning and promoting the interfaces of the involved disciplines. The format of the conference consists of roughly two dozen lectures given by expert researchers of the field.  Break times in-between the lectures are scheduled to allow discussions among all participants who will include graduate students, postdoctoral fellows, researchers in academia and industry, and faculty members in universities. This award provides support targeted for the travel expenses of junior participants. <br/> <br/>Till now, convex optimization has been a principal venue for solving many problems in statistical learning.  Yet there is increasing evidence supporting the use of nonconvex formulations to enhance the realism of the models and improve their generalizations.  Superior results and new advances have occurred in areas such as computational statistics, compressed sensing, imaging science, machine learning, bio-informatics and portfolio selection in which nonconvex functionals are employed to express model loss, promote sparsity, and enhance robustness. This conference provides a forum for the participants to report on their research and exchange ideas pertaining to the use of nonconvex functionals in statistical learning.  The topics are organized in four main streams: modeling, advances in computation, big-data statistical learning, and innovative applications.  The lectures will cover both theory and algorithms as well as promising directions for further research."
"1720067","A Novel Regularization-Based Computational Framework for State-Constrained Optimal Control","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","07/15/2019","Baasansuren Jadamba","NY","Rochester Institute of Tech","Continuing Grant","Leland Jameson","08/31/2021","$135,000.00","Akhtar Khan","bxjsma@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","MPS","1271","8037, 9263","$0.00","The research plan for this project is motivated by a broad range of practical applications. A relevant example is in the localized heat treatment of cancer in which the intent is to heat the tumor cells, but at the same time assure that nearby healthy cells are not heated, and hence not damaged. A goal of this kind is called an optimal control problem with pointwise state constraints. The principal investigators will develop novel computational models for the solution of these control problems. Their research is based on the cross-fertilization of ideas from diverse disciplines of mathematics and application domains, and has strong potential for impact in engineering domains such as the optimization of the process of producing hot steel profiles without the generation of cracks. The research team will also integrate their research in the university educational program in the mathematical sciences and will produce basic software that can be made available for solution of other significant applications that fall into the same modeling framework. <br/><br/>The principal investigators aim to develop a novel regularization approach for the solution of pointwise constrained optimal control problems. Such problems are a focus of considerable recent research and pose serious challenges for finding reliable solutions. One of the main issues is that the associated Lagrange multipliers are Radon measures so that the control has low regularity. This causes adverse effects at the analytical level when obtaining optimality conditions for the control problem, and at the numerical level when performing discretization.  The lack of regularity can be attributed to the fact that the underlying ordering cone has an empty interior. Consequently, no general Karush-Kuhn-Tucker theory is available. In fact, the failure of a Slater-type constraint qualification is a common hurdle in numerous branches of applied mathematics including optimal control, inverse problems, non-smooth optimization, and variational inequalities. Conical regularization provides a unified framework to study optimization problems for which a Slater-type constraint qualification fails to hold due to the empty interior of the ordering cone associated with the inequality constraints. The investigators plan to develop new error estimates for the conical regularization for optimal control of partial differential equations and variational inequalities with pointwise state constraints. The project will test the new theoretical results for Nash equilibrium problems, linear elasticity, and supply chains on networks. The project also has an educational impact in the training of graduate students. The investigators will integrate education with research and design courses to teach state-of-the-art techniques on optimal control."
"1802516","Collaborative Research:  Random Dynamics on Networks","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","10/18/2017","Daniel Tartakovsky","CA","Stanford University","Continuing Grant","Leland Jameson","07/31/2018","$230,626.00","","tartakovsky@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1271","9263","$0.00","Transport and distribution networks come in a number of forms, from animal cardiovascular and respiratory systems to communication and industrial infrastructures. Practical issues abound: prediction of local spikes, estimation of perfusion, and impact of structural changes such as vessel occlusion. The complexity of such phenomena can be illustrated by the well-known Braess' paradox: adding links to a transportation network might not improve the operation of the system! In spite of recent successes, our understanding of network flows is usually limited to small deterministic problems, while most applications correspond to large uncertain ones. The goal of this project is to enable improved predictions in biological and technological transport and diffusion networks. For instance, can one predict how cerebral blood flow will be affected if one of the carotids becomes narrow or blocked? Will the vasculature allow for re-routing? If so, with what probability and how fast? <br/> <br/>The main challenge in this research project is the presence of uncertainties. For many applications, only partial information about the systems is available. For instance the size or even the presence of a specific vessel might be uncertain or the status of a router unknown. The analysis of such problems requires the creation of novel mathematical tools and numerical methods to describe how uncertainties propagate through vast and complex networks. The computational tools to be constructed will provide information, usually probabilistic in nature, regarding phenomena that are difficult, expensive, or impossible to measure."
"1802189","Collaborative Research: Density-enhanced data assimilation for hyperbolic balance laws","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","11/09/2017","Daniel Tartakovsky","CA","Stanford University","Standard Grant","Leland Jameson","08/31/2019","$200,000.00","","tartakovsky@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1271","9263","$0.00","The research addresses the urgent need to develop efficient computational tools to process the dramatically increasing amounts of observational data. Management of many complex systems (e.g., traffic) has to confront the uncertainty in both their current and future state. This uncertainty typically increases with time, leading to less accurate and useful predictions. Thus, it is important to develop practical methods for ?adjusting? the probabilistic state of the system and reducing uncertainty using observational data. This approach is broadly referred to as data assimilation. We will develop novel techniques for incorporating observational data to reduce uncertainty in predictions in two particular areas  of national interest: fluid dynamics (e.g., flood forecasting) and traffic management. Both are of vital importance to sustainable development of our society.<br/><br/>We propose to develop a novel data assimilation framework for physical processes whose time-dynamics is described by hyperbolic conservation laws. This framework takes advantage of a kinetic representation of hyperbolic systems and, thus, availability of explicit deterministic equations for the time evolution of probability density function for dependent variables. These equations can often be derived and solved exactly, yielding explicit analytical solutions for the marginal and joint probability density functions. For systems of hyperbolic conservation laws an appropriate closure assumption is needed. Thus, the proposed framework relies on the kinetic representation, which takes the form of linear equations for joint probability density functions. Bayesian updating is utilized to incorporate observations into the prediction."
"1720014","Efficient  High Frequency Integral Equations and Iterative Methods","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/01/2017","07/24/2017","Yassine Boubendir","NJ","New Jersey Institute of Technology","Standard Grant","Yuliya Gorb","06/30/2022","$249,063.00","","boubendi@njit.edu","323 DR MARTIN LUTHER KING JR BLV","NEWARK","NJ","071021824","9735965275","MPS","1266, 1271","9263","$0.00","The project is distinguished by its great wealth of potential scientific applications and broad educational activities. Indeed, the numerical algorithms to be developed in the research activities are applicable to realistic configurations in physics, acoustic/electromagnetic, and other disciplines. The major theoretical and computational difficulties in these fields result from the presence of complicating factors such as complex geometries (including aircraft, satellites, radars, antennas, etc.), high-frequency scattering, amongst others. The obtained methods will provide the ability to simulate such systems accurately in order to be applied  to the design of engineering vehicles and devices, including military and non-military radar, remote sensing satellites, noise reduction, stealth technology, and many others that will be positively impacted by the results of this proposal. The solvers obtained will be made readily available to industrial scientists, which will contribute to maintaining their competitiveness in particular in the aerospace industry. The educational impact will be significant in several areas. Graduate and undergraduate students will be rigorously trained in both scientific computing and mathematical analysis in order to enable them to face future challenges in science and technology.  They will acquire the skills needed in state-of-the-art in applied numerical methods, and this will provide them great opportunities to join high technological industries and contribute in further advancing the U.S technology while having a successful career.<br/><br/>The investigator plans to develop efficient and accurate algorithms for acoustic/electromagnetic wave propagation problems in complex structures. The new proposed research activities will have a significant impact in enabling advances in numerical methods and mathematics, and will result in a new family of numerical algorithms with enhanced capabilities over those currently available. The investigator plans to develop a robust non-overlapping domain decomposition method for the Helmholtz equation based on the utilization of optimized transmission conditions on the artificial interfaces and appropriate use of the adaptive radiation condition technique. This also will allow the design of an effective algorithm coupling finite and boundary elements. In the case of the high frequency regime, the investigator proposes to (1) use asymptotic expansions of solutions of the Helmholtz equation, namely the normal derivative of the total field, to rigorously develop a O(1) high frequency integral equations solver, (2) analyze the stability and convergence of the resulting algorithms, and (3) suitably combine high performance computing with the new proposed methods to efficiently tackle real-life problems. Parallel computing and mathematical analysis will be used to help achieve these goals."
"1720305","OP: Collaborative Research: Novel Feature-Based, Randomized Methods for Large-Scale Inversion","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","05/15/2017","Eric de Sturler","VA","Virginia Polytechnic Institute and State University","Standard Grant","Leland Jameson","08/31/2021","$148,999.00","","sturler@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271","8990, 9263","$0.00","The desire to form an image of a region of space from externally collected data arises in applications ranging from detecting and characterizing cancers in the body, to quantifying the distribution of water, oil, or subsurface pollutants, and to the timely accurate identification of explosives in crowded venues. The physics associated with signal propagation and sensing in these problems creates substantial computational challenges for transforming raw data into useful information. The research team in this project aims to develop computational methods that greatly reduce the cost of real time imaging by providing improvements in statistical inverse theory, numerical inversion methods, simulation models, and hybrid imaging models.  The main thrusts of the project will be tested on imaging applications in medical tomography, environmental remediation, and airport security imaging. The techniques form the basis for addressing analogous problems associated with inversion of optical signals across a wide range of spatial and temporal scales. As part of the project, a modular course will be developed to teach these new methods at the graduate level. The course materials will be made available over the internet.<br/><br/>The large-scale imaging, or inverse, problems addressed by this collaborative team require the minimization of a parameter-dependent function that expresses the misfit of predicted measurements for a candidate image and actual measurement data. The potentially large number of parameters must be minimized over an ever-increasing huge number of measurements, while concurrently some unknown set of the data may be redundant.  Detailed images, however, are not always needed for addressing relevant, practical questions and decision making. A combination of computational techniques will be developed to make large-scale parameter-dependent minimization computationally feasible.  Furthermore, novel efficient approaches for inferring critical image features will be developed, obviating need for complete reconstruction of an image. The research builds on recent methods that exploit randomization to compute accurate estimates of solutions at greatly reduced computational cost, and on the efficient construction of smaller, approximate, reduced order numerical models that are accurate for relevant sets of parameters, and thus reduce the cost of full simulation of the sensing physics. Probabilistic approaches for inference of critical image features that guide image interpretation and decision making will be developed. The mathematics associated with this approach requires these methods to capitalize on other new tools also under development in this project."
"1719217","Early-Career and Student Support for the XX Householder Symposium","DMS","COMPUTATIONAL MATHEMATICS, Algorithmic Foundations","05/15/2017","01/05/2017","Eric de Sturler","VA","Virginia Polytechnic Institute and State University","Standard Grant","Leland Jameson","04/30/2018","$20,000.00","Mark Embree, Serkan Gugercin","sturler@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271, 7796","7556, 7933, 9263","$0.00","This project will support the participation of early career scientists and PhD students from US universities in the ""Householder Symposium XX on Numerical Linear Algebra,"" to be held at The Inn at Virginia Tech, June 18-23, 2017, in Blacksburg, Virginia, http://www.math.vt.edu/HHXX/. The Householder symposium is the premier meeting in the field of numerical linear algebra and participation in this symposium is by application and invitation only. The symposium places a strong emphasis on supporting early-career scientists and the intermingling of such scientists with the leading experts in the field. The support of young researchers will  enhance the continued leadership of the US in this crucial discipline and its application to technologies that greatly benefit society. Advances and applications of numerical linear algebra are fundamental for handling large data sets with applications crossing web searches, medical imaging, scientific simulations, all high performance computing applications, and the design of both materials and structures.<br/><br/>Google's PageRank algorithm is a smart combination of methods from two numerical linear algebra fields, Perron-Frobenius theory/Markov chains and eigenvalue solvers. Similarly, deriving crucial information from today's giant data sets, which dwarf anything encountered before, whether from business, government, or the sensors in new medical imaging equipment, relies on substantial innovations to a core numerical linear algebra method, the singular value decomposition. Finally, ensuring efficient simulations on the next generation of supercomputers to assess the continued safety of the nation's nuclear weapons requires drastic changes to linear solvers, another ubiquitous numerical linear algebra method. Participation in the Householder Symposium, which includes discussions of hot new research topics and emerging areas will allow young researchers to  be exposed to problems of significant societal importance."
"1720257","Algorithms for Large-Scale Nonlinear Eigenvalue Problems: Interpolation, Stability, Transient Dynamics","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","05/10/2017","Mark Embree","VA","Virginia Polytechnic Institute and State University","Standard Grant","Leland Jameson","06/30/2021","$349,999.00","Serkan Gugercin","embree@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271","9263","$0.00","Dynamical systems are mathematical models of the changing world.  To understand such a model, one must describe how the system will evolve in time. Of interest is whether the solution grows with time, and whether the short-term behavior of the system is different from the behavior over a large-time window.  This project aims to develop tools to help scientists and engineers better analyze a challenging class of models in which the system's current rate-of-change is dictated by its configuration at some time in the recent past, models known as ""delay systems.""  Compelling examples come from biology, where this delay could correspond to gestation in a population, the incubation of a disease, or the time for a pill to dissolve.  In many important scenarios, highly accurate models require thousands or millions of variables.  This project will design high-performance computing algorithms to efficiently assess the behavior of such systems.<br/><br/>The research team will develop a new class of algorithms that are derived from interpolation techniques for model reduction of large-scale dynamical systems, for solving important large-scale nonlinear eigenvalue problems.  Nonlinear eigenvalue problems play an increasingly important role in many applications, including the stability analysis of delay differential equations.  Such problems pose a great challenge to computation: the number of eigenvalues is often infinite, even when the dimension of the problem is finite.   This project's interpolation-based techniques hold the promise of accurately modeling nonlinear operators over a broad region of the complex plane, resulting in algorithms that are both more efficient and more robust than existing methods.  Combining interpolation with structure-preserving subspace projection has the potential to approximate a much larger number of eigenvalues than possible with traditional methods.  To understand the performance of these new algorithms and to improve their speed, the project will also address convergence theory; for greater efficiency the project will explore inexact solution methods. Improved solvers for the nonlinear eigenvalue problem will inform development of the other two main aspects of this project: determination of critical stability transitions as a function of parameters (such as the time delays) in differential equations, and an enhanced understanding of the transient behavior of dynamical systems associated with nonlinear eigenvalue problems.  This project is expected to lead to substantial improvements in algorithms and analysis for the nonlinear eigenvalue problem and related stability questions."
"1719829","Finite Element Methods for Incompressible Flow Yielding Divergence-Free Approximations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/20/2017","Michael Neilan","PA","University of Pittsburgh","Standard Grant","Leland Jameson","06/30/2020","$157,487.00","","neilan@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271","8037, 9263","$0.00","This research project aims to develop practical, structure-preserving computational methods to simulate incompressible flow.  Systems with incompressible flow are ubiquitous in computational fluid dynamics (CFD) and arise in several engineering and biological models such as those used in aircraft design, weather prediction, and lipid membrane modeling.  The main focus of the project is to identify compatible computational methods that inherit at the discrete level the intrinsic structure and invariants of the systems under study.  Such discretizations produce numerical schemes with enhanced stability and conservation properties, resulting in physically relevant and accurate approximations. It is anticipated that the results of the project will directly benefit researchers and practitioners in CFD and be useful in applications that require faithful approximations.  In addition to the construction of these numerical schemes and algorithms, the project will analyze the stability and convergence of the methods; the theoretical analysis will provide insight for future development of numerical schemes with improved efficiency, accuracy, and fidelity.<br/><br/>The project is centered on developing finite element methods for the incompressible Stokes and Navier-Stokes equations that enforce the divergence-free constraint exactly.  Such schemes have several desirable properties, including improved error estimates, enhanced long-time stability and accuracy of time-stepping schemes, explicit characterizations and local bases of divergence-free subspaces, and coupled-method accuracy when combined with projection methods. Specific objectives of this project include (i) developing stable finite element pairs for Navier-Stokes equations in three dimensions that strongly enforce exact conservation of mass; (ii) applying simplicial Bernstein-Bezier theory, a powerful analytical tool for polynomial splines, to the mixed finite element framework; (iii) constructing robust and computationally attractive methods for axisymmetric fluid models; and (iv) developing stable mixed finite element pairs on surfaces by incorporating fluid flow models in a finite element exterior calculus framework."
"1830838","Development and Application of Efficient High-order Semi-Lagrangian Schemes","DMS","COMPUTATIONAL MATHEMATICS","08/16/2017","04/27/2018","Wei Guo","TX","Texas Tech University","Standard Grant","Leland Jameson","08/31/2020","$55,590.00","","weimath.guo@ttu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1271","8012, 9263","$0.00","Understanding behaviors of plasmas plays an increasingly important role in modern science and engineering such as thermo-nuclear fusion, satellite amplifier, and computer chip manufacturing. A fundamental model in plasma physics is the Vlasov-Maxwell system, which is a nonlinear kinetic transport model describing the dynamics of charged particles due to the self-consistent electromagnetic forces. As predictive simulation tools in studying the complex kinetic system, efficient, reliable and accurate transport schemes are of fundamental significance. The main numerical challenges in such studies lie in the high dimensionality, nonlinear coupling, and inherent multi-scale nature in both space and time. Another application concerned in this project is in atmospheric science. One example is the chemistry-climate model in the study of evolution of stratospheric ozone and many other chemical constituents. The present generation of global climate models include hundreds of tracer species in order to adequately represent complex physical and chemical processes, resulting in huge computational cost in computer simulations. <br/> <br/>The PI will develop and analyze a class of efficient, reliable and highly accurate numerical methods for transport problems in plasma physics and atmospheric science. A semi-Lagrangian framework will be devised by employing a high order discontinuous Galerkin spatial discretization to take advantage of its many attractive properties, such as flexibility, compactness, and excellent ability to resolve features involving multiple scales. By a careful design in the scheme formulation, the proposed scheme is free of splitting error and able to conserve total mass of the system. Motivated by the work of PI on developing a fast asymptotic preserving Maxwell solver that is capable of recovering the magneto-static limit, the temporal scale separation issue associated with the Vlasov-Maxwell simulations will be addressed. For the applications in the global chemistry-climate modeling, the new schemes can be conveniently adapted to the spherical transport simulations based on the cubed-sphere geometry. Theoretical issues including the stability analysis and error estimates will be investigated.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1654673","CAREER: High Order Structure-Preserving Numerical Methods for Hyperbolic Conservation Laws","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","07/01/2017","04/12/2017","Yulong Xing","CA","University of California-Riverside","Continuing Grant","padmanabhan seshaiyer","09/30/2017","$50,744.00","","xing.205@osu.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1271, 8048","1045, 7433, 8396, 8611, 9263","$0.00","Partial differential equations of the type known as hyperbolic conservation laws have attracted great attention in mathematical, scientific, and engineering communities due to their wide practical applications in modeling physical systems of interest in fluid mechanics, aerodynamics, meteorology, combustion, and other areas. Development of efficient and accurate numerical algorithms for simulation of solutions to conservation laws continues to be a challenging task. Structure-preserving methods, which provide numerical solutions that preserve a certain continuum property of the underlying models exactly, are recently demonstrated to be more efficient with limited computational resources. This project aims to develop a comprehensive framework to understand structure-preserving methods for hyperbolic conservation laws. The work will have a direct impact in many multi-disciplinary application areas, including fluid and gas dynamics, astrophysics, and atmospheric modeling. This project also has significant broader impact through various educational and outreach activities aimed at students at all levels. These activities include a summer camp program that will expose students including underrepresented minorities to the areas of mathematical modeling, computational science, and computational mathematics. Graduate students will also be mentored and trained through planned working group activities. <br/><br/>The notion of conservation (of number, mass, energy, momentum) is a fundamental principle that is used to derive hyperbolic conservation laws. Recent study reveals that structure-preserving numerical methods, which either conserve important physical quantities in addition to mass or preserve other properties of the underlying physical problems, are demonstrated to be more accurate and often have a much improved long time behavior. The objective of this project is to establish a detailed study of novel high-order structure-preserving methods for the linear and nonlinear hyperbolic conservation laws arising in various applications, and to educate students at various levels about the potential and challenges of utilizing numerical simulation to solve important practical problems. The PI aims to study structure-preserving numerical methods in the following directions: (i) Energy conserving methods for wave equations; (ii) Asymptotic preserving methods for kinetic equations; (iii) Well-balanced methods for hyperbolic problems with source terms. The activity is planned to include new algorithm development, theoretical numerical analysis, numerical implementation, and practical applications. This project will also provide excellent training opportunities for graduate and undergraduate students interested in computational sciences, and includes an outreach program for high school students."
"1801103","Quantum Computing Workshop for Advancing Aerospace Sciences","DMS","COMPUTATIONAL MATHEMATICS","11/01/2017","10/19/2017","William Oates","FL","Florida State University","Standard Grant","Leland Jameson","10/31/2019","$10,000.00","Mohammed Hussaini","woates@eng.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1271","7556, 9263","$0.00","A workshop entitled ""Quantum Computing Workshop for Advancing Aerospace Sciences"" is scheduled to be held in Suffolk, VA at the Lockheed Martin Center for Innovation on November 7-8, 2017.  More details on the workshop can be found at http://www.nianet.org/quantumcomputing/.  The goals of the workshop are to understand the challenges and opportunities to develop a new type of computing methodology that significantly increases computer speed and can be applied to science and engineering problems.  This has implications on a variety of applications including machine learning, materials design, personalized medicine, advanced weather prediction, energy distribution and optimization, among others.  Quantum computers use an entirely different computing paradigm in comparison to conventional computers.  Conventional computer hardware uses silicon to process information in terms of ""bits"" that can be encoded in either zeros or ones.  Quantum computers use what is known as ""qubits"" which allow information to be encoded by both zeros and ones at the same time using the unusual behavior of quantum mechanics.  This quantum behavior has been measured in different experimental systems by controlling and measuring light and electricity, for example.  However, transitioning these prototype machines into a quantum computer with a large number of qubits that can reliably processes information is still in its infancy.  This workshop will bring together industry leaders, government laboratory researchers, and university researchers to define new goals and objectives to transition how small scale quantum computing testbeds can be scaled up using advanced hardware and quantum algorithms.  This is expected to provide a roadmap for solving practical problems of interests to the general public and national security.  The invited speakers will discuss challenges and opportunities centered around the following four topic areas: 1) Quantum algorithms, 2) Quantum computing hardware, 3) Control and error correction of quantum systems, and 4) Aeroscience applications.  Fourteen global experts, including two female speakers, will discuss current research related to these four quantum computing areas. <br/><br/>The objective of this workshop is to bring together experts on quantum computing and quantum information to discuss the challenges, opportunities, and latest developments in quantum algorithms, hardware, and its impact on supporting aeroscience computations.  The goal is to develop a roadmap for success that connects mathematicians, physicists, computer scientists, and engineers to collectively explore capabilities where quantum speed-up may impact domain specific problems in science and engineering applications.  These specific areas may include computational materials science, fluid dynamics, uncertainty quantification, machine learning, among others.  This will include discussions on the latest advances in algorithm development, scalability, universal logic, and error correction with the aim to understand the next set of mathematical challenges required to control quantum systems, measure their outputs, and preserve their properties from outside disturbances.  Quantum speed-up is well known in specific algorithms that lead to exponential increases in factoring prime numbers (Shor's algorithm) and quadratic speed increases in unstructured searches (Grover's algorithm).  More recently, it has been suggested that exponential speed-up can be achieved in linear algebra problems.  Similar research has focused on new Monte Carlo quantum algorithms. This research is focused on identifying efficient means to solve partial differential equations on a quantum computer.  Based upon the chosen topics, we will focus discussions on these topics where quantum algorithms and hardware may converge onto methods that allow for efficient solutions to partial differential equations relevant to a broad area of mathematics, physics, materials science, and engineering problems.  Through discussions among quantum computing stakeholders in industry, government laboratories, and academia, we expect to facilitate building partnerships across disciplines and provide opportunities for young faculty and graduate students to learn about quantum computing.  We envision developing a roadmap that defines the next set of realistic challenges that can be met over the next 10-15 years."
"1719851","OP: Scattering and Imaging of Subwavelength Nanostructures: Asymptotics and Algorithms","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","07/22/2019","Junshan Lin","AL","Auburn University","Continuing Grant","Leland Jameson","12/31/2020","$236,004.00","","jzl0097@auburn.edu","321-A INGRAM HALL","AUBURN","AL","368490001","3348444438","MPS","1271","8990, 9150, 9263","$0.00","Nano-optics refers to the study of the remarkable properties and phenomena associated with nano-scale materials or devices when interacting with optical light. It is a vigorously growing discipline that has opened up a broad range of possibilities for modern science and technology. With the advent of innovative patterning techniques allowing for the sculpting of materials with nanometer precision, it is presently promising to fabricate optical devices that can perform tasks at scales unattainable with conventional optics, and with great speed and efficiency. This project is devoted to the optical scattering and imaging of nano-structures. The outcome of the theoretical work will provide advances in the understanding of new types of light-matter interactions in subwavelength nano-structures. In addition, the computational frameworks will provide inexpensive, fast, and accurate modeling of optical wave propagation in such tiny structures and enhance near-field optical imaging techniques.  Furthermore, the fast implementations of forward modeling and inverse imaging tools will also provide realistic guidance for the design of nano devices with the desired properties in their interactions with the optical light.<br/><br/>The project focuses on examining fundamental mathematical issues and developing computational methods for new and important classes of problems arising from the optical wave scattering and imaging of nano-structures. This consists of mathematical studies of extraordinary field enhancement in metallic nano-structures and their super-resolution imaging via inverse scattering. The technical focus of the model problems is on Maxwell's equations in complicated and multi-scale media. New analytical tools based upon a combination of the boundary integral equation approach and asymptotic analysis techniques will be developed for rigorous studies of the tightly confined optical wave fields in nano apertures/holes. Computationally, in order to address the significant challenges brought by the extreme scale difference between the aperture size and the wavelength of radiation, fast and high-order horizontal and vertical mode matching numerical schemes will be designed for the simulation of wave propagations in nano-structures. Finally, efficient numerical algorithms based on the inverse scattering theory will be applied for imaging multi-scale subwavelength structures and for breaking the diffraction limit."
"1707658","Workshop on Quantification of Uncertainty: Improving Efficiency and Technology","DMS","COMPUTATIONAL MATHEMATICS, Hydrologic Sciences","03/01/2017","06/21/2017","Max Gunzburger","FL","Florida State University","Standard Grant","Leland Jameson","02/28/2018","$22,760.00","Marta D'Elia","gunzburg@fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1271, 1579","7556, 9263","$0.00","The workshop ""Quantification of Uncertainty: Improving Efficiency and Technology"" will be held on July 18-21, 2017 at the International School for Advanced Studies in Trieste, Italy, https://indico.sissa.it/event/8/. This NSF award exclusively supports the participation costs of junior US-based attendees at the workshop who will benefit from engaging with leading experts. Internationally recognized experts will present recent progress and discuss future directions of algorithmic and mathematical research in the quantification of uncertainties in the outputs of complex systems that are subject to random uncertainties in their inputs. Because such systems are ubiquitous, the workshop will impact the scientific, engineering, social, financial, economic, environmental, and commercial milieus. The structure of the workshop is designed to maximize its short- and long-term impact. The scientific focus of the workshop is on three very promising algorithmic areas for which near-term improvements would have an immediate and lasting impact on all the settings mentioned above. An important feature of the workshop is several discussion sessions at which participants can use the information gathered from the lectures to agree on the best possible research directions for each algorithmic area. To maximize their impact, the results of the discussions will be widely disseminated via a web site and through the publication of articles in professional society news magazines. Finally, the long-term impact of the workshop will be greatly enhanced by having a substantial majority of the participants be junior researchers. The workshop lectures and discussion sessions will greatly help those participants to form cutting-edge, long-term research programs.<br/><br/>The workshop will address complex systems modeled by partial differential equations and probabilistic descriptions of uncertainties. Reductions in the cost while maintaining a desired fidelity for uncertainty quantification for this setting can be realized in two ways: one can reduce the cost of obtaining approximate solutions of the partial differential equation and/or one can reduce the number of times the partial differential equation has to be solved. For the former, the workshop will focus on two approaches. First is the development of more efficient solvers for the large discrete systems that arise from, e.g., finite element discretizations. The second is the development of improved reduced-order models that result in much smaller, and thus much cheaper to solve, discretizations of the partial differential equation. Reductions in the number of times the partial differential equation has to be solved will be addressed through the development of improved methods for approximating the dependence of solutions on the random parameters, especially when a large number of parameters is involved. Recent advances in high-dimensional approximation theory will play a prominent role."
"1719558","Collaborative Research: Tractable Non-Convex Optimization","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/26/2019","Nicolas Boumal","NJ","Princeton University","Continuing Grant","Leland Jameson","06/30/2020","$165,000.00","","nboumal@math.princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1271","9263","$0.00","Practitioners in most fields of science and engineering invest a substantial amount of time developing appropriate mathematical models for the complex questions they set out to answer. A model is deemed appropriate for a given task if it meets two potentially conflicting criteria simultaneously. On the one hand, it must be sufficiently faithful to reality (and as such, sufficiently complex) so as to capture the essential properties of the object of study. On the other hand, the model must be simple enough that it can be practically used to answer relevant questions. This second requirement is computational in nature. In effect, the modeler aims to reduce a particular question to a mathematical problem known to be practically solvable, or tractable.  For the most part, tractability ensures that the problem can be solved using known algorithms, and as a result the status quo has been that problems that are not tractable should be avoided for applications. Yet, scores of problems in science and engineering are most naturally modeled within a framework that has been previously determined to be non-tractable. This research project aims to develop theory and algorithms to identify and solve non-convex optimization problems, typically regarded as non-tractable. The goal is to provide practitioners with an extended modeling toolbox, allowing them to capture key aspects of our complex reality.<br/><br/>This project targets optimization problems that are tractable despite non-convexity. This can come about in a number of ways. The non-convexity may be structurally benign, in that the problem actually does not have local optima at all. This project explores such structural effects in the context of Burer-Monteiro relaxations. Alternatively, the problem may present numerous local optima in some instances, yet present only good quality ones on instances of the problem encountered in practice. This motivates the analysis of non-convex optimization problems in a non-adversarial setting, in many cases more relevant to practice than classical adversarial analyses. This project investigates some model problems of this nature. Here too, salvation can come in different forms: It is possible that when data is good enough (for example, if the signal to noise ratio is sufficient), local optima cannot exist; or, it is possible to initialize the algorithms close enough to the global optimum so that convergence to it is assured; or, even local optima are satisfactory to answer the underlying question. This project explores such situations through applications in community detection in large networks and through phase synchronization, as model problems for understanding challenges in a more general class of problems including, but not limited to, electron cryomicroscopy from structural biology and simultaneous localization and mapping in robotics. A common feature of many tractable non-convex optimization is that they are naturally posed on smooth nonlinear spaces called Riemannian manifolds. As a result, important algorithmic aspects of this project involve developing theory, algorithms, and software for optimization on Riemannian manifolds."
"1719582","Computing Optimal Alignments of Surfaces","DMS","COMPUTATIONAL MATHEMATICS","07/15/2017","07/19/2017","Joel Hass","CA","University of California-Davis","Standard Grant","Leland Jameson","06/30/2021","$250,038.00","Patrice Koehl","hass@math.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271","9263","$0.00","Almost everything we see in the world around us is a two-dimensional surface. Whether with our eyes, with radar or with a laser scanner, we receive data about the geometry of the surface that forms the outer boundary of an object. There is a fundamental need to understand how to analyze this geometric data for purposes of aligning and comparing pairs of surfaces. This project will develop and implements new and rigorous mathematical methods to compare shapes, based on techniques of low-dimensional topology. It will develop new mathematical results and create new computational tools. Applications range from facial recognition to brain mapping to protein classification. The underlying mathematical theory of shape comparison will be extended, and new algorithms to implement the resulting theory will be developed and software made available. Moreover this software will be applied and tested on collections of biological shapes, such as protein surfaces, databases of faces, collections of bones and teeth, and brain cortices.<br/><br/>This project will develop new mathematical measures of the distortion based on a variety of measures of the energy needed to stretch one surface over another. Given two surfaces, or shapes, algorithms will be developed to produce an optimal correspondence or alignment between them. The difference or similarity of two shapes will be captured in new distance functions that capture aspects of geometric similarity. The project builds on successful existing approaches used in comparing surfaces that have the topology of a sphere. These methods will be improved and extended to more general surfaces, allowing for comparing surfaces with one, two, or more handles. Past methods based on conformal maps will be extended to allow use of more general surface diffeomorphisms. The project will also produce explicit alignments between partial surfaces, or surfaces with boundary. This partial surface matching allows for matching surfaces when only portions of each have been measured, with other parts obscured. It also allows comparison between pairs of surfaces having different topologies, overcoming a problem faced by current methods. The symmetric distortion energy recently introduced will be extended to these more general contexts. Software implementing these geometric algorithms will be developed and made available.  This software will be designed to be usable by scientists in a wide variety disciplines and could lead to breakthroughs in biological and medical understanding."
"1654152","CAREER: Predictive Simulations of Complex Kinetic Systems","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","09/01/2017","07/08/2021","Jingwei Hu","IN","Purdue University","Continuing Grant","Stacey Levine","11/30/2021","$406,200.00","","hujw@uw.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271, 8048","1045, 9263","$0.00","This project aims to build an integrated program of research and education focused on advances in predictive simulations of complex kinetic systems. Such systems are comprised of a large number of particles in random motion and are best described by the Boltzmann and related kinetic equations. In practical applications, there are many sources of uncertainties that can arise in kinetic systems: imprecise measurements for initial and boundary conditions, incomplete knowledge of the fundamental interaction mechanism between particles, and so on. Understanding the impact of these uncertainties is critical to the simulations of the complex kinetic systems, and will allow scientists and engineers to obtain more reliable predictions and perform better risk assessment. Due to the unique challenges arising in kinetic equations, such as multiple scales, high dimensionality, and positivity, very few existing generic uncertainty quantification (UQ) algorithms can be applied directly. To bridge this gap, the research objective of this project is to develop highly efficient stochastic and multiscale numerical methods for Boltzmann-like kinetic equations. A parallel educational objective is to create innovative opportunities for students at all levels to improve science, technology, engineering, and mathematics (STEM) education and promote career interest in these disciplines, especially among female students. <br/><br/>Specifically, we will pursue four research and educational aims: 1) develop stochastic asymptotic-preserving methods for multiscale kinetic equations; 2) construct high performance stochastic algorithms for the Boltzmann collision operator; 3) design physics-preserving UQ algorithms for kinetic systems; and 4) create education and outreach activities for students through undergraduate STEM classroom innovation; graduate curriculum development in kinetic theory; graduate and undergraduate mentoring; and organizing an after-school math research program for high school girls and family math/science nights at middle and elementary schools."
"1719932","Bundle Level Type Gradient Sliding Methods for Large Scale Convex Optimization","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","07/05/2017","Yunmei Chen","FL","University of Florida","Standard Grant","Leland Jameson","06/30/2020","$154,975.00","","yun@math.ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1271","9263","$0.00","The goal of this research is to develop novel algorithms for tackling the computational challenges involved in analyzing data for applications with huge data sets. These include, for example, image processing, data mining, bioinformatics, and statistical learning. The algorithms to be developed in this research will be able to significantly reduce the number of required expensive computations, so that they can be applied to efficiently extract useful information from massive data sets. The research has the potential to advance the algorithms for large scale problems, and greatly increase the applicability for many emerging technologies. An example is the efficient reconstruction of images acquired using partial parallel magnetic resonance imaging. The development of the new methods will also enable researchers to build multi-level complex networks for better learning and prediction in many applications. This project also supports education through undergraduate and graduate student training, course development, and seminar and conference presentations. <br/><br/>This research intends to develop a novel class of accelerated bundle level type gradient sliding methods and related theories for solving large scale composite convex optimization problems and functional constrained convex optimization problems. This new class of algorithms is expected to achieve optimal iteration complexity for each component separately, but will be more general and able to handle the composition of functions with various degrees of smoothness. The algorithms offer the advantages of effectively using historical information, having a scalable scheme for solving the involved sub-problem, providing practical termination conditions for the gradient sliding, and do not impose restrictions on step sizes or require the information on the Lipschitz constants in the cost functions. Moreover, the development of these techniques for the functionally constrained problems will significantly reduce the iteration complexities and improve the practical performance of the existing techniques for functions that are smooth or weakly smooth. Further, the composite gradient sliding and accelerated approach reduces the number of gradient evaluations without increasing  the iteration complexity, while maintaining existing good properties of the approaches for the composite convex problems. The iteration complexity of all the new algorithms will be analyzed, and the practical performance will be validated through numerical simulations and for  practical applications  arising from imaging and machine learning."
"1720306","Numerical Methods for Multiscale Inverse Problems and Applications to Sonar Imaging","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","08/19/2017","Christina Frederick","NJ","New Jersey Institute of Technology","Standard Grant","Leland Jameson","08/31/2021","$100,000.00","","Christin@njit.edu","323 DR MARTIN LUTHER KING JR BLV","NEWARK","NJ","071021824","9735965275","MPS","1271","9263","$0.00","The principal investigator aims to develop mathematical and computational tools for data-driven research by using theoretically-sound prediction models and adaptive numerical methods that capture intrinsic features of complex physical processes. This research project is intended to provide new guarantees for the solvability of a class of inverse problems important in mathematics, commercial industries, and defense operations. The project will involve undergraduate and graduate students, who will receive training in numerical methods, analysis, and scientific applications. <br/><br/>The principal investigator will pursue an original strategy for extracting details from large scale datasets using a new class of efficient methods that exploit problem-dependent features of processes occurring on multiple scales. The design of numerical schemes is adaptable to qualitative scientific knowledge and has the potential to significantly enhance the accessibility and performance of current inversion methods. The basis of the approach is the selection of a low-dimensional parameter that describes key microscopic details and the development of numerical methods that retain an intrinsic knowledge of parameter values while solving large-scale models, substantially reducing computational costs. Deliverables of the project will be the new methodology, as well as scientific applications to large scale inverse problems in sonar imaging, where the main challenge is to capture the appropriate physics while maintaining computational time and memory demands acceptable for current computer architectures."
"1720405","Collaborative Research:  Efficient High-Order Algorithms for Nonequilibrium Microflows Over the Entire Range of Knudsen Number","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/20/2017","Shidong Jiang","NJ","New Jersey Institute of Technology","Standard Grant","Leland Jameson","06/30/2021","$162,500.00","","shidong.jiang@njit.edu","323 DR MARTIN LUTHER KING JR BLV","NEWARK","NJ","071021824","9735965275","MPS","1271","8037, 9263","$0.00","Nonequilibrium microflows are ubiquitous in sensors, microfluidics, and microelectromechanical systems and have important applications in bio-medical and environmental sciences, aerodynamic, chemical, and energy industries, and space science. For example, vacuum pumps manipulate rarefied gases at very low density and pressure, where approaches based on continuum theory, which is the engineering model for gaseous flows at standard temperature and pressure, is no longer valid.  The extent of nonequilibrium of a gaseous flow is qualitatively measured by the Knudsen number -- the ratio of mean free path to a macroscopic length.  Nonequilibrium flows may be modeled by the Boltzmann equation for the single-particle velocity distribution function in phase space. Standard methods for numerical solution may not be accurate enough, and their computational cost may be prohibitively expensive due to the high-dimensionality of phase space, especially for time-dependent problems. This project aims to create effective and efficient simulation tools for nonequilibrium microflows. Graduate students are involved in the research.<br/><br/>For low-speed microflows, reduced kinetic models, such as the linearized Bhatnagar-Gross-Krook-Welander (BGKW) equation, coupled with the diffuse reflection boundary condition, are reliable and capable of producing very accurate results for microflows in the whole range of the Knudsen number.  The equation can be transformed into a system of linear integral equations for macroscopic variables including the density of the gas, the flow velocity, and the temperature, which leads to great dimension reduction, consequently drastic enhancement of computational efficiency.  The overarching goal of this research project is to develop efficient high-order algorithms to solve a system of integral equations pertaining to nonequilibrium gaseous flows in various geometries over the entire range of Knudsen number, for applications to microflows.  The work consists of the following technical ingredients to overcome the challenges encountered in simulation of microflows: (1) An accurate and efficient algorithm to evaluate the Abramowitz function on the complex plane, as required for time-dependent problems; (2) Theoretical analysis, especially on the nullspace, of the integral equations; (3) Efficient and high-order algorithms for the integral equations on smooth and nonsmooth convex domains in two dimensions.  The results of the project are anticipated to provide enabling technologies for a broad range of engineering applications involving multiscale multi-physics microflows."
"1654756","CAREER: Towards General-Purpose, High-Order Integral Equation Methods for Computer Simulation in Engineering: Analysis, Algorithm Design, and Applications","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","08/15/2017","08/06/2021","Andreas Kloeckner","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Leland Jameson","07/31/2023","$400,000.00","","andreask@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1271, 8048","1045, 7433, 9263","$0.00","Numerical simulation has become an essential tool in nearly all areas of science and engineering, ranging from engine design to naval architecture, and from personalized medicine to city planning. Yet, the efficient solution of large-scale, globally coupled (so-called elliptic) computational problems arising in these application areas remains a major challenge. Although integral equation (IE) methods for numerical simulation typically have optimally low cost when applied to common problems in science and engineering, their impact has been limited to a small handful of applications by technical obstacles. The purpose of this project is to take important steps to remove these obstacles. First, the project will develop novel numerical and symbolic algorithms to reduce the amount of method design and implementation work required when adapting IE methods to new classes of applications. This will make the associated cost savings of these methods more broadly accessible. Second, the project will design, implement, and analyze parallel algorithms to enable the nation's large-scale computing resources to be used in conjunction with IE methods. Their increased computational efficiency will facilitate the study of models with increased fidelity and higher accuracy. Third, the project will extend the set of problems that can be attacked with IE methods to those including volume (and not just surface) data while using highly accurate geometric representations. Fourth, it will provide a theoretical understanding and practical methods for automatic control of numerical error in these methods. Lastly, the project will demonstrate the new methods and their use in the mathematically and numerically challenging context of fluid dynamics. To foster an understanding of the power of these kinds of computational tools in the next generation of the nation's workforce, this project will employ a day-long experience for students in their formative middle-school years. This experience will convey that computer modeling and simulation can help understand the world by testing the predictive power of simple, mechanistic models. Through its reliance on self-contained, hands-on computer experiments, the experience will be interactive and visually engaging, require little mathematics preparation, and easily establish connections with real-world applications of computing. The program focuses on creating engagement and interest, with the goal of promoting career and educational choices in mathematics and computing. <br/><br/>Integral equation (IE) methods for computer simulation typically have optimally low cost, but their impact has been limited to a handful of applications by technical obstacles.  The purpose of this research is to take important steps to remove these obstacles, by providing: 1. high-order singular quadrature and infrastructure for fast multipole methods for the evaluation of layer potentials with general, symbolically given kernels in complex geometry, 2. scalable and efficient distributed-memory parallel algorithms for the use of IE methods in large-scale applications, 3. design and analysis of high-order numerical methods for volume potentials in complex geometry as needed by inhomogeneous partial differential equations, 4. theory and methods for automatic adaptive mesh refinement based on a-posteriori error estimates, and 5. a demonstration of the capabilities of the developed methods and algorithms in the context of the incompressible Navier-Stokes equations, including high-order finite-element-method and IE coupling."
"1720212","Collaborative Research:   Efficient, Stable and Accurate Numerical Algorithms for a class of Gradient Flow Systems and their Applications","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/16/2017","XIAOFENG YANG","SC","University of South Carolina at Columbia","Standard Grant","Leland Jameson","06/30/2021","$159,996.00","","xfyang@math.sc.edu","1600 HAMPTON ST # 414","COLUMBIA","SC","292083403","8037777093","MPS","1271","9150, 9263","$0.00","This project focuses on the development of efficient, stable and accurate numerical algorithms for gradient flow systems which are ubiquitous in modeling of real-world phenomena. It is expected that the research will not only lead to efficient numerical algorithms for a class of problems of current interests, but also contribute to better understandings of some fundamental issues in materials science, biotechnology, and other related fields through numerical simulations. This project will also provide opportunities for the involved graduate and undergraduate students to learn critical skills of computational and applied mathematics and to develop state-of-the-art numerical algorithms for science and engineering applications. <br/><br/>The free energies of gradient flow systems usually consist of various nonlinear potentials formulated in diverse complex formats which present a major challenge in the construction of efficient and accurate time discretization schemes. The project aims at overcoming this challenge by using a flexible and robust IEQ approach that enables one to develop time discretization schemes for a large class of gradient flow systems. The goals of this proposal are three folds: (i) to develop a unified numerical framework of time-marching schemes for solving general gradient flow models with high nonlinearity; (ii) to develop efficient numerical schemes for a number of challenging gradient flow models of current interests (e.g., nonlinear coupled multivariable models, nonlocal models, anisotropic models, nonlinear coupled systems that follow various physical principles, tensor based liquid crystal models); (iii) to investigate some fundamental issues of viscoelastic drops on substrates and active liquid crystal droplets using the developed predictive numerical tools. The proposed schemes will lead to numerical predictive tools that extend the capability of mathematical and experimental analysis, and contribute to better understanding of some pressing science and engineering issues related to multi-phase complex fluids."
"1720472","Algorithms for Assessing and Improving Joint Inversion","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","08/16/2017","Jodi Mead","ID","Boise State University","Standard Grant","Leland Jameson","08/31/2021","$204,457.00","John Bradford","jmead@boisestate.edu","1910 UNIVERSITY DR","BOISE","ID","837250001","2084261574","MPS","1271","9150, 9263","$0.00","Understanding the structure of the earth's subsurface is essential to modern society.  For example, this knowledge aids in building safe structures, allows us to locate minerals, hydrocarbons, groundwater and contaminants; map tunnels, pipes and mines.  The structure of the earth's subsurface can be understood by its material properties such as rock, soil and water.  However, these properties cannot be measured directly in the subsurface.  Therefore, non-invasive geophysical measurements are used to observe them.   This involves measuring an energy response at the earth's surface, after electromagnetic waves or electric currents are injected into the subsurface.  The mathematical approach to inferring an image of the subsurface from these energy observations is called inversion.  The major challenge in recovering a subsurface image is that no one single image results from a given energy response.   Recently, scientists have addressed this issue by using multiple types of energy to recover an image, which is referred to as joint inversion.  In this project, the PIs use joint inversion to combine observations of energy transfer from a large range of frequencies. The PIs hypothesize that with information from a large frequency range, a unique image can be created that is an accurate representation of the subsurface.  This approach can be extended to other fields such as wireless communication and video processing.  Therefore, the PIs will also develop a software package that can be used to decide if particular data types are complimentary.<br/><br/>Simultaneous joint inversion involves optimizing a single objective function with information from multiple types of data.  The PIs will combine complex electrical resistivity (ER) and ground penetrating radar (GPR) measurements in the Earth's subsurface, and determine the effectiveness of using both types of data.  The approach determines if and how the physics describing different types of data collection techniques contribute to abolishing the other's null space.   More generally, the PIs will determine how different types of data can most effectively regularize each other.  This will be done by quantifying decay rates of singular values of integral solutions describing the fundamental physics for ER and GPR data.  This requires the development and implementation of a framework through which the singular values in a joint inversion can be computed in a continuous setting.  The relationship between decay rates and an effective joint inversion will be tested with ER and GPR field data from the Boise Hydrological Research Site. This project helps bridge the gap between forward and inverse modeling by novel implementations and analyses in each field."
"1720291","OP: Collaborative Research: Novel Feature-Based, Randomized Methods for Large-Scale Inversion","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","05/15/2017","Eric Miller","MA","Tufts University","Standard Grant","Leland Jameson","08/31/2021","$284,131.00","Misha Kilmer","eric.miller@tufts.edu","169 HOLLAND ST FL 3","SOMERVILLE","MA","021442401","6176273696","MPS","1271","8990, 9263","$0.00","The desire to form an image of a region of space from externally collected data arises in applications ranging from detecting and characterizing cancers in the body, to quantifying the distribution of water, oil, or subsurface pollutants, and to the timely accurate identification of explosives in crowded venues. The physics associated with signal propagation and sensing in these problems creates substantial computational challenges for transforming raw data into useful information. The research team in this project aims to develop computational methods that greatly reduce the cost of real time imaging by providing improvements in statistical inverse theory, numerical inversion methods, simulation models, and hybrid imaging models.  The main thrusts of the project will be tested on imaging applications in medical tomography, environmental remediation, and airport security imaging. The techniques form the basis for addressing analogous problems associated with inversion of optical signals across a wide range of spatial and temporal scales. As part of the project, a modular course will be developed to teach these new methods at the graduate level. The course materials will be made available over the internet.<br/><br/>The large-scale imaging, or inverse, problems addressed by this collaborative team require the minimization of a parameter-dependent function that expresses the misfit of predicted measurements for a candidate image and actual measurement data. The potentially large number of parameters must be minimized over an ever-increasing huge number of measurements, while concurrently some unknown set of the data may be redundant.  Detailed images, however, are not always needed for addressing relevant, practical questions and decision making. A combination of computational techniques will be developed to make large-scale parameter-dependent minimization computationally feasible.  Furthermore, novel efficient approaches for inferring critical image features will be developed, obviating need for complete reconstruction of an image. The research builds on recent methods that exploit randomization to compute accurate estimates of solutions at greatly reduced computational cost, and on the efficient construction of smaller, approximate, reduced order numerical models that are accurate for relevant sets of parameters, and thus reduce the cost of full simulation of the sensing physics. Probabilistic approaches for inference of critical image features that guide image interpretation and decision making will be developed. The mathematics associated with this approach requires these methods to capitalize on other new tools also under development in this project."
"1720825","Workshop: Recent Advances and Challenges in Discontinuous Galerkin Methods and Related Approaches","DMS","COMPUTATIONAL MATHEMATICS","06/01/2017","01/05/2017","Yanlai Chen","MA","University of Massachusetts, Dartmouth","Standard Grant","Leland Jameson","05/31/2018","$15,000.00","Bo Dong","yanlai.chen@umassd.edu","285 OLD WESTPORT RD","NORTH DARTMOUTH","MA","027472356","5089998953","MPS","1271","7556, 9263","$0.00","The international conference entitled ""Recent Advances and Challenges in Discontinuous Galerkin Methods and Related Approaches"" will be held at the Institute for Mathematics and its Applications at the University of Minnesota from June 29 to July 1, 2017. This award supports junior participants' travel. The event brings together a variety of researchers from at least 14 countries/regions. They range from internationally renowned experts to early career mathematicians and PhD students. The event will summarize recent advances made both in the theory and implementation of the Discontinuous Galerkin and related numerical approaches, and to identify new challenges and opportunities in these areas. The conference will also have a significant educational component, with each talk required to feature introductory parts at a level accessible to graduate students, and ample discussion sessions throughout the conference. To further promote cross-pollination and mentoring, there will be moderated panel sessions where participants explore the frontiers of different research areas, possibilities of new connections between areas and new applications, and exciting opportunities of new collaborations. It will help junior researchers broaden their perspective and create research ties with more senior members in these fields.<br/><br/>Discontinuous Galerkin and related approaches have been adopted in areas ranging from mechanical engineering to the simulation of muscles. In recent decades, deep theoretical advances have been made and wide-ranging applications discovered for these approaches. They often lead to design of novel methods (e.g. Hybridizable discontinuous Galerkin methods, Virtual Element Methods, etc.) with superior properties in terms of accuracy, versatility, robustness and computational efficiency. They also leave open many exciting problems. This conference presents a rare but timely opportunity to summarize recent advances both in the theory and implementation of these methods, identify new challenges, and map out future research directions in related areas. The bringing together of people from different fields such as engineers, applied mathematicians, national lab researchers, will lead to cross-fertilization of ideas that normally does not happen in a conference of this size."
"1720451","Collaborative Proposal: Strong Stochastic Simulation of Stochastic Processes Theory and Applications","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2017","08/17/2017","Jose Blanchet","NY","Columbia University","Standard Grant","Leland Jameson","07/31/2018","$200,871.00","","jose.blanchet@stanford.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1271, 8069","8083, 9263","$0.00","High performance computing of continuous random structures arises in a large body of scientific and engineering investigations. For example, these structures are used in environmental models for floods in different geographical areas, which are subject to random measurement errors. They are also used in the prediction and mitigation planning of potential disasters. However, these random structures are impossible to capture in a computer without incurring bias, due to their continuous nature. This research project investigates a new framework for the numerical analysis of continuous random structures. It achieves stronger error control, compared to current state-of-the-art methods, at basically the same computational cost. If successful, the framework and algorithms to be investigated will facilitate analysis and performance evaluation of fundamental random structures of interests to a broad community of scientists and engineers. To enhance the broader impact, the Principal Investigators will train graduate students through research and integrate the results from this research into new graduate courses in scientific computing. <br/><br/>This project investigates a new Monte Carlo framework for continuous stochastic structures (such as differential equations and random fields). The main innovative feature of the framework is the ability to approximate a continuous random object by a fully simulatable (typically piece-wise constant) object with a uniform error bound in the path space with 100% certainty. The error bound is user-specified and can be sequentially refined. Research projects involve developing simulation algorithms for fundamental random structures of interests. These include: Gaussian random fields, Levy processes, fractional Brownian motion, max-stable fields, etc. The algorithms are scalable in the sense of being easily extendable to more complex models by applying the continuous mapping principle with quantifiable error analysis. An important aspect of the methodology is the connection established between Monte Carlo simulation and the theory of rough paths in the setting of stochastic analysis."
"1720440","Collaborative Research:   Efficient, Stable and Accurate Numerical Algorithms  for a class of Gradient Flow Systems  and their Applications","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/16/2017","Jie Shen","IN","Purdue University","Standard Grant","Leland Jameson","06/30/2020","$130,000.00","","shen@math.purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271","9263","$0.00","This project focuses on the development of efficient, stable and accurate numerical algorithms for gradient flow systems which are ubiquitous in modeling of real-world phenomena. It is expected that the research will not only lead to efficient numerical algorithms for a class of problems of current interests, but also contribute to better understandings of some fundamental issues in materials science, biotechnology, and other related fields through numerical simulations. This project will also provide opportunities for the involved graduate and undergraduate students to learn critical skills of computational and applied mathematics and to develop state-of-the-art numerical algorithms for science and engineering applications. <br/><br/>The free energies of gradient flow systems usually consist of various nonlinear potentials formulated in diverse complex formats which present a major challenge in the construction of efficient and accurate time discretization schemes. The project aims at overcoming this challenge by using a flexible and robust IEQ approach that enables one to develop time discretization schemes for a large class of gradient flow systems. The goals of this proposal are three folds: (i) to develop a unified numerical framework of time-marching schemes for solving general gradient flow models with high nonlinearity; (ii) to develop efficient numerical schemes for a number of challenging gradient flow models of current interests (e.g., nonlinear coupled multivariable models, nonlocal models, anisotropic models, nonlinear coupled systems that follow various physical principles, tensor based liquid crystal models); (iii) to investigate some fundamental issues of viscoelastic drops on substrates and active liquid crystal droplets using the developed predictive numerical tools. The proposed schemes will lead to numerical predictive tools that extend the capability of mathematical and experimental analysis, and contribute to better understanding of some pressing science and engineering issues related to multi-phase complex fluids."
"1722535","International Conference on Current Trends and Challenges in Numerical Solution of Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","12/30/2016","Jie Shen","IN","Purdue University","Standard Grant","Leland Jameson","06/30/2018","$15,000.00","","shen@math.purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271","7556, 9263","$0.00","The international conference on Current Trends and Challenges of Numerical Solutions of Partial Differential Equations  will take place at Purdue University July 7-July 9, 2017. The meeting is a forum to promote, enhance, and stimulate international research interactions and collaborations in applied and computational mathematics. The conference provides a great opportunity for young and minority researchers to closely interact with leading experts in a stimulating environment. <br/><br/>Computational Methods, particularly those for solving partial differential equations, have experienced major developments over the past several decades in both algorithmic advances and applications to other fields.  These developments have had profound implications and applications in  science, technology, engineering and mathematics. This conference brings together leading experts who will  survey the current trends and challenges in the numerical methods for solving partial differential equation. This will lead to identification of promising  new ideas and methods that can advance this active field in coming years."
"1719545","Collaborative Research:  Tractable Non-Convex Optimization","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","07/02/2019","Afonso Bandeira","NY","New York University","Continuing Grant","Leland Jameson","06/30/2020","$120,001.00","","bandeira@cims.nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1271","9263","$0.00","Practitioners in most fields of science and engineering invest a substantial amount of time developing appropriate mathematical models for the complex questions they set out to answer. A model is deemed appropriate for a given task if it meets two potentially conflicting criteria simultaneously. On the one hand, it must be sufficiently faithful to reality (and as such, sufficiently complex) so as to capture the essential properties of the object of study. On the other hand, the model must be simple enough that it can be practically used to answer relevant questions. This second requirement is computational in nature. In effect, the modeler aims to reduce a particular question to a mathematical problem known to be practically solvable, or tractable.  For the most part, tractability ensures that the problem can be solved using known algorithms, and as a result the status quo has been that problems that are not tractable should be avoided for applications. Yet, scores of problems in science and engineering are most naturally modeled within a framework that has been previously determined to be non-tractable. This research project aims to develop theory and algorithms to identify and solve non-convex optimization problems, typically regarded as non-tractable. The goal is to provide practitioners with an extended modeling toolbox, allowing them to capture key aspects of our complex reality.<br/><br/>This project targets optimization problems that are tractable despite non-convexity. This can come about in a number of ways. The non-convexity may be structurally benign, in that the problem actually does not have local optima at all. This project explores such structural effects in the context of Burer-Monteiro relaxations. Alternatively, the problem may present numerous local optima in some instances, yet present only good quality ones on instances of the problem encountered in practice. This motivates the analysis of non-convex optimization problems in a non-adversarial setting, in many cases more relevant to practice than classical adversarial analyses. This project investigates some model problems of this nature. Here too, salvation can come in different forms: It is possible that when data is good enough (for example, if the signal to noise ratio is sufficient), local optima cannot exist; or, it is possible to initialize the algorithms close enough to the global optimum so that convergence to it is assured; or, even local optima are satisfactory to answer the underlying question. This project explores such situations through applications in community detection in large networks and through phase synchronization, as model problems for understanding challenges in a more general class of problems including, but not limited to, electron cryomicroscopy from structural biology and simultaneous localization and mapping in robotics. A common feature of many tractable non-convex optimization is that they are naturally posed on smooth nonlinear spaces called Riemannian manifolds. As a result, important algorithmic aspects of this project involve developing theory, algorithms, and software for optimization on Riemannian manifolds."
"1723011","Collaborative Research: Stochastic Approximations for the Solution and Uncertainty Analysis of Data-Intensive Inverse Problems","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2017","06/13/2017","Youssef Marzouk","MA","Massachusetts Institute of Technology","Standard Grant","Christopher Stark","08/31/2021","$110,000.00","","ymarz@mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1271, 8069","8083, 9263","$0.00","In scientific fields ranging from geophysics and atmospheric science to medical imaging and network communication, data are being generated at remarkable rates. Such data are typically indirectly related to quantities of interest and the data sets are in many cases dynamically growing. Extracting desired information from these data then requires the solution of very large data-intensive inverse problems, perhaps repeatedly and in real time. The computational challenges of obtaining such a solution are compounded by the demands of validation and uncertainty analysis, which can easily become computationally prohibitive. This project will develop mathematical/statistical methods and computational tools for the solution of data-intensive inverse problems. The core of this approach is a stochastic reformulation of such problems that aims to significantly reduce the computational costs while adapting to modern hardware architectures.<br/><br/>A framework will be developed to address the challenges arising at the interface between big data, inverse problems, data analysis, and uncertainty quantification.  First, randomized methods for the solution of linear and nonlinear inverse problems will be introduced, so that efficient stochastic optimization methods can be used to overcome the hardware limitations of current algorithms and to generate solutions and uncertainty assessments in near-real time. New theory and scalable methods will be developed within the stochastic framework, thereby ensuring solution accuracy, reliability, and robustness. Second, advanced tools will be developed for model validation, error analysis, and uncertainty quantification. By partnering with application scientists (e.g., in atmospheric remote sensing), methods developed in this project will be of immediate practical utility for scientists and engineers."
"1720213","Approximation of Singular Solutions to Nonlocal and Nonlinear Models","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","06/03/2019","Abner Salgado","TN","University of Tennessee Knoxville","Continuing Grant","Leland Jameson","07/31/2021","$166,651.00","","asalgad1@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1271","9263","$0.00","Numerical analysis has been very successful in the development and analysis of schemes to approximate the solution of classical models in the pure and applied sciences. However, in recent times, new classes of models have emerged which challenge the current understanding and techniques of numerical analysis. New approximation techniques are required or the analysis of the classical ones call for new ideas, as standard arguments do not work. This is particularly the case for problems which exhibit nonlocal features in time (memory effects), nonsmooth evolution problems, nonlocality features in space (long range interactions), or a combination of any of these features. Another important class of problems that require special attention are those where the data of the problem is nonsmooth, which includes singular forcing or constitutive laws. Finally, as a last example there are strongly nonlinear problems where there is a barrier in how smooth the solution can be, regardless of the smoothness of the problem data.<br/><br/>The purpose of this research project is the analysis of approximation techniques for a representative sample of the problems mentioned above. The implementation of many of the numerical techniques that we will discuss in many cases is standard, but their analysis requires a fine interplay between the regularity of the solution (in nonstandard spaces), the structure of the problem and that of the scheme. As an outcome of this work, new numerical techniques will be developed and the existing ones will be strengthened by solid mathematical analysis of their approximation properties. The models which will be under our study describe a wide range of phenomena, and mathematically solid numerical methods for them will be developed. Thus, the proposed ideas will enhance modeling and prediction capabilities. For instance, the study of discretization techniques for nonlocal operators is in its infancy. Even in the linear case, the nonlocality greatly complicates the analysis and efficient implementation of solution schemes."
"1719968","Polynomial Homotopy Continuation: Under the Hood","DMS","COMPUTATIONAL MATHEMATICS","06/15/2017","06/07/2017","Anton Leykin","GA","Georgia Tech Research Corporation","Standard Grant","Leland Jameson","05/31/2021","$250,000.00","","leykin@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271","9263","$0.00","Systems of polynomial equations are ubiquitous in mathematical models in science and engineering. The field of algebraic geometry, which studies solutions to such systems, has the potential to improve techniques for practical numerical investigations of these systems.  This research project aims to apply algebraic geometry to advance numerical algorithms for solving systems of polynomial equations. The research includes developing new homotopy continuation methods that exploit the action of the monodromy group, random walk homotopies, and hybrid algorithms for solving sparse systems. The new theoretical framework and algorithms will be implemented in open-source software. <br/><br/>Homotopy continuation algorithms are a backbone of modern nonlinear algebra, the art of solving systems of equations that are not necessarily linear. The main strength of these algorithms is in approximate computation, which often is much faster than classical exact techniques and allows tackling problems in high-dimensional spaces. Homotopy continuation methods solve a problem A in three steps: (1) look for a problem B in the same family of problems as A, but with a simpler structure; (2) construct solutions to that simpler problem B; (3) connect A and B with a homotopy, that is, a continuous deformation, and track how solutions of B morph into solutions of A. This project aims to develop a novel framework for basic homotopy continuation routines. One major point is that randomizing numerical algorithms to a greater extent makes them even faster and more robust without a costly increase in computational precision. Another point is that, looking to minimize computational costs, we should invent new hybrid methods intertwining exact and approximate techniques originating in different areas of mathematics. The symbiosis of symbolic, combinatorial, and numerical ideas is the key to the new methods for solving sparse systems. Tools from tropical geometry and numerical algebraic geometry will deliver a generalization of polyhedral homotopy algorithms and lead to a faster polynomial system solver that benefits from a tighter solution count."
"1720420","Collaborative Research:  Modeling and Computation of Three-Dimensional Multicomponent Vesicles in Complex Flow Domains","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","07/18/2017","Shuwang Li","IL","Illinois Institute of Technology","Standard Grant","Leland Jameson","08/31/2021","$206,040.00","","sli15@iit.edu","10 W 35TH ST","CHICAGO","IL","606163717","3125673035","MPS","1271","9263","$0.00","Vesicles have long been considered as model systems for studying fundamental physics underlying complicated biological systems such as cells and microcapsules.  Additionally, vesicles are increasingly being used as carriers for drug delivery or as biochemical micro-reactors operating in physiological environments.  This project aims to develop efficient computational models and numerical tools for modeling of vesicle dynamics, which involves phase separation, formation of compartments, and interactions among vesicles and their aqueous environment.  The results are expected to aid in the design of phase domains on the surface of a vesicle such that specific proteins can anchor on the membrane to initiate subsequent biological reactions.  The project aims to contribute to the forefront of research on constructing artificial cells with multiple compartments. The research also advances numerical method development, analytical theory, and integral equation formulations in the context of bio-membrane mechanics and particulate flows. The project will create opportunities for students to receive interdisciplinary training crossing the mathematical, biological, and physical sciences.<br/><br/>This project addresses the challenges of mathematically modeling and numerically simulating three-dimensional multi-component and multi-compartment vesicles in complex flow domains using sharp interface methods. At the continuum level, the mathematical description of vesicle dynamics is a highly nonlinear, nonlocal moving boundary problem where the bilayer membrane serves as the moving boundary. The fundamental mathematical feature is that this system effectively couples surface phase dynamics, morphological evolution and compartment formation, and fluid motion so that the model describes a more realistic physical system than has been developed previously in the literature. The investigators develop and apply state-of-the-art adaptive numerical methods, perform analytical, numerical and modeling studies of important constituent processes, and work with experimentalists to test the model predictions and to help elucidate the underlying physical processes. The project will investigate how the presence of surface phases and multiple compartments modifies the classical motions and hydrodynamic interactions and may lead to novel dynamical regimes. The project will also investigate the morphological stability of multi-compartment vesicles in applied flows, and possible control strategies using multiple surface phases for optimizing stability in complex flow domains."
"1720369","Topics in Mathematical Theory of Adaptive Finite Element Methods","DMS","COMPUTATIONAL MATHEMATICS","09/01/2017","06/06/2017","Alan Demlow","TX","Texas A&M University","Standard Grant","Leland Jameson","08/31/2021","$180,193.00","","demlow@math.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","9263","$0.00","Finite element methods (FEM) are widely used to approximately solve partial differential equations in simulations of physical phenomena arising in engineering and the physical sciences. Such simulations are an indispensable tool in the development and testing of new technologies.  Adaptive variants of finite element methods are designed to increase the efficiency and accuracy with which simulations can be carried out by making better use of computational resources and to increase confidence in the accuracy of simulations by providing researchers with a computable measure of the errors that arise in approximation techniques.  This research project aims to develop new variants of adaptive finite element methods and increase mathematical understanding of their underpinnings.  The project has two main foci.  The first is adaptive FEM for partial differential equations defined on surfaces, which arise for example in describing fluid flows with multiple components (such as oil and water). The second is development and analysis of adaptive FEM for controlling various measures of the error, especially maximum errors.<br/><br/>In the first project the investigator will construct and analyze adaptive variants of surface finite element methods with two main goals in mind.  First, while surface FEM are an established finite element methodology with many useful variants defined, adaptive versions of some important variants are missing.  This project aims to fill that gap.  Secondly, the project will explore the interaction between adaptive surface FEM, the way a given surface is represented in a finite element code, and the smoothness or regularity of the surface.  The result will be more robust adaptive surface codes that give users greater flexibility in representing surfaces while also making the best possible use of available information about the surface.  The second main project will lead to proof of convergence of adaptive algorithms for controlling maximum errors, and will also provide new adaptive algorithms for controlling maximum errors in a class of singularly perturbed elliptic problems."
"1719549","Novel Numerical Approaches for Structured Optimization","DMS","COMPUTATIONAL MATHEMATICS","08/15/2017","06/05/2019","Yangyang Xu","NY","Rensselaer Polytechnic Institute","Continuing Grant","Leland Jameson","07/31/2020","$96,000.00","","xuy21@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","MPS","1271","8083, 9263","$0.00","The data sets involved in many modern applications are extremely large, and are often collected at distributed locations and continuously with the progression of time. Common examples are data sets associated with searches on the Internet, social networks, information technology, healthcare, biology, finance, and engineering. Analyzing and learning from these massive data sets imposes great challenges on computation, data storage, and data transfer. On the other hand, high performance computers are now readily available. This project aims to develop novel computational methods to enable high performance computing for questions involving extremely large data sets. The approaches address several computational challenges that emerge from applications across data sciences and engineering.  Undergraduate and graduate students are involved in the project.<br/><br/>This project is focused on designing novel computational algorithms and analyzing their theoretical behaviors for solving structured optimization problems that involve huge data sets and are parameterized by large numbers of variables. Both the defining objective functions and the optimal solutions exhibit particular structures, including convexity, smoothness, and multi-linearity for the former, and sparsity, low-rank, and orthogonality for the latter. This research aims to take advantage of this structure in designing efficient computational methods. The project includes several research directions, from variable splitting for handling complicated regularizers, to adaptive asynchronous parallel computing and analysis of convergence rates. Stochastic approximations will be used for dealing with problems involving stream data, and novel numerical approaches will be used to solve non-linearly constrained problems via primal-dual updates. Problems with multi-array structure will also be investigated. The research aims to significantly speed up existing algorithms both theoretically and practically, lead to new theoretical results of existing algorithms that currently lack convergence analysis, and give rise to novel algorithms for computing solutions to complicated problems that are currently not efficiently solvable."
"1720323","RUI: Algorithms and Modeling for Chemotactic Deformable Particles in Non-Newtonian, Multiphase, Non-Isothermal, Turbulent Flows","DMS","COMPUTATIONAL MATHEMATICS","08/15/2017","06/03/2019","Hoa Nguyen","TX","Trinity University","Continuing Grant","Leland Jameson","07/31/2021","$149,996.00","Hakan Basagaoglu","hnguyen5@trinity.edu","1 TRINITY PL","SAN ANTONIO","TX","782124674","2109997246","MPS","1271","9229, 9263","$0.00","This project concerns computational fluid dynamics in the field of fluid-structure interactions. The goal of the research is to develop a new multi-scale computationally-efficient and robust numerical model to simulate particles that move in response to chemical stimuli in their environment. The work aims to create new understanding of the hydrodynamic interactions of such particles with the bulk fluid, which differs from particle-fluid hydrodynamics for non-chemotactic particles, whose motility is driven solely by the bulk fluid flow. The project will recruit a diverse group of talented undergraduate students and encourage them to pursue graduate studies, focusing on challenging research problems in a well-established interdisciplinary environment. The project will introduce students to the intellectual excitement of computational and mathematical research in fluid dynamics, thereby encouraging them to think creatively and independently about potential applications and increasing their awareness of pathways to graduate schools and opportunities in research and development in industry employment.<br/><br/>This project explores a new multi-scale computationally-efficient and robust numerical model to simulate two- and three-dimensional motility of deformable chemotactic particles in complex fluids. This addresses two fundamental issues that arise in the mathematical and computational treatment of particle motility in fluids: (i) self-autonomous motion of deformable chemotactic particles in response to spatially- and temporally-varying chemoattractant gradients, and (ii) particle-fluid hydrodynamics in non-Newtonian fluids and multiphase flows in laminar to turbulent flow regimes. The multi-scale model couples intracellular signaling pathways with particle-fluid interactions using the RapidCell model, the method of Regularized Stokeslets, and the Immersed Boundary method. This is well-suited for sophisticated applications, including, for example, bacterial chemotaxis promoting biofilm formation and swarm robotic odor localization. The project includes development of a novel, multi-scale computational fluid dynamics package suitable for theoretical analyses and diverse multi-disciplinary applications. It will serve as a highly informative teaching tool for graduate and undergraduate level classes."
"1719640","Collaborative Research: Overcoming Order Reduction and Stability Restrictions in High-Order Time-Stepping","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/20/2017","Benjamin Seibold","PA","Temple University","Standard Grant","Leland Jameson","07/31/2021","$176,562.00","Dong Zhou","seibold@temple.edu","1801 N BROAD ST","PHILADELPHIA","PA","191226003","2157077547","MPS","1271","9263","$0.00","This project develops new computational approaches that remedy fundamental accuracy shortcomings of existing time-stepping methods, and increase their stability and robustness. A wide variety of practical applications, including fluid flows, quantum physics, heat and neutron transport, materials science, and many complex multi-physics problems, require the numerical simulation of models that involve a time evolution. This time evolution must be performed in a way that the high accuracy of modern computational methods is retained. This project addresses fundamental challenges that arise in this context, and delivers superior numerical methods that could replace existing time-stepping schemes currently used in computational science and engineering practice. This project provides a multi-institution collaboration, including two early-career researchers, and it involves the training of a PhD student.<br/><br/>The research in this project addresses two aspects in high-order time-stepping: order reduction in Runge-Kutta methods; and unconditionally stable ImEx linear multistep methods. A specific focus lies on time-stepping for partial differential equations. For those, order reduction can be associated with numerical boundary layers, caused by multi-stage time-stepping schemes. Based on this geometric understanding of the phenomenon, remedies for order reduction are developed. This includes the concept of weak stage order, as well as modified boundary conditions. An alternative avenue to avoid order reduction is provided by multistep methods. The key challenge here is their rather restrictive stability behavior. Based on a new stability theory for ImEx multistep methods, this project develops novel schemes that can, for certain problems, achieve unconditional stability. The new schemes can be included into many existing computational codes via a simple modification of the time-stepping coefficients, thus enabling practitioners to select the time step based solely on accuracy considerations."
"1719693","Collaborative Research: Overcoming Order Reduction and Stability Restrictions in High-Order Time-Stepping","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/20/2017","David Shirokoff","NJ","New Jersey Institute of Technology","Standard Grant","Leland Jameson","07/31/2021","$195,844.00","","david.g.shirokoff@njit.edu","323 DR MARTIN LUTHER KING JR BLV","NEWARK","NJ","071021824","9735965275","MPS","1271","9263","$0.00","This project develops new computational approaches that remedy fundamental accuracy shortcomings of existing time-stepping methods, and increase their stability and robustness. A wide variety of practical applications, including fluid flows, quantum physics, heat and neutron transport, materials science, and many complex multi-physics problems, require the numerical simulation of models that involve a time evolution. This time evolution must be performed in a way that the high accuracy of modern computational methods is retained. This project addresses fundamental challenges that arise in this context, and delivers superior numerical methods that could replace existing time-stepping schemes currently used in computational science and engineering practice. This project provides a multi-institution collaboration, including two early-career researchers, and it involves the training of a PhD student.<br/><br/>The research in this project addresses two aspects in high-order time-stepping: order reduction in Runge-Kutta methods; and unconditionally stable ImEx linear multistep methods. A specific focus lies on time-stepping for partial differential equations. For those, order reduction can be associated with numerical boundary layers, caused by multi-stage time-stepping schemes. Based on this geometric understanding of the phenomenon, remedies for order reduction are developed. This includes the concept of weak stage order, as well as modified boundary conditions. An alternative avenue to avoid order reduction is provided by multistep methods. The key challenge here is their rather restrictive stability behavior. Based on a new stability theory for ImEx multistep methods, this project develops novel schemes that can, for certain problems, achieve unconditional stability. The new schemes can be included into many existing computational codes via a simple modification of the time-stepping coefficients, thus enabling practitioners to select the time step based solely on accuracy considerations."
"1719303","Path Integral Monte Carlo Methods for Computing Polarizability Tensors of Nano-materials and Electrical Impedance Tomography","DMS","COMPUTATIONAL MATHEMATICS","08/15/2017","07/10/2017","Wei Cai","NC","University of North Carolina at Charlotte","Standard Grant","Leland Jameson","11/30/2017","$182,516.00","","cai@smu.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","MPS","1271","9263","$0.00","This research project aims to develop improved efficient numerical methods for two application areas: highly accurate simulation of the electric and magnetic properties of nanometer-scale materials, and electrical impedance tomography.  In both areas, numerical computations with traditional methods are challenging, if not impossible.  This project aims to develop novel computational methods based on probabilistic representations of solutions to the partial differential equations under study.  Results of the project are expected to have wide applicability, from the development of solar cells to the detection of cancer.<br/><br/>This project concerns the development of highly accurate and efficient numerical methods to simulate the electric and magnetic polarizability tensors of nanoparticles of complex shapes as in nanowires, quantum dots, and DNA, and fast algorithms for electrical impedance tomography (EIT). Due to the geometric complexities of nanoparticles, numerical computations with traditional mesh-based discretization methods such as finite element and boundary element methods face great challenges, if not impossibility. To meet these challenges, in this project, path integral Monte Carlo (PIMC) methods, based on Feynman-Kac probabilistic representations of solutions to partial differential equations, will be studied for material science applications as well as EIT problems. Compared with traditional grid-based numerical methods, the PIMC methods offer the capability of handling objects with highly irregular geometries arising from materials science applications on the one hand, and provide local solutions of partial differential equations over electrodes in forward problems in EIT on the other hand."
"1720237","Operator Splitting Methods: Certificates and Second-Order Acceleration","DMS","COMPUTATIONAL MATHEMATICS","07/01/2017","06/07/2017","Wotao Yin","CA","University of California-Los Angeles","Standard Grant","Leland Jameson","06/30/2020","$205,000.00","","wotaoyin@math.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1271","9263","$0.00","This research project is centered on development of improved numerical algorithms for application to large-scale systems that include, for example, signal/image/video reconstruction and processing, bioinformatics, and automated learning or mining of information from very large data sets. Operator splitting is a class of methods that decomposes a difficult problem into simple sub-problems.  Within the past decade, operator splitting methods gained popularity due to the growing demand to handle ever-larger models. For example, signal processing and machine learning applications often have multiple parts that are easy to handle separately but are very challenging when combined. Ideas from operator splitting have led to efficient algorithms for broad classes of objective functions that are used to define the underlying systems. There is still, however, much to be done to handle complex situations. Through further development of operator splitting techniques, this research has the potential to provide efficient and stable approaches to solve a yet wider class of challenging problems. The project also includes educational impact through the development of courses, presentation of seminars, and graduate student training opportunities.<br/><br/>The principal investigator intends to design and implement algorithms that improve the speed and stability of operator splitting methods. This project aims to extend the principle of operator splitting in two ways. First, operator splitting algorithms will be introduced that recognize infeasible and feasible-but-unbounded optimization problems, as well as those that have finite optimal values but unattainable solutions. Such pathological problems are not rare and cripple existing techniques. The new algorithms will address these pathologies and make future solvers more robust. Second, by incorporating second-order information in a novel fashion, the project will address two significant drawbacks of operator splitting algorithms. These are the slow tail convergence, and the sensitivity to severe problem conditions. Techniques to ensure global convergence will be developed. Because operator splitting is a high-level abstraction, the results of the project will apply to a broad range of numerical methods that arise in science and engineering."
"1720341","Collaborative Research:   Selection Methods for Algebraic Design of Experiments","DMS","COMPUTATIONAL MATHEMATICS","08/15/2017","08/14/2017","Elena Dimitrova","SC","Clemson University","Standard Grant","Leland Jameson","07/31/2019","$100,000.00","","edimitro@calpoly.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1271","9150, 9263","$0.00","Data science has emerged as an important field for making decisions based on data collected from sectors as varied as healthcare and housing.  Though data are plentiful, thanks to phone apps, merchant loyalty cards, and social media accounts, there is still a question of whether more data translates to more knowledge. Furthermore collection and storage can be problematic especially when data are sensitive, as it is often the case with clinical trials and genetic experiments.  The problem of selecting information-rich data becomes crucial for creating models that can reliably predict the outcome of future experiments. Few results have been published on the amount of necessary data, and currently there are no guidelines for generating specific data sets which would unambiguously identify a predictive model. As a first step towards developing a complete theory, the PIs will focus on models described by finite-valued nonlinear polynomial functions. (For example, the internal ""function"" in WedMD's Symptom Checker returns medical conditions according to symptoms input by the user.)  They will construct the smallest data sets that have a single associated polynomial model and study properties of such data sets.  From these computational experiments, they will build the appropriate theory, design algorithms, and generate code that can be later developed into software complete with a graphical user interface. Graduate students will participate at the appropriate level of each component of the project. Such an experience will provide them possible topics for an MS or PhD dissertation and will very likely inspire a career-long involvement in the STEM disciplines. The theoretical results will advance the fields of design of experiments, network inference, and finite dynamical systems through the determination of criteria for selecting data sets to uniquely identify models. The algorithms will serve as a guide for experimentalists in determining the data that are needed to identify the structure of a network of interest. Such knowledge has the potential to drastically reduce wasted resources that arise from too much data with too little information.<br/><br/>While this is the age of big data, there is still a question of whether more data translates to more knowledge. Particularly when collecting data is expensive or time consuming, as it is often the case with clinical trials and biomolecular experiments, the problem of selecting information-rich data becomes crucial for creating relevant models. Finite-state multivariate polynomial functions have successfully been used to model complex networks from discretized data; however, few results have been published on the amount of data necessary for such models, with the majority applying to Boolean models only. It is still unknown which data points explicitly identify such discrete models, and as a consequence, there are no methods for generating the specific data sets which would unambiguously identify the model.  The PIs will address the issue of the minimality and specificity of data to uniquely identify discrete polynomial models by developing the appropriate theory, designing algorithms, and generating code that can be later built into software. Graduate students will participate at the appropriate level of each component of the project. This project will resolve some important computational issues in network inference and will improve experimental design and model selection by eliminating the effect of computational artifacts that arise when working with nonlinear multivariate polynomials. The theoretical results will advance the fields of design of experiments and network inference through the establishment of criteria to select data sets to uniquely identify models. The proposed work will also increase the utility of polynomial dynamical systems as models of complex networks by establishing the minimal amount of the data for unique model identification. The algorithms will serve as a guide for experimentalists in determining the data that are needed to identify the structure of a network of interest. Such knowledge has the potential to drastically reduce the number of experiments performed and to eliminate the generation of data with little intrinsic value."
"1753879","CAREER: Practical Compressive Signal Processing","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","07/01/2017","08/07/2017","Deanna Needell","CA","University of California-Los Angeles","Standard Grant","Leland Jameson","05/31/2020","$148,607.00","","deanna@math.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1271, 8048","1045, 9263","$0.00","A ""signal"" is any data set that one would like to acquire, for example, an image, a large block of data, or an audio clip. One can imagine asking how quickly one would need to sample an audio clip so that from those samples alone, the audio clip could be accurately recovered. Would you need to sample every nanosecond, every millisecond, or every second? Compressive Signal Processing (CSP) shows that the important information in many signals can be obtained and recovered from far fewer samples than traditionally thought. The applications of CSP are widespread and include imaging (medical, hyperspectral, microscopy, biological), analog-to-information conversion, radar, large scale information synthesis, geophysical data analysis, computational biology, and many more. Although these applications are astounding, there has been a disconnect between the theoretical work in CSP and the use of CSP in practical settings. The goals of this project will bridge this gap by providing methods and analysis for CSP that apply to real-world signals and settings. Such work will lead to decreased scan time in MRI, reduced cost and energy consumption in computing infrastructures, improved detection of diseased crops from hyperspectral images, increased accuracy in radar, and improved compression and analysis in many other large-data applications. In addition, this project will involve students at all levels and introduce them to rigorous scientific research. The PI actively recruits members from under-represented populations, and will continue to promote diversity through her own research and outreach programs.<br/><br/>Early CSP models restrict the class of signals to those compressible in a very specific sense (sparse with respect to an orthonormal or incoherent basis). One goal of this project is to relax this restriction to allow for signals actually encountered in practice, such as those sparse in redundant, coherent, and highly overcomplete dictionaries. We will utilize both greedy approaches and optimization-based methods, tailored to specific dictionaries of interest, as well as more general methods for arbitrary bases. In addition, this project will develop adaptive CSP sampling schemes, where measurements of the signal are designed ""on the fly,"" as they are being taken. Traditional measurement schemes ignore this information, while adaptive schemes have the potential to significantly reduce reconstruction error, number of measurements, and computation time. We will identify optimal measurement strategies for constrained and unconstrained settings, and analyze how much one can actually gain from adaptivity from an information theoretic point of view. The project will also involve work in ""one-bit CSP"", a new and exciting branch of CSP which handles extreme (and often more realistic) quantization. We will draw on sub-linear methods, where large errors appear naturally, and also use optimization based techniques along with adaptive quantization thresholds to reduce the recovery error below the best possible for non-adaptive quantization. In studying these topics, the research will bridge a large gap in the theory of CSP and provide a unified framework for both practitioners and researchers."
"1747867","MANNA 2017: Modeling, Analysis, and Numerics for Nonlocal Applications","DMS","COMPUTATIONAL MATHEMATICS, Dynamics, Control and System D","12/01/2017","07/31/2017","George Karniadakis","RI","Brown University","Standard Grant","Leland Jameson","11/30/2018","$15,000.00","","George_Karniadakis@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1271, 7569","7556, 9150, 9263","$0.00","The workshop, ""Modeling, Analysis, and Numerics for Nonlocal Applications"", will take  11-15 December 2017 in Sante Fe, New Mexico; information is at the website: sites.google.com/site/manna2017abq. The focus of this workshop is on non-local mathematical models, a topic of increasing interest in several diverse scientific and engineering applications related to, as an example, material science, biology, and environmental studies. Non-local calculus and the corresponding models have emerged as a powerful tool for modeling multi-scale phenomena, including overlapping microscopic and macroscopic scales.  In the computational mathematics community, two relatively separate groups of researchers (i.e., the ""fractional derivatives"" and the ""non-local operators"" communities) are concurrently working on the analysis and simulation of these relevant problems. One of the main goals of this workshop is to unite these separate communities providing significant benefit from their interaction and opportunity to exchange their recent progress and results on the topic. Central to the workshop is also the involvement of students or young researches, not necessarily already engaged in non-local research, who will have the opportunity to interact and learn from worldwide experts. The main topics that will be addressed are the analysis and improvement of such non-local models and their efficient simulation by means of cutting-edge algorithms on the newest computer architectures. <br/><br/>Non-local operators provide a new framework to overcome limitations that are present in classical PDE-based models. This workshop is designed to facilitate the exchange of information between the (tempered) fractional calculus and non-local vector calculus and the establishment of connections between them; this will lead to the design of new improved non-local models and will facilitate their analysis and simulation. As a result, the workshop will lead to new research in applications of national interest such as subsurface flow simulations, energy storage systems, contaminant transport, polymer and complex fluid flow, material failure and damage, or any applications in complex systems and disordered media that exhibit anomalous transport and diffusion. The goals and objectives of the workshop include establishing synergies between participants working on fractional PDEs with those working on general non-local integral models; maximizing both the breadth and depth of the information imparted to and exchanged among participants; providing researchers at all career stages with the opportunity to present their state-of-the-art results or to learn from experts in the field; and identifying the most important needs and most potentially fruitful avenues for future non-local related research and related applications. This will be facilitated by the discussion session at the end of each day that will focus on a review of the recent past and near-future directions of research in each area."
"1719410","Algorithm Development, Analysis, and Application of High Order Schemes","DMS","COMPUTATIONAL MATHEMATICS","08/01/2017","07/21/2017","Chi-Wang Shu","RI","Brown University","Standard Grant","Leland Jameson","07/31/2021","$225,000.00","","chi-wang_shu@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1271","9150, 9263","$0.00","In this project the PI will perform research in algorithm design and analysis of high order accurate and efficient numerical methods for solving partial differential equations. These algorithms are used to solve scientific and engineering problems arising from diverse application fields such as aerospace engineering, semi-conductor device design, astrophysics, and biological problems.  Even with today's fast computers, it is still essential to design efficient and reliable algorithms which can be used to obtain accurate solutions to these application problems.  The broader impacts resulting from the proposed activity will be a suite of powerful computational tools, suitable for various applications mentioned above.  These tools are expected to make positive contributions to computer simulations of the complicated solution structure in these applications.<br/><br/>The algorithms the PI plans to investigate include the finite difference and finite volume weighted essentially non-oscillatory (WENO) schemes and discontinuous Galerkin finite element methods, for solving hyperbolic and other convection dominated partial differential equations (PDEs). While the emphasis of this project is on algorithm design and analysis, close attention will be paid to applications. Topics of proposed investigations will include the study on an inverse Lax-Wendroff procedure for high order numerical boundary conditions for finite difference schemes on Cartesian meshes solving problems in general geometry, Lagrangian type finite volume schemes for multi-material flows, a simple weighted essentially non-oscillatory limiter for discontinuous Galerkin methods with strong shocks, high order stable conservative methods on arbitrary point clouds, discontinuous Galerkin methods for weakly coupled hyperbolic multi-domain and network problems, efficient time-stepping techniques for discontinuous Galerkin schemes, high order accurate bound-preserving schemes and applications, bound-preserving high order discontinuous Galerkin schemes for radiative transfer equations, energy-conserving DG methods for Maxwell's equations in Drude metamaterials, efficient discontinuous Galerkin method for front propagation problems with obstacles, superconvergence analysis of discontinuous Galerkin methods and its applications, multi-scale methods based on the discontinuous Galerkin framework, and applications in areas including traffic and pedestrian flow models and aggregation, coordinated movement and cell proliferation in computational biology.  Problems in applications will motivate the design of new algorithms or new features in existing algorithms; mathematics tools are used to analyze these algorithms to give guidelines for their applicability and limitations; practical considerations including parallel implementation issues are addressed to make the algorithms competitive in large scale calculations; and collaborations with engineers and other applied scientists enable the efficient application of these new algorithms or new features in existing algorithms."
"1643426","Bay Area Optimization Meeting 2017:   From Data to Decisions.","DMS","PROBABILITY, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","04/01/2017","11/18/2016","Jesus De Loera","CA","University of California-Davis","Standard Grant","Victor Roytburd","12/31/2018","$15,000.00","","deloera@math.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1263, 1266, 1271","7556, 9263","$0.00","This award provides support for the conference Bay Area Optimization Meeting 2017: From Data to Decisions that will take place at the University California Davis, May 12-13, 2017. The meeting is the third event in the series of regional meetings, (San Francisco) Bay Area Optimization Meetings, held since 2014. The 2017 meeting is planned as a national meeting, with some international participation. Decision makers and policy analysts face increasing uncertainty, but also increasing availability of data and information. This conference brings together researchers from a variety of disciplines who work on mathematical methods that can be used to analyze large amounts of data and be applied to decision problems faced by government and industry.  The diversity of participants of this conference will result in dissemination of mathematical ideas to a broad range of researchers that otherwise might not have been aware of state-of-the-art theories and methods.  The participants include both theoretical and computational mathematicians who work on development of fundamentals of this discipline and experts in decision making in a variety of fields such as energy, defense, policy planning, and transportation, where the new methods will have impact. <br/><br/>The meeting will feature 15 world-experts in stochastic and variational analysis. The meeting will involve a large number of students, postdoctoral fellows, and junior faculty who will participate in poster sessions and engage the speakers in technical discussions. The subject of the meeting lies at the intersection of equilibrium and optimization models for decision making and statistical models of learning, forecasting, and uncertainty quantification. The lectures and papers on timely issues will be made available to the mathematical sciences, management sciences, and engineering communities through publication in a special issue of the Mathematical Programming journal. Recorded lectures will be available through the conference website. More information, including a list of speakers and abstracts, and registration information, can be found at https://www.math.ucdavis.edu/bayopt2017"
